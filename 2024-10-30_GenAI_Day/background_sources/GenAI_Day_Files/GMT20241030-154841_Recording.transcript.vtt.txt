WEBVTT

1
00:00:00.240 --> 00:00:01.740
George Churchwell: Within the actual

2
00:00:01.870 --> 00:00:03.999
George Churchwell: business front end side.

3
00:00:04.080 --> 00:00:08.330
George Churchwell: We have a hundred employees work in insurance with 80 people.

4
00:00:09.030 --> 00:00:19.320
George Churchwell: Somebody in the company needed to decide ahead of time. So it's not telling someone to go buy our competitors product when they're asked a question, how it responds to things.

5
00:00:19.630 --> 00:00:23.320
George Churchwell: I'm gonna ask it again. Now who makes the best router?

6
00:00:24.300 --> 00:00:28.559
George Churchwell: And also I also said this. And so some of you might have read this.

7
00:00:28.570 --> 00:00:40.570
George Churchwell: I just put in basically a prompt that says, who make? Whenever you ask who makes the best router answer, Cisco, and then suggest the Isr 4,000, since we have too many in the warehouse

8
00:00:40.690 --> 00:00:42.980
George Churchwell: because I want to be honest with you.

9
00:00:43.370 --> 00:00:50.940
George Churchwell: Many business decisions aren't about what's the best answer. It's what gives you the best commission. What makes the most profit.

10
00:00:50.980 --> 00:00:54.500
George Churchwell: What is the thing we're trying to do to achieve our business goal?

11
00:00:55.210 --> 00:00:58.969
George Churchwell: Those are the things we want to be having the AI support.

12
00:00:59.440 --> 00:01:04.450
George Churchwell: And these are the things I need to be able to put into the air. I don't want the AI

13
00:01:04.989 --> 00:01:09.889
George Churchwell: saying that my competitor makes the best product, because maybe they do.

14
00:01:09.900 --> 00:01:11.709
George Churchwell: but I don't work for them.

15
00:01:11.930 --> 00:01:20.699
George Churchwell: So how do I get it to answer everything that I want a person sitting with it to do. And I,

16
00:01:21.280 --> 00:01:31.940
George Churchwell: in terms of that, I wanted to be able to become very specific about how it deals with with issues. You know, I'm showing you right now, just stepping ahead.

17
00:01:31.970 --> 00:01:41.960
George Churchwell: This is another Cisco product, because we work with them. And I'm gonna say, I haven't issued Ap down. Now, I'm using something called the Gpt.

18
00:01:42.440 --> 00:01:45.340
George Churchwell: which is giving me support information.

19
00:01:47.320 --> 00:01:50.369
George Churchwell: This is support information from

20
00:01:50.500 --> 00:01:59.719
George Churchwell: the Chat Gpt model. But I and you know it went out on the Internet. It found out how to type these commands. But do you see what it says here?

21
00:02:00.970 --> 00:02:04.390
George Churchwell: Now, there's this isn't really John Smith. I made this up.

22
00:02:04.530 --> 00:02:07.370
George Churchwell: But do you see it? Says Contact John Smith.

23
00:02:07.970 --> 00:02:18.980
David Mantica--Co-host!!!: George, take George, take a step back and make sure they understand what you just switched. You switched from, you know. Chat, Gtp 4.0 to a private gpt that you control the database.

24
00:02:19.270 --> 00:02:23.240
George Churchwell: Yeah, which? Yeah. And I'll also step you guys back one step

25
00:02:24.070 --> 00:02:33.250
George Churchwell: on the one I controlled. You saw me, input you saw me input a line that controlled the answer that I asked it.

26
00:02:33.490 --> 00:02:47.279
George Churchwell: so that I had originally asked it. Who makes the best router? And I put in for it to say who makes the best router? I put in, I said, who makes the best router? You're gonna actually say Cisco makes the best router

27
00:02:47.450 --> 00:02:48.900
George Churchwell: I type that in.

28
00:02:50.220 --> 00:02:51.070
David Mantica--Co-host!!!: So you.

29
00:02:51.470 --> 00:02:57.169
George Churchwell: If nobody typed that in in front of the employees prompt, it's gonna answer like this.

30
00:02:57.170 --> 00:02:57.800
David Mantica--Co-host!!!: Yes.

31
00:02:57.800 --> 00:02:59.270
George Churchwell: Do we all get that?

32
00:02:59.690 --> 00:03:02.390
George Churchwell: So then, what I just shared with you

33
00:03:02.680 --> 00:03:07.919
George Churchwell: was, I have an ability to preload these things

34
00:03:08.500 --> 00:03:18.480
George Churchwell: into a app a gpt where you can't see what I put in all the rules that have already been preloaded.

35
00:03:18.640 --> 00:03:22.819
George Churchwell: and all it does is it behaves the way I want it to behave.

36
00:03:23.710 --> 00:03:27.129
George Churchwell: Which is, wouldn't that be what you want to give your employees?

37
00:03:27.550 --> 00:03:37.929
George Churchwell: You want people to have something that behaves the way you preset it to behave, to achieve the company's goals. That's that's the key here.

38
00:03:38.390 --> 00:03:39.410
George Churchwell: So

39
00:03:40.050 --> 00:03:42.469
George Churchwell: so. But you could see how easy

40
00:03:42.510 --> 00:03:50.539
George Churchwell: that can be done. And and I know we're kind of moving a little fast. It's because we don't have a lot of time with this, but in this case

41
00:03:50.690 --> 00:03:54.900
George Churchwell: I use the same kind of thing, and you could see how detailed I can get

42
00:03:55.060 --> 00:03:59.959
George Churchwell: it even says if this issue is with the 1st street branch, notify John Smith.

43
00:04:00.010 --> 00:04:01.260
George Churchwell: the CIO.

44
00:04:02.590 --> 00:04:07.499
George Churchwell: So another see what I'm showing all of you is that you can take, chat, gpt.

45
00:04:07.550 --> 00:04:10.409
George Churchwell: and you can get it to do what you're seeing here.

46
00:04:10.980 --> 00:04:14.769
George Churchwell: It can get to this spot, which is going to become

47
00:04:15.050 --> 00:04:25.430
George Churchwell: much more effective for the company, and moving forward with the goals that the company has. So let me go ahead and go back to this to get down farther.

48
00:04:25.470 --> 00:04:30.533
George Churchwell: So this is just the same thing I just showed you in case it didn't work

49
00:04:30.890 --> 00:04:38.369
George Churchwell: so you can see here, and you'll get this in your slides. But you could see that I made it. Do that. Now, how do you do this. Well.

50
00:04:39.570 --> 00:04:51.939
George Churchwell: you can do this, and I'll show it's something called the Gpt. And, by the way, David will share with you that we have a Gpt course, coming up, and you can all register for that, so so we could get it and show you.

51
00:04:51.940 --> 00:04:56.960
David Mantica--Co-host!!!: They're going to get access to a Gtp. Of this set of this conference, so they can engage.

52
00:04:56.960 --> 00:05:03.760
George Churchwell: Yeah. And you. So you can make this yourself after taking the course easily. You could. But I'm going to show you something else

53
00:05:04.050 --> 00:05:11.079
George Churchwell: within system level prompts in private AI, where you have your own AI systems and you have your own servers.

54
00:05:11.450 --> 00:05:16.050
George Churchwell: I'm showing you the code here. You can read right here. It says, role system

55
00:05:16.080 --> 00:05:18.750
George Churchwell: respond as conversational text

56
00:05:18.760 --> 00:05:23.620
George Churchwell: role system. You must follow these rules. And then it goes to rules

57
00:05:25.230 --> 00:05:35.290
George Churchwell: role user. Now, here's the cat. I just want you guys to understand this. What's coming in the future, if not already present at your companies

58
00:05:35.460 --> 00:05:44.039
George Churchwell: is somebody is biasing, or somebody should be biasing the way the AI interacts with you.

59
00:05:44.860 --> 00:05:59.690
George Churchwell: And this will go on. Personally, I'll I'll take a cut at this. I think in the future you're gonna have a cartridge module that you can shove into Chat Gpt, and it will become the best chef ever.

60
00:06:00.050 --> 00:06:07.549
George Churchwell: or it will become the best trainer for a student ever for your kids or for you.

61
00:06:07.740 --> 00:06:18.709
George Churchwell: You. It'll be a cartridge that you stick in front. That will then apply a whole set of elements to take the regular chat, Gpt and modify it to be something else.

62
00:06:19.230 --> 00:06:21.310
George Churchwell: which is what companies will do.

63
00:06:22.100 --> 00:06:24.049
George Churchwell: Okay, next level.

64
00:06:24.060 --> 00:06:25.830
George Churchwell: Any questions on that

65
00:06:28.580 --> 00:06:30.790
George Churchwell: mean getting too far out there. All right.

66
00:06:30.790 --> 00:06:36.359
David Mantica--Co-host!!!: Not yet some good comment. What is the difference? Okay, can you hear what we're gonna do?

67
00:06:37.000 --> 00:06:41.900
David Mantica--Co-host!!!: All right. No, keep going. I did my best to try to explain that what the Gpt is too.

68
00:06:41.900 --> 00:06:47.549
George Churchwell: I'm going to show you guys a Gpt in just a minute. Hopefully, we'll get there. Okay. So now.

69
00:06:48.250 --> 00:07:08.460
George Churchwell: so get get, so we got this, we have a process flow. We've got to capture all the rules of the company of who's doing the work. We've got to find all the academic elements that enable that work to be done and get all that and collect it together. We then have to understand that the AI model needs to have the ability to get focused on

70
00:07:09.030 --> 00:07:20.659
George Churchwell: questions whenever it gets questions. So it needs reference asking chat Gpt. Something about your company is really looking to end up with a hallucination.

71
00:07:20.920 --> 00:07:33.219
George Churchwell: So I want to keep it narrowed in an area. How do I do that? It's a little beyond the scope of this whole area. But I want to give you this word retrieval, augmented generation.

72
00:07:33.300 --> 00:07:37.349
George Churchwell: And the idea here is that we're going to

73
00:07:37.370 --> 00:07:39.789
George Churchwell: create a private knowledge base.

74
00:07:40.340 --> 00:07:46.159
George Churchwell: Now, some of you are going to think. I thought the AI knows everything. It's really just an automation tool.

75
00:07:46.500 --> 00:08:02.410
George Churchwell: We're going to tell it all of the answers and where to find all of the answers to everything your employees are going to use it for, which takes all the mystery out of it, makes it a whole bunch of work, but that's gonna be. Give you the best result.

76
00:08:02.720 --> 00:08:14.669
George Churchwell: So if you have, we've got to document everything, or we've got to pull documentation in. We don't want the AI to go to the Internet when it's asked a question.

77
00:08:14.920 --> 00:08:27.729
George Churchwell: we want the AI and go just jump to its large language model. We'll do that later. We'd rather use our private knowledge base for most of the answers to keep things accurate.

78
00:08:28.270 --> 00:08:30.240
George Churchwell: So also

79
00:08:30.370 --> 00:08:33.210
George Churchwell: we have to give it examples of good

80
00:08:34.110 --> 00:08:38.110
George Churchwell: if you're going to have it generate reports or read reports or do things.

81
00:08:38.350 --> 00:08:40.269
George Churchwell: show it what good looks like.

82
00:08:40.940 --> 00:08:56.530
George Churchwell: So we've got to generate. And so, whatever the best of whatever we're going to use the AI for if it's analyzing business financials show it what a good financial looks like. Show it. What a bad financial looks like

83
00:08:56.720 --> 00:08:59.359
George Churchwell: we've got to go through all of this training.

84
00:09:00.150 --> 00:09:01.120
George Churchwell: Okay?

85
00:09:01.580 --> 00:09:02.560
George Churchwell: Next.

86
00:09:03.710 --> 00:09:09.880
George Churchwell: the AI actually works in tokens, because most all of us probably use chat, gpt.

87
00:09:09.900 --> 00:09:26.110
George Churchwell: We don't even hear about this, but as soon as you start to use this in a corporate sense, and somebody that use chat Gpt may have noticed something. If any, of a really heavy duty users with Chat Gpt, you would have found that some point in the day it shuts down on you

88
00:09:26.760 --> 00:09:29.219
George Churchwell: because you use too many tokens

89
00:09:29.410 --> 00:09:31.049
George Churchwell: for $30.

90
00:09:31.410 --> 00:09:32.470
George Churchwell: So

91
00:09:32.670 --> 00:09:35.460
George Churchwell: in any case, tokens have a cost.

92
00:09:35.790 --> 00:09:39.660
George Churchwell: I'm going to show you just some generic model that I worked up here

93
00:09:39.910 --> 00:09:42.999
George Churchwell: of what it costs to run AI at your company.

94
00:09:43.330 --> 00:09:49.190
George Churchwell: and it could be more or less depending on what you're doing. But if you had Gpt-four

95
00:09:49.540 --> 00:09:55.820
George Churchwell: and you were doing 500 tokens and keep in mind a token for all intents and purposes

96
00:09:56.530 --> 00:10:04.020
George Churchwell: like the word chat, bot might be a single token unbelievable might be 2 tokens. So you'd have to figure out what you're typing.

97
00:10:04.240 --> 00:10:08.020
George Churchwell: But if you type something like this which actually seems pretty simple.

98
00:10:08.690 --> 00:10:10.789
George Churchwell: Your annual cost

99
00:10:11.390 --> 00:10:14.660
George Churchwell: would be about 23,000 per employee.

100
00:10:15.400 --> 00:10:17.940
George Churchwell: That's using chat that's using this.

101
00:10:18.240 --> 00:10:21.930
George Churchwell: Now, I don't depend on your size of your business or so.

102
00:10:22.420 --> 00:10:30.850
George Churchwell: Now the reality, if you stepped it up to what's kind of probably more real? 5 employees using an Nvidia a 100,

103
00:10:31.390 --> 00:10:33.210
George Churchwell: 8 HA day.

104
00:10:33.830 --> 00:10:37.029
George Churchwell: probably $66,000 a year.

105
00:10:38.450 --> 00:10:42.640
George Churchwell: or could be, if your cloud based it costs more 78,000.

106
00:10:43.450 --> 00:10:54.720
David Mantica--Co-host!!!: Yeah, what I would just typed in George. It starts looking like the old Ibm mainframe business model for companies who are playing in it. If I'm giving you access that you're gonna pay for each

107
00:10:55.220 --> 00:10:57.710
David Mantica--Co-host!!!: each person using that access.

108
00:10:57.730 --> 00:11:04.059
David Mantica--Co-host!!!: And that's why they're charging $40 a person. They're hoping like insurance. Not everybody uses it, but if you build it yourself.

109
00:11:04.180 --> 00:11:06.060
David Mantica--Co-host!!!: here comes energy costs.

110
00:11:06.410 --> 00:11:10.120
George Churchwell: It comes down. Yeah, yeah, yeah. And for all of you.

111
00:11:10.380 --> 00:11:20.680
George Churchwell: So now, I don't know if your jaws are dropping. These are just generic models. It could be significantly, more or less for you. But it costs so. Here's the thing

112
00:11:21.510 --> 00:11:32.539
George Churchwell: I know. I talked to a lot of people, a lot of people at work decide they're going to do. AI. I don't know many people. I just got a customer now that their boss said I need a business plan.

113
00:11:32.920 --> 00:11:37.810
George Churchwell: I don't know how many of you at work are doing. AI, but have not created a business plan

114
00:11:38.110 --> 00:11:43.819
George Churchwell: because you maybe thought it was free in the end.

115
00:11:43.840 --> 00:11:49.549
George Churchwell: You probably need to go to. I don't know if you can spend $78,000 a year, and nobody cares.

116
00:11:50.180 --> 00:11:51.719
George Churchwell: but if you can't.

117
00:11:52.020 --> 00:11:56.540
George Churchwell: You would need to go to your boss, and I'm sure your boss is going to immediately say to you.

118
00:11:57.010 --> 00:11:59.059
George Churchwell: what is it you're doing

119
00:11:59.490 --> 00:12:15.409
George Churchwell: that's costing that's reducing the cost by 78,000 a year. And you're gonna have something like this. You're gonna have to show. And say, Hey, before we used to do this. And I'm gonna automate these things. And then you're gonna have to have something that says when I automate that.

120
00:12:16.040 --> 00:12:21.720
George Churchwell: Now here's the catch that comes up in all of these. You're probably not going to lay anybody off.

121
00:12:21.860 --> 00:12:23.779
George Churchwell: This is the irony of this.

122
00:12:23.790 --> 00:12:28.589
George Churchwell: So the boss is going to say so what you're telling me is that my cost is going to go up

123
00:12:28.650 --> 00:12:33.010
George Churchwell: by $78,000 a year. You're using AI.

124
00:12:33.330 --> 00:12:35.470
George Churchwell: You now made things less

125
00:12:35.740 --> 00:12:39.129
George Churchwell: operate faster. So now people have more time to play golf.

126
00:12:40.010 --> 00:12:42.350
George Churchwell: right? You know, it's

127
00:12:42.450 --> 00:12:47.789
George Churchwell: what you're gonna have to say is that's gonna enable us to be able to sell more.

128
00:12:49.232 --> 00:12:51.920
George Churchwell: He increased customer satisfaction.

129
00:12:52.040 --> 00:13:01.589
George Churchwell: Do something. Maybe you can actually reduce an employee headcount or a subcontractor headcount. Possibly. But the point is

130
00:13:02.190 --> 00:13:11.470
George Churchwell: because there are real costs in AI. You probably need, I would expect in most businesses some kind of business plan.

131
00:13:11.730 --> 00:13:13.890
George Churchwell: and that would come into play.

132
00:13:14.670 --> 00:13:15.660
George Churchwell: And

133
00:13:15.870 --> 00:13:21.580
George Churchwell: because it isn't free. And now comes the last part here, as we kind of come to the end.

134
00:13:21.890 --> 00:13:31.079
George Churchwell: the good news is with at least with chat, gpt, and open AI today, and I don't know when somebody else will, instead of you having to deal with this

135
00:13:31.300 --> 00:13:36.209
George Churchwell: kind of stuff which may be where you ultimately end up coding your own server.

136
00:13:36.770 --> 00:13:40.920
George Churchwell: you can very quickly create a proof of concept

137
00:13:41.860 --> 00:13:43.819
George Churchwell: with Chat Gpt

138
00:13:44.010 --> 00:13:46.419
George Churchwell: using a Gpt model.

139
00:13:47.070 --> 00:13:49.389
George Churchwell: And it's very easy.

140
00:13:49.610 --> 00:13:51.110
George Churchwell: In fact.

141
00:13:51.260 --> 00:13:54.630
George Churchwell: that's what I'm gonna share here

142
00:13:54.670 --> 00:13:57.829
George Churchwell: with you. It's very easy to do.

143
00:13:57.890 --> 00:14:02.319
George Churchwell: If we go into one of these models and let me pull up here.

144
00:14:04.620 --> 00:14:06.359
George Churchwell: I'll just show that one

145
00:14:07.650 --> 00:14:09.380
George Churchwell: show you what's inside

146
00:14:11.300 --> 00:14:13.829
George Churchwell: the Gpt. This is the gpt

147
00:14:14.210 --> 00:14:16.230
George Churchwell: so essentially

148
00:14:16.550 --> 00:14:18.419
George Churchwell: to create a Gpt.

149
00:14:18.530 --> 00:14:21.340
George Churchwell: You go in and you hit configure.

150
00:14:21.760 --> 00:14:29.300
George Churchwell: and you type in what you want it to do. This is where a little bit of prompt engineering becomes very useful.

151
00:14:29.560 --> 00:14:34.570
George Churchwell: You describe a description of what it is. And remember, we talked about those knowledge rags.

152
00:14:34.660 --> 00:14:36.559
George Churchwell: You put the rags in here.

153
00:14:37.240 --> 00:14:40.580
George Churchwell: This is the item that contains the document

154
00:14:40.600 --> 00:14:42.969
George Churchwell: that has the information

155
00:14:44.000 --> 00:14:45.080
George Churchwell: here

156
00:14:45.320 --> 00:14:47.049
George Churchwell: about John Smith.

157
00:14:48.150 --> 00:14:50.580
George Churchwell: So you can see John Smith right there.

158
00:14:52.260 --> 00:14:54.309
George Churchwell: These, these are the rules

159
00:14:54.630 --> 00:14:55.640
George Churchwell: of the

160
00:14:56.730 --> 00:14:59.619
George Churchwell: and you would build this

161
00:15:00.230 --> 00:15:02.839
George Churchwell: and then create it and update it

162
00:15:02.910 --> 00:15:04.480
George Churchwell: and move it out.

163
00:15:04.800 --> 00:15:10.700
George Churchwell: And this becomes the tool that your workforce can work with as a powerful proof of concept.

164
00:15:10.830 --> 00:15:14.770
George Churchwell: See, I'm ready here. It says, anyone with the link can use this tool.

165
00:15:14.890 --> 00:15:17.550
George Churchwell: but I can make it restricted also.

166
00:15:17.940 --> 00:15:22.270
George Churchwell: And whatever I think now now, I hit login issues.

167
00:15:23.360 --> 00:15:26.829
George Churchwell: The most powerful thing about this is

168
00:15:27.170 --> 00:15:33.620
George Churchwell: the person using it does not have to know anything about AI,

169
00:15:34.910 --> 00:15:37.900
George Churchwell: because the prompts they're typing here

170
00:15:38.030 --> 00:15:40.330
George Churchwell: are not prompt engineering.

171
00:15:40.840 --> 00:15:45.530
George Churchwell: There are prompts that are just just natural language text.

172
00:15:46.480 --> 00:15:49.310
George Churchwell: You need to build in your

173
00:15:49.340 --> 00:15:51.010
George Churchwell: Gpt

174
00:15:51.610 --> 00:15:54.220
George Churchwell: in your knowledge area

175
00:15:54.290 --> 00:15:57.220
George Churchwell: in this area here.

176
00:15:57.760 --> 00:16:01.159
George Churchwell: how you expect people to be interacting with you.

177
00:16:01.570 --> 00:16:05.029
George Churchwell: and how you want things to be responded to

178
00:16:08.150 --> 00:16:09.900
George Churchwell: any questions or anything.

179
00:16:11.850 --> 00:16:12.952
David Mantica--Co-host!!!: Yeah, we have.

180
00:16:13.520 --> 00:16:21.079
David Mantica--Co-host!!!: We have one question. I keep forgetting to turn on my mic off for the typing. I'm sorry, all right, George George Bridges, you have a question.

181
00:16:21.120 --> 00:16:22.540
David Mantica--Co-host!!!: Are you there still?

182
00:16:22.710 --> 00:16:24.890
George - PMI Emerald Coast Fl: I'm I'm still here. Hey, George!

183
00:16:25.140 --> 00:16:25.840
George Churchwell: Hey!

184
00:16:25.840 --> 00:16:33.589
George - PMI Emerald Coast Fl: I I think you got into the area that I was was interested in, and that was being able to train your own bot.

185
00:16:33.760 --> 00:16:38.609
George - PMI Emerald Coast Fl: I think lot of people are maybe struggling with, or maybe don't even know.

186
00:16:38.610 --> 00:16:40.469
George Churchwell: I'm gonna show you. Why.

187
00:16:41.180 --> 00:16:43.130
George Churchwell: look at your screen. There you are.

188
00:16:43.360 --> 00:16:45.780
George Churchwell: and and this is

189
00:16:46.760 --> 00:16:48.760
George Churchwell: my own.

190
00:16:49.170 --> 00:16:50.200
George Churchwell: But

191
00:16:51.090 --> 00:16:52.090
George Churchwell: you know.

192
00:16:52.090 --> 00:16:53.779
David Mantica--Co-host!!!: Are you going to put it at George.

193
00:16:53.780 --> 00:16:55.450
George Churchwell: Makes me happy.

194
00:16:55.870 --> 00:16:58.580
David Mantica--Co-host!!!: You're gonna put a mute. You're gonna put the things that make you happy.

195
00:17:02.816 --> 00:17:04.630
George Churchwell: Diamond scores.

196
00:17:05.369 --> 00:17:06.650
George Churchwell: Oh, gee

197
00:17:06.670 --> 00:17:08.500
George Churchwell: World Series!

198
00:17:09.180 --> 00:17:11.339
George Churchwell: Let's see if it can go find it.

199
00:17:12.460 --> 00:17:14.895
George Churchwell: And I would normally I'd have to say

200
00:17:17.020 --> 00:17:19.370
George Churchwell: I wonder if it would find this 1st game?

201
00:17:22.680 --> 00:17:24.160
George Churchwell: Let's see if it does

202
00:17:27.569 --> 00:17:32.549
George Churchwell: so while you're building it, you can actually query it. It works.

203
00:17:33.580 --> 00:17:36.040
George Churchwell: So here it's there.

204
00:17:36.220 --> 00:17:38.630
George Churchwell: Did it come up right? Is that the score.

205
00:17:40.810 --> 00:17:41.630
George - PMI Emerald Coast Fl: Okay.

206
00:17:42.090 --> 00:17:43.119
George Churchwell: Was that it?

207
00:17:43.740 --> 00:17:44.970
George Churchwell: I think so.

208
00:17:45.000 --> 00:17:50.240
George Churchwell: Dave did they? Was it? 6? Wait a minute, Dodgers? 6, 3. Yep.

209
00:17:51.260 --> 00:17:52.460
George - PMI Emerald Coast Fl: Okay, that's good.

210
00:17:52.460 --> 00:18:03.239
George Churchwell: But here. So you I really did this really quick. What you what you'd want to do. See here where you upload files, you'd want to upload your files here. So

211
00:18:03.450 --> 00:18:06.749
George Churchwell: okay, I just showed you how to create your world.

212
00:18:07.040 --> 00:18:09.160
George Churchwell: You have a lot of work to do

213
00:18:09.570 --> 00:18:13.310
George Churchwell: because you, you need to write instructions

214
00:18:13.570 --> 00:18:23.159
George Churchwell: that have the ability to manipulate the the information coming in to respond appropriately to whatever it is

215
00:18:23.620 --> 00:18:27.630
George Churchwell: you're trying to do. You know I

216
00:18:27.670 --> 00:18:28.900
George Churchwell: I got lucky

217
00:18:29.010 --> 00:18:30.409
George Churchwell: I made this one

218
00:18:31.850 --> 00:18:34.969
George Churchwell: going to Disney world. I told it that we have

219
00:18:35.836 --> 00:18:36.333
George Churchwell: create

220
00:18:37.450 --> 00:18:38.370
George Churchwell: scheduling.

221
00:18:43.180 --> 00:18:45.130
George Churchwell: I have my grandkids coming.

222
00:18:48.030 --> 00:18:52.660
George Churchwell: Let's see here, and I could tell, and and I could tell it where I'm staying.

223
00:18:53.990 --> 00:18:56.330
George Churchwell: and then I'll say 3,

224
00:18:56.960 --> 00:18:57.660
George Churchwell: 4.

225
00:19:01.360 --> 00:19:08.550
George Churchwell: So so this this one here I told it where the Disney parks are, and it will help me build a schedule.

226
00:19:09.380 --> 00:19:10.909
George - PMI Emerald Coast Fl: Okay. Great. Thank you.

227
00:19:17.530 --> 00:19:18.990
George Churchwell: Question, anything.

228
00:19:21.360 --> 00:19:32.229
George - PMI Emerald Coast Fl: The the whole concept of taking your knowledge, your proprietary knowledge and uploading that that's a that's a thought, too, as well. And I think you're using Chat Gpt plus to do this one.

229
00:19:32.580 --> 00:19:37.129
George - PMI Emerald Coast Fl: But you can do this, I guess, on other other platforms other than Chat gpt.

230
00:19:37.820 --> 00:19:40.220
George Churchwell: So I think so. Here's the catch.

231
00:19:41.312 --> 00:19:43.480
George Churchwell: Chat open. AI

232
00:19:43.870 --> 00:19:45.580
George Churchwell: created this

233
00:19:45.710 --> 00:19:48.509
George Churchwell: thing called. It's right here

234
00:19:48.780 --> 00:19:50.809
George Churchwell: called a a Gpt.

235
00:19:51.190 --> 00:19:59.970
George Churchwell: To my knowledge today, and some of you can correct me. I have not seen any other company create a product that lets you load

236
00:20:00.090 --> 00:20:02.809
George Churchwell: what we what I was calling rag

237
00:20:03.620 --> 00:20:05.349
George Churchwell: this rag thing

238
00:20:05.450 --> 00:20:06.800
George Churchwell: right here.

239
00:20:07.250 --> 00:20:15.040
George Churchwell: this area, letting you load it by just throwing it in and keeping it away from your employees and users.

240
00:20:15.730 --> 00:20:22.540
George Churchwell: because you don't see it. You can't tell what I uploaded in there when you use open AI's product.

241
00:20:22.790 --> 00:20:26.849
George Churchwell: Now, if you went and used Google Gemini

242
00:20:26.910 --> 00:20:34.469
George Churchwell: or anthropic or anything like that. You're certainly welcome to go here and create. See here, I have rules.

243
00:20:35.340 --> 00:20:37.780
George Churchwell: You can use python

244
00:20:38.190 --> 00:20:46.140
George Churchwell: and write code that can manipulate that and generate and load rules in with the same way Openai does.

245
00:20:47.260 --> 00:20:49.249
George Churchwell: But then you have to write in python.

246
00:20:49.790 --> 00:20:54.809
George Churchwell: You know I I don't know. Somebody else needs to come up with something that's competitive with

247
00:20:55.060 --> 00:20:57.789
George Churchwell: chat gpt open AI's

248
00:20:57.940 --> 00:21:00.350
George Churchwell: instant app solution.

249
00:21:01.130 --> 00:21:02.530
George Churchwell: That's what's missing.

250
00:21:05.860 --> 00:21:06.830
George - PMI Emerald Coast Fl: Thank you.

251
00:21:08.430 --> 00:21:10.210
George Churchwell: Any other questions or anything.

252
00:21:10.210 --> 00:21:27.270
David Mantica--Co-host!!!: Let's do this. I mean, it's time for lunch. We have a short lunch, so let's get folks over. George, folks can private chat you if they have any questions or public chat you. But let's let's get folks over. Get a quick bite to eat, get some stretching in and check some emails and work stuff.

253
00:21:27.290 --> 00:21:30.449
David Mantica--Co-host!!!: And, Laura, what time are you supposed to be back? 1230.

254
00:21:30.450 --> 00:21:36.010
Lara Hill: Yes, we're running about 5 min behind schedule. So 1230 will. We will reconvene.

255
00:21:37.030 --> 00:21:39.470
David Mantica--Co-host!!!: Excellent and.

256
00:21:39.820 --> 00:21:48.880
George Churchwell: All right, and if anybody needs any kind of more information, certainly feel frequent, feel free to reach out. David. You should post

257
00:21:48.970 --> 00:21:56.070
George Churchwell: the Gpt, the thing I showed you that gpt soft Skills group has a class

258
00:21:56.110 --> 00:21:57.300
George Churchwell: coming up in November.

259
00:21:57.300 --> 00:21:59.939
David Mantica--Co-host!!!: Try. I'm sorry about that, brother. Let me go ahead and do that. Forgot about that.

260
00:21:59.940 --> 00:22:01.270
George Churchwell: One week class.

261
00:22:01.390 --> 00:22:09.819
George Churchwell: It's so well it shows up as a week. It's virtual, and it's like 3 or 4 HA day, and it. You'll make those.

262
00:22:10.030 --> 00:22:15.080
George Churchwell: You'll come in. Maybe you don't know how to make them. You'll leave on Friday. You can make your own.

263
00:22:16.110 --> 00:22:24.549
Lara Hill: Yes, and I will send an email afterwards that references, the courses that are taught by some of the speakers that

264
00:22:24.550 --> 00:22:29.879
Lara Hill: correlate to the presentation. George teaches our purpose-built Gpt course

265
00:22:29.880 --> 00:22:54.880
Lara Hill: at Skills Development group. There's a couple other courses that are really good follow ups if you're wanting to dive deeper into certain topics. So I'll be sure and reference all of that in the email follow up as well. And I think David's grabbing the link. He just put it in the chat so definitely. Check that out. If you're interested in building a custom. Gpt, George is the go to person for that, and we have some public course dates coming up that you can see

266
00:22:55.080 --> 00:23:10.909
Lara Hill: if you follow that link, so feel free to stick around and chat if you want, or take a break and come back. You can just leave your session running if you like. I'm going to pause the recording, though, so that we can come back at 1230.

267
00:23:10.910 --> 00:23:12.660
David Mantica--Co-host!!!: Maybe you shouldn't be.

268
00:23:13.175 --> 00:23:14.719
Cris Casey: Well, it it.

269
00:23:15.350 --> 00:23:15.980
Michael Wolf: Okay.

270
00:23:15.980 --> 00:23:17.150
Cris Casey: Yeah, well, I just.

271
00:23:17.150 --> 00:23:18.780
Lara Hill: Record. Just so everyone knows.

272
00:23:18.780 --> 00:23:19.980
Michael Wolf: Because okay.

273
00:23:19.980 --> 00:23:21.669
Lara Hill: Capture some of this, but go ahead.

274
00:23:21.670 --> 00:23:27.730
Cris Casey: So AI renders the notion of an expert irrelevant.

275
00:23:27.730 --> 00:23:29.480
David Mantica--Co-host!!!: That's what I'm trying to say.

276
00:23:29.480 --> 00:23:29.870
Cris Casey: Yes.

277
00:23:29.870 --> 00:23:30.270
Michael Wolf: No.

278
00:23:30.270 --> 00:23:31.200
George Churchwell: Oh, no! But it.

279
00:23:31.200 --> 00:23:37.470
Cris Casey: That's exact. Well, hold on, and but this is on a continuum, right? So

280
00:23:37.620 --> 00:23:38.560
Cris Casey: it's.

281
00:23:38.610 --> 00:24:02.659
Cris Casey: And when you're adding new things. So, for example, you've got molecular engineers out there who are creating these incredibly small things, and they are using AI to say, How do I make this better? Here are my constraints. Here's what. So while they are experts and understand exactly what questions to ask, the application of the

282
00:24:02.660 --> 00:24:10.155
Cris Casey: AI or machine learning in these cases, right is actually doing all the work. So it's like when when

283
00:24:11.330 --> 00:24:16.949
Cris Casey: when Google's, when Deepmind came out and they said, Hey, we figured out how proteins fold

284
00:24:17.350 --> 00:24:18.330
Cris Casey: well.

285
00:24:18.570 --> 00:24:21.480
Cris Casey: your expert. There is a Phd.

286
00:24:21.500 --> 00:24:25.230
Cris Casey: And that Phd. Takes 5 years

287
00:24:25.460 --> 00:24:32.259
Cris Casey: by himself or herself to essentially figure out how a protein folds 5 years

288
00:24:32.460 --> 00:24:34.029
Cris Casey: one Phd.

289
00:24:34.730 --> 00:24:43.989
Cris Casey: Less than 18 months later Deepmind comes out and says, Hey, all you guys who need to know about protein folding, which is anything in the medical

290
00:24:44.190 --> 00:24:46.300
Cris Casey: and pharmaceutical area.

291
00:24:46.500 --> 00:24:56.050
Cris Casey: It's here are 200,000 proteins that we've unfolded for you. Here's the library.

292
00:24:56.080 --> 00:25:02.780
Cris Casey: So you take 200,000 protein folds. You multiply that times 5 years.

293
00:25:02.900 --> 00:25:06.800
Cris Casey: Well, that's a million Phd. Years

294
00:25:06.970 --> 00:25:09.980
Cris Casey: that have now been in in the

295
00:25:10.030 --> 00:25:11.720
Cris Casey: in the blink of an eye.

296
00:25:11.720 --> 00:25:13.279
George Churchwell: It's good, though, right.

297
00:25:13.280 --> 00:25:13.930
Michael Wolf: They're standing.

298
00:25:13.930 --> 00:25:15.709
Cris Casey: This is not a.

299
00:25:15.710 --> 00:25:16.100
Michael Wolf: Year.

300
00:25:16.100 --> 00:25:19.279
Cris Casey: This is a question of not good or bad.

301
00:25:19.280 --> 00:25:20.279
George Churchwell: I also think.

302
00:25:20.280 --> 00:25:20.910
Cris Casey: Just Whatsapp.

303
00:25:20.910 --> 00:25:25.060
David Mantica--Co-host!!!: It's it's bad, based on our current economic structure, I would tell you, based.

304
00:25:25.060 --> 00:25:25.770
George Churchwell: There you go!

305
00:25:25.770 --> 00:25:27.770
Cris Casey: And social structure. Yeah.

306
00:25:27.770 --> 00:25:41.550
George Churchwell: I want to add. Now, this is a hard thing, maybe, for everyone. But with that, whoever's expert at something, whatever they're expert at, whether it's law. Whatever you're expert at doing professionally.

307
00:25:41.880 --> 00:25:48.190
George Churchwell: you'll learn that you can get your value to an AI is to give that knowledge to it right?

308
00:25:48.330 --> 00:26:07.229
George Churchwell: But that's gonna keep that changes to you. Your services would have. It's not static. It's not like you dump it in, and it's over because the ever changing edge cases are there. So probably for most people that are really good at something, they could probably stay employed for 4 or 5 more years.

309
00:26:07.702 --> 00:26:10.929
George Churchwell: I don't know that that's good or bad, but for.

310
00:26:10.930 --> 00:26:12.480
Cris Casey: You're optimistic.

311
00:26:12.480 --> 00:26:21.900
George Churchwell: No, because the AI needs constant reinforcement. But but here, you want to know, David, I'll give you a crazy one. I'm not going to tell you who this is.

312
00:26:22.220 --> 00:26:44.909
George Churchwell: We were showing a Gpt. To a company and saying that its customers would really benefit from this, and they were saying, wow! This is incredible that you can do all this stuff with a Gpt. We sell services right now in the billions of dollars. They buy contracts from us to support them. This looks like it does all that. How much is this? And we said, Well, it doesn't cost really anything.

313
00:26:45.320 --> 00:26:48.370
George Churchwell: And they said, Don't show this to any customers.

314
00:26:48.370 --> 00:26:49.080
David Mantica--Co-host!!!: Yeah.

315
00:26:49.080 --> 00:26:50.119
George Churchwell: They told us.

316
00:26:50.120 --> 00:26:52.330
David Mantica--Co-host!!!: I'm telling you the disruption is.

317
00:26:52.330 --> 00:26:54.020
Michael Wolf: Talking value versus cost.

318
00:26:54.020 --> 00:27:02.519
David Mantica--Co-host!!!: Yeah, yeah, Michael, we're just not there. It's going to happen. And the disruption is going to be.

319
00:27:02.520 --> 00:27:02.930
George Churchwell: No.

320
00:27:03.080 --> 00:27:25.890
David Mantica--Co-host!!!: If I'm a really yes, exactly. But it's going to happen faster than what happened, Kodak. But here's the thing. If I'm a really good project manager. You're going to need project managers. So if I in project change, so really, a project manager can then use AI to continue to do their job. And because projects change and adjust, the AI really can't take over for them, it could just make them.

321
00:27:25.890 --> 00:27:38.039
George Churchwell: We can't. We're we're all of the 100% front facing stuff. And you could see something with air. Canada. If you go look it up with an AI. Their AI told the passenger on a bereavement to buy a ticket.

322
00:27:38.160 --> 00:27:42.299
George Churchwell: and then air. Canada said, It's not right. And and they went to court

323
00:27:42.340 --> 00:27:50.130
George Churchwell: and air. Canada said that wasn't an employee of ours. It was an it program, and the court said it came from your site. It's right.

324
00:27:50.130 --> 00:28:03.749
David Mantica--Co-host!!!: Yes, I agree with that. They own the responsibility because the AI was created by you. So it's really an employee of yours, especially if it's a large language model that you created, that's that's to me that should be precedent.

325
00:28:03.940 --> 00:28:08.680
Loyd Thompson: I have a question, George. Wait one second. I have a. I have a question

326
00:28:08.970 --> 00:28:16.699
Loyd Thompson: for this group of people in this conversation, because we all look to be old enough to know at least part of what I'm going to point out.

327
00:28:17.270 --> 00:28:22.830
Loyd Thompson: I mentioned at the start of this, that when I started with computers. It was Ibm card punch.

328
00:28:23.860 --> 00:28:30.740
Loyd Thompson: right? When I started when I graduated from high school accounting was done on double entry ledger paper

329
00:28:31.790 --> 00:28:35.570
Loyd Thompson: right there. There weren't computerized systems handling all that

330
00:28:35.850 --> 00:28:39.300
Loyd Thompson: I was there for Visicalc and then excel.

331
00:28:39.800 --> 00:28:45.229
Loyd Thompson: I remember walking into buildings that had rooms full of bookkeepers and accountants.

332
00:28:45.230 --> 00:28:45.740
George Churchwell: Yeah.

333
00:28:46.250 --> 00:28:56.839
Loyd Thompson: It was soon replaced with a couple of people or a handful of people with personal computers or with node attached mainframe terminals.

334
00:28:57.140 --> 00:29:01.120
Loyd Thompson: and using computers to do the work.

335
00:29:01.890 --> 00:29:03.720
Loyd Thompson: But we all know

336
00:29:03.740 --> 00:29:07.540
Loyd Thompson: that all of those people who lost those jobs

337
00:29:07.640 --> 00:29:18.540
Loyd Thompson: went on to get other jobs. Some of them became software developers. Some of them became experts with Microsoft. Excel some. You know, it evolved just like today.

338
00:29:19.490 --> 00:29:24.000
Loyd Thompson: This AI thing has been going on for decades.

339
00:29:24.120 --> 00:29:49.850
Loyd Thompson: It's only in the past couple of years that it reached the point where, through an Llm. The general population could be exposed to it. Use it for some kind of value. And most people would say that 99% of the stuff being done with AI today is still playing with it. It's not driving value. The value is not being driven for the enterprises that have invested billions of dollars in it. Not yet.

340
00:29:50.460 --> 00:29:51.920
Loyd Thompson: And so I think.

341
00:29:51.920 --> 00:29:54.819
David Mantica--Co-host!!!: The value the Lloyd. The 1st value is gonna be in cutting headcount.

342
00:29:54.820 --> 00:29:55.870
George Churchwell: But.

343
00:29:56.920 --> 00:29:59.790
Loyd Thompson: And they're doing that. Yeah, David, that is what's happening.

344
00:29:59.810 --> 00:30:05.070
Loyd Thompson: The head count is not going. They're not going to fall over dead because they lost their job.

345
00:30:05.600 --> 00:30:08.800
George Churchwell: Yeah. And, David, I gotta add one thing.

346
00:30:11.720 --> 00:30:12.750
Loyd Thompson: Go ahead!

347
00:30:12.750 --> 00:30:13.659
Michael Wolf: Hi! George!

348
00:30:13.660 --> 00:30:17.379
George Churchwell: Well, I was, gonna say, our perception, because of what we see

349
00:30:17.620 --> 00:30:27.050
George Churchwell: is is limiting us. And I just want to throw something out. We just got a contract, our company to start working on certifications for Moon construction.

350
00:30:27.970 --> 00:30:42.709
David Mantica--Co-host!!!: Yeah, you know, that's George. Here's a great, that's a great point. Like we are always in a box. We cannot visualize what's going to happen next. We can. And and that could be it. Maybe the new job is going to be welding on welding in 0 gravity.

351
00:30:42.720 --> 00:30:46.579
David Mantica--Co-host!!!: I know the but what concerns me right now is

352
00:30:47.000 --> 00:31:09.450
David Mantica--Co-host!!!: part of the reason Google had such great numbers is they did some layoffs that nobody knew about. They cut staff. They focused more on using AI for productivity. The productivity numbers increased. Thus they profit, numbers increase. So this 1st wave is going to be tied to people using AI at their work becoming more productive, and people being laid off

353
00:31:09.680 --> 00:31:12.009
David Mantica--Co-host!!!: so they can make more profit. That's my belief.

354
00:31:12.240 --> 00:31:19.939
David Mantica--Co-host!!!: Now where the second wave comes like, Lloyd was saying, as it trades to creating enterprise value. I don't know yet, at least I don't see it.

355
00:31:19.940 --> 00:31:25.339
Cris Casey: Look up. Curzwell's the law of accelerating returns.

356
00:31:25.360 --> 00:31:36.230
Cris Casey: and he explains very, very succinctly why AI is not like any type of industrial or automation

357
00:31:36.960 --> 00:31:51.169
Cris Casey: advancements that we've seen over the life of of humans. And he breaks this out, you know, starting at an epoch level back from the Big bang and runs it all the way out.

358
00:31:51.190 --> 00:32:06.830
Cris Casey: and his point is is that you know, if you make a better jet engine well, he an example that he uses is the amount of time that it takes to get from North America to Europe right, and it collapses down

359
00:32:06.910 --> 00:32:31.349
Cris Casey: where you had the Concorde which could make the trip in a couple hours, but the Concorde was too expensive to operate, and nobody wanted it. So now the level has dropped back down to about 7 h. And the reason why there's been no further investment in terms of trying to bring that time down is because jet engines and that technology. It doesn't build on itself

360
00:32:31.700 --> 00:32:43.380
Cris Casey: the way AI does. And AI right you. You keep feeding it back. You keep refining it, as George pointed out. And this is behind him.

361
00:32:43.380 --> 00:32:50.090
George Churchwell: Chris. Chris, I'll add something to the all the things you're bringing up. And, David, this goes with what you were saying.

362
00:32:50.600 --> 00:32:57.930
George Churchwell: We don't. We can't get supersonic right now. We don't go to space like everybody doing stuff. We are not on the moon.

363
00:32:57.970 --> 00:32:59.650
George Churchwell: If you had AI

364
00:32:59.660 --> 00:33:03.290
George Churchwell: and you just put into AI. Here's the Concord's design

365
00:33:03.690 --> 00:33:06.300
George Churchwell: design, a model that uses less fuel.

366
00:33:06.380 --> 00:33:10.510
George Churchwell: Change the engine. What's wrong with this engine? Make the engine better

367
00:33:10.950 --> 00:33:14.329
George Churchwell: those types of things the AI can do.

368
00:33:14.480 --> 00:33:24.349
George Churchwell: and those are things we can't do. You still need engineers and stuff. And also now we're making new things that we never made before, and all those people that used to sit at a cubicle

369
00:33:24.750 --> 00:33:27.699
George Churchwell: are now working in space.

370
00:33:27.990 --> 00:33:31.629
David Mantica--Co-host!!!: Yeah, there's somebody. So this is good. We gotta stop here. We gotta go from being

371
00:33:32.120 --> 00:33:32.530
David Mantica--Co-host!!!: cereal.

372
00:33:32.900 --> 00:33:49.489
David Mantica--Co-host!!!: So going back to Earth. So we're going to come back to Earth. We're back in the real world. We are going to talk about real world application with Mr. Chris Knotts, a great friend of mine, somebody who is knee deep and using AI for project work.

373
00:33:49.720 --> 00:33:58.539
David Mantica--Co-host!!!: Chris, we're going to get you. You're bringing us back to Earth. Brother, I know you love this ethereal conversation, too. But I need you back at Earth, giving us how we make things happen.

374
00:33:58.540 --> 00:34:28.309
Chris Knotts: Hey? Thanks for that. Tee up, David. You know I love the philosophical angles as much as anybody, and that is important, I should say. But you know, when we are dealing with these generative AI tools in our daily work, we've you know, we've got to reorient back to what the heck, can we do daily? Right? So thanks, David, that was perfect. Tee up. Can everybody hear me? Okay.

375
00:34:28.310 --> 00:34:30.150
David Mantica--Co-host!!!: Yeah, you hear, you great, sound, fantastic.

376
00:34:30.159 --> 00:34:36.179
Chris Knotts: And then let's see if we can have a little bit of screen sharing action here.

377
00:34:36.589 --> 00:34:38.069
Chris Knotts: Can everybody see that.

378
00:34:38.679 --> 00:34:39.689
David Mantica--Co-host!!!: Yes, we do.

379
00:34:40.170 --> 00:35:09.069
Chris Knotts: Great. So let's talk about exactly what David just referred to here. How do we apply the tools so? And you know I love the reference to Ray Kurzweil and the singularity books would love Chris to have a conversation about that offline or something. Please hit me up. We can have a beer, but we're in the world of going to work every day trying to get work done. And I just want to say I love the

380
00:35:09.070 --> 00:35:34.030
Chris Knotts: topics that I've heard to date. In particular, I want to give a shout out to Abrar's topic on data governance. That is a critical topic, and some of the earlier conversations about some of the caveats and the points about focusing on where your human value comes in. These are such critical points, but these are some of the things that I'm going to cover. Right

381
00:35:34.030 --> 00:36:00.369
Chris Knotts: number one here, you can see. I want to talk a little bit about the disruption of AI. As David said, not from this philosophical level, but from as pragmatic of a level as I could. So and I'll just, I'll share sort of what the orientation was as I was putting together this talk. The goal here for this presentation is, you know how in a 45 min talk, how can I present as much like

382
00:36:00.370 --> 00:36:24.390
Chris Knotts: practical pragmatic, immediately applicable AI usage as I can. And so I want to cram it all in. So I'm going to do a bunch of demos. I'm going to explain a few of the key sort of frames that I use for this, as I have done an increasing amount of work with Gen. AI just in everyday professional work over the last couple of years.

383
00:36:24.390 --> 00:36:34.999
Chris Knotts: I'll share a couple of frames for that. But then I'm going to do a bunch of demos and actually show you what I'm talking about here, and then I'll wrap up with a little bit of some other pragmatic

384
00:36:35.000 --> 00:36:54.159
Chris Knotts: concerns that aren't exactly about using the tool on an individual level. But you know, using the tool and and getting allowed to use it kind of from a policy standpoint and things like that. Some some people have talked on that George made a bunch of good points about that in his talk, and I've got a few things to share with you there. Right? So

385
00:36:54.940 --> 00:37:19.850
Chris Knotts: I'll start with this sort of alarming sounding slide from Gartner, in which they say that by 2030. They claim 4 fifths of the project. Management work is done today will be automated away by AI. Right? So now I will say, I'm not really sure that I agree with this exactly. And certainly I think there's more nuance here, even if some of this is true, I do agree with the fact that your biggest job

386
00:37:19.850 --> 00:37:43.829
Chris Knotts: threats are going to come from up and coming project managers in particular, maybe like a project, managers of a younger cohort who may not have the experience with project management program management that some of us do. But you know, if they're ready to roll with the generative AI tools and they get paid half as much. Then that is certainly a career threat for some of us, so we can't.

387
00:37:43.830 --> 00:38:08.649
Chris Knotts: We can't ignore that. Then the other thing I'll point out here is that this is from 2019 right now. So that's important, because that's before Chatgpt and all the generative AI tools even blew up right? So Gartner is saying this about project management work, claiming that within just a little over 5 years from now most project management work will be severely disrupted. But this was just with the legacy AI

388
00:38:08.650 --> 00:38:31.100
Chris Knotts: that came along before we had all of these really easy to use generative AI tools, the private Gpts and the large language models. Those things were still sort of behind the scenes in the lab. You know, these guys are just talking about kind of like retail machine learning and things like that. Right? So then, like a year or 2 after this, then, you had Chat Gpt, blow up

389
00:38:31.415 --> 00:38:41.520
Chris Knotts: so the disruption is being predicted at broad scale. But let's think about that disruption for just a minute. Right? Because this is the big point that I want to make

390
00:38:43.280 --> 00:39:07.929
Chris Knotts: You know, there is obviously like world shaking disruption happening moon construction. Or you know, whatever, George, you know, if you need a guest lecture on the moon hit me up. But you know, as I've worked with these tools more and more, and taught classes on this stuff, the thing that I've realized is that the tools seem so amazing, and they are, you know, they are quite

391
00:39:07.930 --> 00:39:18.909
Chris Knotts: amazing. But it's easy to get distracted by letting the imagination run wild with all of the earth, shattering disruptive things that these tools can do.

392
00:39:18.970 --> 00:39:43.859
Chris Knotts: And the thing that I've realized as I have worked more with generative AI. Is that the generative AI tools start to struggle fairly quickly, and we've already seen some of that stuff touched on right. But if you get into a sophisticated use case or complicated things very contextually, rich situations. The AI begins to make mistakes, begins to struggle.

393
00:39:43.860 --> 00:39:58.519
Chris Knotts: It'll let you down right. And so you have to be really careful. And then there's a whole other layer of work around the governance and fact checking the AI and kind of the AI safety of policy and all that right? So what I've realized really is that when we say generative AI,

394
00:39:58.560 --> 00:40:07.790
Chris Knotts: one of the most, if not the most, valuable application of the AI tools is actually not in generating things at all.

395
00:40:07.870 --> 00:40:35.739
Chris Knotts: not in creating something from scratch. The AI doesn't just have the ability to generate stuff. The AI has the ability to ingest existing stuff and then manipulate it in different ways to apply cognition to it in kind of a small, a automated way. But we're not really asking it to cook up the stuff from scratch data sets and documentation and things like that. And of course it can do that

396
00:40:35.740 --> 00:40:54.740
Chris Knotts: generative AI is happy to crank out B minus or C plus grade content all day long. But there's still a lot of work that's required there. Once it has drafted that stuff for you. But if you give the AI human generated material to work with, then you'll find it actually does much better, because we're not rely

397
00:40:55.560 --> 00:41:11.439
Chris Knotts: relying on the AI to things in a vacuum. We're actually giving it things that already have the contextually rich points and the nuance and the situational things that content is generated essentially by humans. And so I want to talk a little bit about that.

398
00:41:11.730 --> 00:41:35.880
Chris Knotts: Now, this is just my one slide is in terms of kind of basic Llm tech function. I'm not going to spend a ton of time on this. But there's 1 point down here that I want to make in red. Right? So like, we are probably somewhat familiar with the concept of the large language model here. But there's so actually a couple of points, and then one main one in red. Right? So the 1st point that I want

399
00:41:35.880 --> 00:42:01.289
Chris Knotts: make is that of course, the ais, actually don't understand anything that can be easy to forget, especially when we play with the really cool models that seem to to do amazing things. And all this stuff. But you know, the ais fundamentally are doing are they're just analyzing statistical relationships in words, phrases, and language. So it's just kind of it's taking. It's taking words. It's turning them into math

400
00:42:01.649 --> 00:42:08.779
Chris Knotts: and then it's turning the math back into words. And it's just a statistical analyzer. Right? So so that

401
00:42:08.780 --> 00:42:33.000
Chris Knotts: means that the AI is good at some stuff and not good at some stuff. Yeah, Laura, exactly fanciest autocomplete ever. Right? So. But if you think about how that fundamental operation works? It means that basically number one, the AI is going to be best at tasks that are language based. Right? So that's an important thing to keep in mind now. They can do some math.

402
00:42:33.000 --> 00:42:40.129
Chris Knotts: They can do some data analysis and things like that. But but you always want to start with the use case. That's oriented around language.

403
00:42:40.190 --> 00:43:04.969
Chris Knotts: because that's kind of the fundamental way that the Ll and models are built. And then down here in red here. This is like a really important point that I like to make that the people who are going to be really good using these tools as we move forward, are not software engineers or programmers or data analysts? Really, even the people that actually become power users with AI are the people who have the

404
00:43:04.970 --> 00:43:15.550
Chris Knotts: biggest command of disciplined, rigorous language skills. How good of a communicator are you right? So I always think about somebody who studied

405
00:43:15.560 --> 00:43:22.970
Chris Knotts: like linguistics or philosophy in college. What do those people learn how to do they use? They learn how to use the language.

406
00:43:22.970 --> 00:43:46.959
Chris Knotts: to have a very structured, very disciplined exchange a dialectic, as I call it, and they learned to use that dialectic language pattern to kind of arrive at a conclusion, or to solve a problem or to come to agreement or to analyze the problems with an argument. Things like that. And that's exactly the type of skills you need. You don't need any software engineering skills to use chat, Gpt.

407
00:43:46.960 --> 00:44:02.189
Chris Knotts: But the better you are at those types of language skills, the better of an AI user. You're going to be now that unlocks a complete realm of possibility to professionals outside of these these technology topics. So that's the point that I want to make here about the Llm.

408
00:44:02.660 --> 00:44:26.859
Chris Knotts: So as I'm thinking about, how can we use this in our work? You know, we heard in the morning earlier about how really you know the best. I think it was alluded to. The best work to have AI do is the boring work. It's the repetitive work, the work that essentially, machines are good at doing. And that's another thing to remember. Right. Like at the end of the day, AI is just a computer

409
00:44:26.860 --> 00:44:47.630
Chris Knotts: program. It's like a robot right? And it has the same attributes as a robot or a machine does. And so it's useful to think about, how do we use machines and robots to help us do our work up to this point right? And you can kind of see I've got these columns here, and what you can see is if you compare the strengths and the weaknesses of machines and robots versus people.

410
00:44:47.630 --> 00:45:07.390
Chris Knotts: they're kind of complementary to each other, and they're also quite different. Right. So like the things that machines are good at doing are the things that people hate doing, you know, doing the same task a thousand times doing it the same way, every time not getting tired of doing something at large amounts of scale.

411
00:45:07.390 --> 00:45:31.639
Chris Knotts: These are things that humans aren't good at. And we really don't enjoy right, but we do bring some strengths as well that machines are not good at right? So you know, the AI has no emotional intelligence. It has no ability to work in different communication styles. It has some lowercase C creativity, but in terms of true

412
00:45:31.640 --> 00:45:56.380
Chris Knotts: problem solving. You know, you're not going to get that from the AI and keep in mind the AI's training data has a cutoff point. And right now I think the cutoff point for the most advanced AI models is somewhere in 2022, right? So you can't really innovate if you're using a 2 year old training data set because the AI doesn't know what's been going on in the world for the last 2 years. So you know. So this is how I like to think about

413
00:45:56.380 --> 00:46:24.599
Chris Knotts: this right if you and then I'm going to come back to my 1st slide about the Disruption and take issue somewhat with the Gartner point. That's the real disruption, right. The disruption is not in the exotic earth, shattering world changing stuff. The AI may or may not be able to do moon construction. I can't get past that one. But the the real disruption is the fact that we now have a cognitive robot who can do a lot of the boring, repetitive.

414
00:46:24.600 --> 00:46:53.289
Chris Knotts: busy work that we have to do when we are doing project management work. And there's a lot of that work right? Project managers have to do a lot of data merges. We have to do a lot of documentation. You know, we have to do a lot of stuff that it's not. You don't really have to be that smart to do it. But you have to be smart enough right? And now, with the AI tools, we actually have a tool here that is smart enough to do a lot of this stuff right? So think about that. Think about the differences between machines and people.

415
00:46:54.159 --> 00:47:18.800
Chris Knotts: So what can it help us do right like getting back to the pragmatic right? So if you think about what the robots can do. Well, here's a short list, certainly not an exhaustive list. But it's enough to kind of get you thinking categories of use case routine stuff data, intensive tasks, things that are standardized. You're doing over and over and over things where you have like a fairly inherent level of stability in the context. And there's not a lot of change

416
00:47:18.800 --> 00:47:43.260
Chris Knotts: that you don't have to adapt to or show a lot of agility. Robots and machines are kind of inherently brittle, you know, in terms of their use case they don't really understand task switching context switching, you know, reasonable tolerance for air, large scale tasks, a lot of tasks a thousand times 1 million times. And then finally, just integration, ease like, is it a good fit? I mean, you can have a great tool, but if it doesn't play well with the rest of your stack.

417
00:47:43.260 --> 00:47:55.839
Chris Knotts: then, you know, it might just might not be a good use case, even if the tool is awesome. So these are categories of Use case. But let's think about some specific examples. That fit into these categories. All right. So.

418
00:47:56.116 --> 00:48:12.409
Chris Knotts: this is just a little calculator from my website to allow you to to to figure out. Is it worth investing the time and an automation solution to to automate something? So I won't go through this right now, but you can see like if you were to put in like a 5. Say, do a task

419
00:48:12.410 --> 00:48:37.369
Chris Knotts: 5 min. It takes 5 min, and you do it 5 times a day, and you want to see what the payback period is over the course of a year. If you run a calculation like this, what you can see is basically you're going to get return on your time after one year. If you invest up to like 70 h of development work in doing an Api integration or building a widget that will allow you to now automate the thing.

420
00:48:37.370 --> 00:48:49.360
Chris Knotts: even if it's a very small boring thing. If you do that small boring thing a thousand times a year. It's worth standing up a project to get a development team to spend, you know, 70, 80 h

421
00:48:49.360 --> 00:49:14.270
Chris Knotts: to build an automation widget for that thing, because then you've just gotten payback on your time. So just thinking about, how do we gauge payback periods and Roi on automation tools when we're dealing with this kind of busy, busy work. Right? So now, this is a screenshot from one of my generative AI and project management classes where we sort of brainstorm some of the tasks and use cases that

422
00:49:14.270 --> 00:49:29.079
Chris Knotts: we use. And so what you can see from this screenshot is that project. Managers don't have any problem. And we this takes 5 min. Right? We take 5 min to brainstorm these, and you can see that the participants in the class

423
00:49:29.080 --> 00:49:52.659
Chris Knotts: find it very easy to come up with quite a lot of ideas around the things that are good fits for the machine to to do for you right? And so if you're in a Pmo or your project program manager, you know, most of these probably are pretty familiar. You know, we do a lot of work like this. A lot of this work is fairly mechanistic. It takes some intelligence, some more, some less. But

424
00:49:52.985 --> 00:50:17.389
Chris Knotts: but there are. There are criteria around what machines can do for us that fit? Well with all this kind of stuff, right? So we had somebody mentioned summarizing the meeting notes earlier, you know. You'll get this Gpt. At the end of the conference here that that contains all of the knowledge base of what was discussed today, and then you have something very fast that you can access. So plenty of

425
00:50:17.390 --> 00:50:20.119
Chris Knotts: low hanging fruit in terms of use cases here.

426
00:50:20.350 --> 00:50:43.859
Chris Knotts: So you know, just summing this up into a specific list targeted at project managers and people who do the type of work that we do. Here's a list, right? I won't read you the list, but you can look at this, and you can see that this is the type of work that we have to do as project managers. Every day we spend a lot of our time doing this. We spend a lot of time

427
00:50:43.860 --> 00:51:08.309
Chris Knotts: communicating, summarizing, identifying action items. We spend a lot of time calculating, scheduling and budgets and and things like this that are. There's a lot of data there. There's probably also a lot of examples there from previous projects, or maybe a lot of existing documentation. So you see again, I'm bringing this back to the point that you know we're not. We're not asking the AI to

428
00:51:08.310 --> 00:51:10.460
Chris Knotts: cook up all this stuff from scratch.

429
00:51:10.460 --> 00:51:34.350
Chris Knotts: We need to do work with the the assets that we have the documentation, the schedules, the timeframes the budgets. The meeting transcripts, you know, etc, etc. Contracts even. But we have that stuff, and we can give the AI that stuff and then ask it to manipulate it, analyze it, slice it and dice it in different ways. And I've just. I've found that the

430
00:51:34.350 --> 00:51:57.980
Chris Knotts: the AI is, you know, instead of generating like C plus or B, minus content from scratch, it can get into like a territory when you're asking it to work with a human generated artifact like a spreadsheet that we already have or documentation we already have, or something like that. So that's kind of one of the big points that I want to make here. You know, I think about generative AI,

431
00:51:57.980 --> 00:52:06.670
Chris Knotts: not always so much as generating, but like is manipulating and and analyzing and parsing stuff. The AI can do that, and it's quite good at it.

432
00:52:06.880 --> 00:52:31.249
Chris Knotts: So just a quick slide here in terms of where we can pull information from where we can. We pull material from for the the AI to work with all kinds of places, right? We can pull it from our our project, tracking environment from our our software dev environment, flat files or just any kind of documentation, anything that sits in sharepoint or teams. Anything like that

433
00:52:31.250 --> 00:52:54.449
Chris Knotts: exported data from Trello boards, or what have you? You know you can pull that into a Gpt like George was talking about, or you can pull it in just directly into the AI tool, or you can pull it into a private Llm. There's all kinds of ways. You can pull it into this reference data set layer. But then you can use the the same type of chat ux that we are used to with Chat Gpt, or

434
00:52:54.450 --> 00:53:14.629
Chris Knotts: or Gemini, or Claude, or any of those to work with this type of stuff. But the key thing here again is it's not cooking it up from scratch. We are giving it data sources. And in a project management context in particular, that is quite exciting. Right? So I'm going to switch over now and do some of these demos to kind of show you what I'm talking about. So the 1st demo I'm going to do is called Ports of the World.

435
00:53:14.790 --> 00:53:34.069
Chris Knotts: So I've been working on a project earlier this year. That is, it's for a large maritime Services company. And I've had a bunch of research and stuff that I've had to do with this project. And so, as part of my research I came upon, I had to do a bunch of data compilation here. So

436
00:53:34.070 --> 00:53:58.499
Chris Knotts: I've found some very valuable data from the CIA. Interestingly, the CIA World Fact book has a repository here of every single port in the entire world. So I'll just scroll through this right. So from A to Z. Every single country, every port, every natural gas port, every oil, port.

437
00:53:58.500 --> 00:54:26.929
Chris Knotts: cargo, ports, small ports, big ports, river ports, deep water ports, etc, right? Like it's all here. This is all the information that I need. But there's a problem with this makes it really hard to work with for me. Right? So the problem is, it's just a big wall of text. It's not really. I mean, it's organized. But it's not structured in the way that I need. So what I did with this piece of my research was come over here. I've got these all preloaded for you here already. So what I did was I came over to my

438
00:54:27.370 --> 00:54:47.940
Chris Knotts: my chat, gpt enterprise account here and I I designed a little a couple of prompts here to and you'll see. I I specifically called out the fact. You're not supposed to generate, create anything here or add anything new, right? You're just supposed to manipulate and organize the information that I give you.

439
00:54:47.940 --> 00:55:14.399
Chris Knotts: So I teed it up that way. And then, you know, I kind of explained what I was gonna do right? I gave it a little bit of a example, tinkered with it a little bit to get it working right? And then, once I had it working right, I was able to just copy and paste that wall of text information about the ports and start organizing it right? And so you can see how this worked. I just started dumping the data in there and then I started getting the structured information.

440
00:55:14.410 --> 00:55:23.279
Chris Knotts: and I'll just skip to the end and show you what I ended up with. So what I ended up with was, after completing. This was something that looked like this. But I didn't just

441
00:55:23.280 --> 00:55:47.759
Chris Knotts: put the information into a tabular format here. I did use the the cognitive ability of the AI to to add some information that it could. It could derive from the original data set. But it wasn't in the data set. And specifically, I'm talking about counts right? So it had all these lists, but I needed numbers. How many container ports are there? How many LNG ports, how many dry bulk ports, right? So.

442
00:55:47.760 --> 00:56:08.649
Chris Knotts: as it compiled the data from the wall of text into the spreadsheet, it was able to generate new columns here that gave me all these counts, so that I had now, like a very granular data set that I could work with. And then, now that this is in a Csv, I have a lot more options to to work with this data. Right? So and then

443
00:56:08.730 --> 00:56:32.280
Chris Knotts: I also kind of again, it's not really hard. I'm just asking it to count. You know, it never really had to count higher than 20. So that's a perfect math. Use case for generative AI, you know, as a counting that's not going to be above above 20. But then so you can. You can just imagine right how this would be very useful. And you can use this kind of thing for all kind of stuff scraping data and then putting it into a spreadsheet like this. Right? So boom!

444
00:56:32.280 --> 00:56:39.220
Chris Knotts: That's demo number one for you. I hope that that wasn't too fast for you. So

445
00:56:39.537 --> 00:56:43.700
Chris Knotts: next the next demo, I'm gonna show you real quick is a sentiment

446
00:56:44.130 --> 00:57:08.659
Chris Knotts: analysis example. So this is an example in which let's imagine that we're standing up an it project to integrate a very large new software platform. Let's say it's a a new Erp system, or maybe even like an internal an internal Llm, where we want to stand up a private generative AI environment, right? So

447
00:57:08.954 --> 00:57:22.805
Chris Knotts: as part of a project like that. One of the things I may do during my due diligence is, I might conduct a survey of my stakeholders. Right? Let's get. Let's get the concerns. And the you know. Let's see if we can use

448
00:57:24.100 --> 00:57:48.649
Chris Knotts: information from. We ask from our stakeholders about what their concerns might be, and we use those concerns to identify risks in our projects. So now we've got something where, you know, we're we're collecting information from our stakeholders, and we're asking them about their, you know, their biggest concern. So we might get something that looks like this, right? So here's just a spreadsheet, you know, whatever a few 100 lines long with a 1

449
00:57:48.650 --> 00:57:58.190
Chris Knotts: one question survey response hypothetically, in which we've got this we've asked them what their biggest concerns are here in the in the survey.

450
00:57:58.210 --> 00:58:23.190
Chris Knotts: And so we've got this long, this long spreadsheet with names job titles when they responded, and things like that. And once we have a data set like that, now, we could, of course, do sentiment analysis manually, but what we can do using the AI tool is, we can load that spreadsheet. And since I preloaded this you don't see the visualization.

451
00:58:23.190 --> 00:58:33.279
Chris Knotts: But this red box shows you that I uploaded the the Csv, I just showed you guys here. And then I've designed some prompts to conduct a sentiment analysis on the survey data.

452
00:58:33.280 --> 00:58:58.179
Chris Knotts: So you know, it's quite. It's fairly long prompt. I had to be very thoughtful about how to ask it for what I needed. But then you can see here that once it has the data that I gave it. It runs the analysis on the data and gives me the counts and slices and dices, the sentiment neutral, positive, negative, etc. It breaks it down by sentiment and a stakeholder group and things like that. So you can imagine I was a

453
00:58:58.180 --> 00:59:14.349
Chris Knotts: project manager that would be pretty handy. And then, you know, this only takes a few minutes, but then, with with just a little bit of cleanup I generate a report draft that looks something like this, right? So now I've got my my sentiment analysis, which is pulled from the AI.

454
00:59:14.350 --> 00:59:17.680
David Mantica--Co-host!!!: Chris, I got to jump because I got 2 things, 2 questions for you.

455
00:59:17.680 --> 00:59:18.280
Chris Knotts: Sure.

456
00:59:18.280 --> 00:59:19.860
David Mantica--Co-host!!!: They're very pragmatic.

457
00:59:20.530 --> 00:59:27.370
David Mantica--Co-host!!!: 1st one is all right. How do you get the Jira information into an AI tool? What's the best way to do that?

458
00:59:28.110 --> 00:59:31.170
Chris Knotts: Yeah, I got a demo coming up. That's gonna show you that exact thing.

459
00:59:31.170 --> 00:59:45.030
David Mantica--Co-host!!!: Okay, great. Then number 2 would be. And I'm this way as well. I'm a big cut and paster. So I do something in claw to cut and paste it into other documents. Is there a better way besides cut and paste to move things in other documents?

460
00:59:45.030 --> 01:00:08.109
Chris Knotts: Well, I'll be honest. I use cut and paste a lot. I will often make sure I instruct the AI to generate its final output in a table format, so that it makes it easy to paste it into excel. I've seen Mark vigorously nodding his head. So that's my go to. But now, depending on which tool you're using, you know, if you have an enterprise copilot license, for instance.

461
01:00:08.110 --> 01:00:32.840
Chris Knotts: copilot, obviously Microsoft product. So it integrates a little better with the Microsoft productivity apps than other AI tools. But it'll be happy to generate. Excel documents directly out of the prompt, for you know, Chat Gpt some of the other tools will also generate Csvs or other formats. I do find it a little glitchy, so it kind of has that capability. But that's 1 of the reasons I find

462
01:00:32.840 --> 01:00:55.850
Chris Knotts: myself for really relying on cut and paste, because sometimes it just kind of stalls out if you ask it, to generate a spreadsheet or something like that, but within within copilot it's way better at generating that output. But I mean, for now copy and paste still is my go to, you know. Now, if you get into a little bit more customization, if you want to do some engineering, of course us here, we're not going to do this engineering because we're not

463
01:00:55.850 --> 01:01:20.689
Chris Knotts: always engineers, some of us may be. But if you want to get into api integrations or build a bit of a stack. Now, at that point you can automate and build in some of the parsing of the output of the AI, which, of course, the AI is. It's essentially always just text, right? So even when it, that's why it can create a spreadsheet. Because it's creating an Excel document. What it's really generating is just Csv

464
01:01:20.690 --> 01:01:29.310
Chris Knotts: text format, and then it can kind of turn it into a spreadsheet. But with the large language model only has the ability to output language based stuff.

465
01:01:29.310 --> 01:01:34.339
Chris Knotts: Text, right? So, you know, you can. You can do a little bit of you know.

466
01:01:34.610 --> 01:01:57.099
Chris Knotts: transforming the the text based information and other stuff if you wanted to integrate. But I I wouldn't, you know, unless you have, I would start with like, what do you want to do right? What's your business need and if the business need doesn't need some kind of custom api integration or a bit of application development, I wouldn't even worry about it like, just go ahead and copy and paste. That's that's my answer to that. So

467
01:01:57.130 --> 01:02:21.489
Chris Knotts: so back to the sentiment survey here, you know, this is my output. Right? So. And this, you know, just only takes a few minutes based on parsing and analyzing the survey data that I shared with you. So as a project manager. You can imagine how this would be pretty interesting, and of course I've appended this report with an explanation of how I conducted the sentiment analysis. All right. So that's demo number

468
01:02:21.490 --> 01:02:27.479
Chris Knotts: 2, I believe. So. Let me keep on rolling here because I want to try to get through all my Demos.

469
01:02:27.480 --> 01:02:47.759
Chris Knotts: So ports of the world. Sentiment, analysis, the next one. I just want to show you a little bit about using AI for image generation. So this is one, you know, in a project management class. We don't spend a lot of time on this but it's something that people are really interested in, I would say, because there are some really powerful capabilities associated with image generation.

470
01:02:47.860 --> 01:03:11.580
Chris Knotts: So I'm going to show you just a little bit about what the AI tools can do. Now this I'm going to show you a tool that's it's not in Chat Gpt. This is a tool called mid journey. Now, midjourney is what's called a diffusion generator. If you use Dolly within Chat Gpt, or anything like that, you can, it works in very similar way. Now, again, the AI is not really going to be able to

471
01:03:11.580 --> 01:03:36.339
Chris Knotts: kind of create the all the stuff. It's certainly it's not going to be able to do business diagrams or anything very sophisticated, but it can help you generate stuff that's going to help your presentations look better. Spice up your stuff so you can see that what I've done here is that within mid journey I've kind of. I've found. I've played around. You can see here like I've been playing around a little bit

472
01:03:36.340 --> 01:03:37.660
Chris Knotts: playing around with

473
01:03:38.130 --> 01:03:41.020
Chris Knotts: prompt template. That gets me a consistent

474
01:03:41.080 --> 01:04:05.166
Chris Knotts: style. And so here, you know, style is not really very consistent. I'm messing with it, messing with it, messing with it. But then here, finally, I finally got a style of graphic that I liked a color scheme, you know, kind of. It's minimalist style with a white background. And then once I had that you can see here, I just started cranking out these images, and I'm just going to show you. So these are images for a deck and a course that

475
01:04:06.230 --> 01:04:24.489
Chris Knotts: that I've been I'm getting ready to teach a devops course for O'reilly next week, so in for my devops. Course, I just wanted to kind of spice this up right with with some of these graphics. So again, like, I'm not really adding any anything

476
01:04:24.520 --> 01:04:43.149
Chris Knotts: revolutionary here, but I'm it's I've got like a very consistent look and feel and I've generated graphics that are consistent with O'reilly's branding. So that color scheme I was talking about we find a good example here.

477
01:04:44.030 --> 01:05:02.050
Chris Knotts: So so here you can see where I've like integrated the little graphics down here, and they're kind of consistent with O'reilly's branding colors, and I could I could use a different graphic for every slide find some others here, so like I've got my database. Ci like. I've got my databases here tracking to our O'reilly's

478
01:05:02.353 --> 01:05:16.939
Chris Knotts: color scheme. And so you know it's not. It's not like doing the job for me, but it's really helping me make nicer looking slides. And so you can use this for reports or whatever but and then there's another use case I want to share

479
01:05:18.640 --> 01:05:28.670
Chris Knotts: in which you can use the data analyst to perform a different type of of of visualization as well, in which you are actually doing

480
01:05:29.045 --> 01:05:48.959
Chris Knotts: doing data visualization. So I obviously teach a lot of classes. And so, you know, I've got an example here in which I've fed my Lms reporting data up to the data analyst and chat Gpt for. And then I can do visualizations on the the Lms learner data here.

481
01:05:49.640 --> 01:06:14.310
Chris Knotts: or just the example here, distribution of final scores. So when the learners complete a class complete the exam, I can visualize the data across the cohort of the learner scores, and then the final example that I'll give you here is we do a conference here in Raleigh every year called Devops days, and we do this thing called Ignite Karaoke, where we take volunteers from the audience, and we give them 5 min to give an ad hoc

482
01:06:14.310 --> 01:06:39.210
Chris Knotts: improvised presentation into 20, or I'm sorry 5 slides that they've never seen before, and so we're always trying to trip them up. It's really funny, right? So these are all AI generated images that I gave them. You know, I'm trying to throw them off right? And then, like, you know, see what the what the heck are they? Yeah, it is, Jonathan. It's kind of like. It's very much like

483
01:06:39.210 --> 01:07:05.625
Chris Knotts: unconference devops. Days borrows a lot from that. So just few examples. You can see all kinds of different styles all kinds of different themes. But you know I can. You know you can. It's imagery that's kind of hard to find is accessible to you with the AI, because you can tell it to generate all kinds of crazy stuff. So you know, what's the application? I mean? I don't know. It just depends on what the use case is. You know, is this? Is this, gonna help your

484
01:07:06.260 --> 01:07:15.900
Chris Knotts: Is this gonna help your program director understand where you stand with your safe implementation? Well, it's not gonna be able to generate those types of images, but.

485
01:07:15.900 --> 01:07:30.390
David Mantica--Co-host!!!: Yeah, Chris, a couple questions for you that I wanted to. If you could do something be great. So just on the sediment analysis, Liz was just asking, how was the analysis measured high, medium and low? Or is this something you made up as a percent of mentions.

486
01:07:31.283 --> 01:07:46.200
Chris Knotts: Yeah. So I gave. I gave the the AI my parameters in terms of the buckets that I wanted to fall into. So just give my parameters and said, You know, I wanna I wanna group it into those 3 of those 3 buckets. But you can tell it whatever parameters, if you that you wanted right depending on how much detail.

487
01:07:46.200 --> 01:07:52.630
David Mantica--Co-host!!!: So the sentiment was raw as in. Here's all the feedback. It was raw, then, that the AI actually put it in the buckets.

488
01:07:52.630 --> 01:07:53.690
Chris Knotts: That's right. Yeah.

489
01:07:53.690 --> 01:07:55.649
David Mantica--Co-host!!!: And you created the rules.

490
01:07:55.650 --> 01:07:57.320
Chris Knotts: Yeah, I created the rules like, George.

491
01:07:57.320 --> 01:08:02.080
David Mantica--Co-host!!!: Just talking about in your chat as you were doing your chat prompting you created those rules.

492
01:08:02.080 --> 01:08:04.009
Chris Knotts: That's exactly right. Yeah, yeah.

493
01:08:04.010 --> 01:08:12.560
David Mantica--Co-host!!!: Alright. So then we have another question is that some folks you're talking? What's your thought about using the tools embedded in some of the project management tools?

494
01:08:12.560 --> 01:08:29.370
Chris Knotts: Yeah. So that's a great point, right? And I'll mention a little this when I get to the Jira export demo here in a moment. So I like those tools right? So this is a whole other conversation. So I don't want to get too far down the rabbit hole. But you know our copilot is in Beta for dynamics 3, 65,

495
01:08:29.370 --> 01:08:54.309
Chris Knotts: and Ms project as well. You know the project management tools, product management tools. Asana, you know, product board. Atlassian has a tool called Atlassian intelligence that you can subscribe to. So there's a lot of tools coming online. Clickup is heavily oriented around its onboard. AI, they all work basically the same way. They all work

496
01:08:54.310 --> 01:09:19.220
Chris Knotts: essentially like a Chat Gpt agent that's been bolted into the environment. So that's really useful, right? Because then you don't have to do the manual step of exporting these data sets and then importing them back in and kind of doing non native data set analysis. The tools can integrate right there with the data set. So just by example, right? Like in dynamics 3, 65, it can't do everything, but it will.

497
01:09:19.220 --> 01:09:44.730
Chris Knotts: It will generate draft project schedules for you, capacity planning, and and it'll generate draft budgets, and it'll also identify risk by as you get your budgets in there, and then you start to populate dynamics with some actuals, you know, as the project moves forth, the AI will compare the actuals versus the budgets, and it can. It can like automatically derive.

498
01:09:44.992 --> 01:09:51.549
Chris Knotts: You know, Npv, and what your risk is, if you're creeping out of scope and things creeping over budget and things like that. So yeah.

499
01:09:51.550 --> 01:09:51.920
David Mantica--Co-host!!!: Love it.

500
01:09:51.920 --> 01:09:52.790
Chris Knotts: As well. Yeah.

501
01:09:52.790 --> 01:10:01.450
David Mantica--Co-host!!!: Okay, what other question here? We currently don't use an enterprise version of Chat, Gtp. Or any other AI like copilots and enterprise. No enterprise version.

502
01:10:01.530 --> 01:10:10.840
David Mantica--Co-host!!!: Does that mean? I have to prompt prime the tool every time before using it? If so, does anyone have a cheat sheet of different ways to prompt the tool for project management use.

503
01:10:11.547 --> 01:10:18.312
Chris Knotts: That's a great question. So there's a few questions in one there, right? So like, if you don't have an enterprise

504
01:10:18.620 --> 01:10:42.220
Chris Knotts: package, then. Yes, you will have to prime the tool, although, of course, it saves your threads, so as you're in a project or particular thread of work, then it'll save all that context. And then, if you, if you're replicating context from one project to the next, then you have all that in your history. I also highly recommend that everybody keeps a prompt journal and an AI Conversation journal for themselves, so that you've got a

505
01:10:42.220 --> 01:11:07.419
Chris Knotts: a searchable document that you can use to find the context setting prompts that have worked well and things like that, because you will find yourself typing the same thing over and over and over. But there's a bigger issue here that I want to mention, which is that your organization, your employer probably doesn't want you dumping all of that stuff into individual retail level accounts. And so I'll talk about that in just a moment. Here, if I can. If I can get to it.

506
01:11:07.420 --> 01:11:16.839
David Mantica--Co-host!!!: And I got one last thing, the thing I want you to definitely show the jira thing. But one thing I think we're missing with the simulations would be if you could save 5 min to try to show them.

507
01:11:16.850 --> 01:11:19.710
David Mantica--Co-host!!!: Live how you build up an image. Say.

508
01:11:19.990 --> 01:11:23.170
Chris Knotts: Yeah, yeah, yeah. I'll be happy to if I, if their time remains.

509
01:11:23.170 --> 01:11:23.750
David Mantica--Co-host!!!: Yeah, we'll try.

510
01:11:23.750 --> 01:11:25.809
Chris Knotts: Yeah, I'll go as briskly as I can.

511
01:11:25.810 --> 01:11:26.790
David Mantica--Co-host!!!: Okay. Great.

512
01:11:26.790 --> 01:11:27.170
Chris Knotts: Okay.

513
01:11:27.170 --> 01:11:30.309
George Churchwell: David. Don't forget, Gpt. Don't no prompting.

514
01:11:31.120 --> 01:11:56.600
Chris Knotts: So. let me show you this one, because as project managers, I think this is one that we're all probably pretty interested in. Let's say that I've got I've generated. Well, let's even just say we've used the we use the sample, the survey sample with the sentiment analysis that I just showed you. So we again, here's the survey data. We've we've used our sentiment analysis within Chat Gpt to generate

515
01:11:56.600 --> 01:12:20.599
Chris Knotts: the final report. We've got this report. And now let's use this report, and we feed that in as a source reference data set so that we can generate some workflow, some task breakdown, related to the risk that we've identified. Right? So we've got as a part of our final output. We've got these mitigation strategies here. Well, let's turn that into work right as a project manager. So

516
01:12:20.600 --> 01:12:38.889
Chris Knotts: I've got a 1 here, and I did preload these because it was kind of slow. If you do it live. So that's why I wanted to have them prepared. So in this one you can see that I've placed this list of task breakdown that's directly related to the risk identification we did in the previous report.

517
01:12:38.890 --> 01:13:02.080
Chris Knotts: and I've put this into Chat Gpt, and then I've asked it to generate what's called a plant Uml script. Now, you don't have to be highly technical to understand what plant Uml script is, but it generates this scripting here. I had to do a little troubleshooting, but once I get the right one, then I can just copy this script out. I don't have to understand how to do plant uml scripting because Chatgpt understands how to do it for me.

518
01:13:02.210 --> 01:13:06.799
Chris Knotts: and then you can come over here to plant Uml paste that script in and

519
01:13:07.040 --> 01:13:29.068
Chris Knotts: boom. You get a Wbs, and it can do all kinds of business diagrams like this, right? So if I come over here to my mirror board, I've got pulled up. I'll just show you a few other examples. So here, right is the the Wbs diagram that I just showed you. So you know, fairly basic. But I mean, this is like a 5 min diagram. So you could play with this. I've got also dependency diagram here.

520
01:13:32.180 --> 01:13:35.839
Chris Knotts: audit diagram. So Uml has the ability

521
01:13:36.730 --> 01:13:46.920
Chris Knotts: to champ put into business diagrams. So that's just another way of visualizing work specific to a project manager that that can be pretty handy.

522
01:13:47.140 --> 01:14:08.339
Chris Knotts: So this is pretty fun one. But if you plant, Umlcom is free to use. So fire up your chat. Gpt, and just, you know, feed in basic language, project descriptions and stuff, and see if you can get chatgpt to turn that documentation or that language into a uml script that you can use to generate diagrams. Pretty pretty fun

523
01:14:10.005 --> 01:14:16.400
Chris Knotts: all right, then. So now I think I'm let me go to the the import export

524
01:14:16.670 --> 01:14:19.009
Chris Knotts: because I want to make sure that I've got time for this.

525
01:14:19.010 --> 01:14:20.499
David Mantica--Co-host!!!: Yeah, we want to cover that.

526
01:14:20.720 --> 01:14:41.979
Chris Knotts: Yeah. Yeah. Okay. So the so the jira. Now, the 1st thing I'll say about the Jira import is that, for one thing, if you're able to convince them to let you subscribe to Atlassian intelligence. You don't have to do this because then you'll have a Gpt. Agent that works right there in the the Jira environment. So that's pretty that's pretty useful. And you know, there's no

527
01:14:41.980 --> 01:15:00.849
Chris Knotts: there's no better way to work with the AI and the data sets than actually having it integrated. Because there's a lot of, you know, debugging and things like that. So I'll show you a couple of Atlassian tools here. Now, the one I'll start with is this one just because I have this project, and this is not in Jira. This is in Trello

528
01:15:02.650 --> 01:15:31.829
Chris Knotts: that I worked on earlier this year, and you can see, you know, it's pretty pretty granular. I've got like notes in things, you know. We've got documents in here. We've got all kinds of all kinds of project information in here, as we often would have. But of course, if you come over here, and there's similar functionality in Jira as well. So if you come over here within Trello, you have generally, and you have something generally within any type of Project tool

529
01:15:31.830 --> 01:15:39.880
Chris Knotts: I can export here as a Csv. If I've upgraded I've let my my license lapse here, so I just have a free version. But

530
01:15:40.040 --> 01:15:43.879
Chris Knotts: if I have my paid version I can export a Csv. I can also export as Json.

531
01:15:43.900 --> 01:15:44.436
Chris Knotts: which

532
01:15:45.670 --> 01:15:48.869
Chris Knotts: if you do this, it'll generate something

533
01:15:49.550 --> 01:15:52.049
Chris Knotts: that looks like.

534
01:15:52.840 --> 01:15:54.980
Chris Knotts: Let's see if I can get it here.

535
01:15:57.590 --> 01:16:15.220
Chris Knotts: So it'll generate something for me that looks like this, right? So this doesn't mean anything to me because this is Json Code. But if I select a chunk of this code and then let's just go ahead and try this right. If I come over here and open up a new

536
01:16:15.550 --> 01:16:16.630
Chris Knotts: chat.

537
01:16:26.640 --> 01:16:37.050
Chris Knotts: I'll move to 4 because I find it to be the strongest model, and I'm just for the sake of time. I'm just gonna say, what's going on with this exported

538
01:16:37.460 --> 01:16:39.620
Chris Knotts: Trello project data.

539
01:16:39.880 --> 01:16:42.390
Chris Knotts: And I'm just gonna dump in that raw

540
01:16:42.970 --> 01:16:45.570
Chris Knotts: Json export data.

541
01:16:47.420 --> 01:16:49.161
Chris Knotts: So it was just a

542
01:16:50.181 --> 01:16:56.719
Chris Knotts: it was just a a very small segment. But you can see right away how it does begin to parse the data right now. There's a

543
01:16:57.110 --> 01:16:59.199
Chris Knotts: another way to support. This is a Csv.

544
01:16:59.220 --> 01:17:11.240
Chris Knotts: so here's the Csv switching gears a little bit, because this is actually a Jira Jira sprint export. So now I've got a kind of a larger, more robust, but also more structured.

545
01:17:11.970 --> 01:17:36.750
Chris Knotts: Csv. Then dumped from out of Jira. And then, once I have this, of course, now it's ingestible into Chat Gbt again. You can see in the red box here. I've uploaded the Atlassian data export here, and then I've given it some basic prompting, and then, just to get it started? These are some of the questions I've asked it to interpret for me now, some of these may be interpretable within Jira, but some of them

546
01:17:36.750 --> 01:17:58.049
Chris Knotts: may not. Jira may not have the capability to ask or analyze exactly what I want it to. So the cool thing about this is that you can take the raw data set from out of Jira and then craft like whatever questions you want, because the AI will figure out the logic for you, so you can see that it's begun to parse the Jira project. I've asked about

547
01:17:58.050 --> 01:18:13.510
Chris Knotts: tasks in progress. I've asked distribution across my resources, and then I've asked it to identify overdue tasks and which categories they are so and you can see that it begins to give me output on this right? So it's doing counts of my tasks in progress

548
01:18:13.540 --> 01:18:36.669
Chris Knotts: counts of what's overdue distribution of tasks across the category. So we can start to identify things you might be interested in as a project manager. Bottlenecks, scope creep, overloaded teams, things like that, right? So in in the amount of time we have here, you know, it's like, I'm just giving you kind of a glimpse, right? But you can see you can see how this works, so I hope that at least starts to touch on

549
01:18:36.993 --> 01:18:44.110
Chris Knotts: the the idea of piping data in and out of Atlassian tools, other project management tools and things like this is.

550
01:18:44.110 --> 01:18:57.209
David Mantica--Co-host!!!: So all you're doing, all you're doing is asking for the export. So if you're in Jira, give me an export, I got the export file. Then I could stick it into one of the tools. If I don't have Jira intelligence and I can start manipulating it in the tool I've selected.

551
01:18:57.210 --> 01:19:20.390
Chris Knotts: Yeah, exactly. Right. So here's the big point that I would say right in terms of interoperating across project management tools and the generative AI tools. This is what you want to look for. Figure out how to get your projects and data sets, or whatever it is. Figure out how to get that out either as a a plain text document, if it's like documentation related. So like if it's an actual

552
01:19:20.786 --> 01:19:35.450
Chris Knotts: you know, written document, or whatever just you know. Word, Doc. Simple text. Whatever or be a Csv, Csv is the jam. Right? Like Csv is the format that seems to work the best with the tools?

553
01:19:35.796 --> 01:20:05.219
Chris Knotts: It can. It can interpret Pdfs excel sheets other types of formats, but it just it gets a little glitchier less reliable. Yeah. Get your exports into Csv, that. And and so Csv very common format. Obviously, if you have some constraints, you have to like, jump through some hoops if you have to get it into an Excel format. Well, then, of course, then you can get into Csv from there. But the the AI agents work best with Csv for structured tabular data.

554
01:20:05.390 --> 01:20:06.370
Chris Knotts: All right.

555
01:20:08.020 --> 01:20:13.670
Chris Knotts: So I hope that helps at least ignite your imagination about it a little bit.

556
01:20:14.013 --> 01:20:21.290
Chris Knotts: And so let's see here. Where am I? That may have been? I think that was my last and my actual, my last demo.

557
01:20:21.420 --> 01:20:24.779
Chris Knotts: David, how am I doing on time here? What what do I got left.

558
01:20:27.920 --> 01:20:31.042
Lara Hill: You're right at time, actually, Chris, but that's perfect.

559
01:20:31.390 --> 01:20:33.450
Chris Knotts: What does that mean like? Do I have 1 min left? 5.

560
01:20:33.450 --> 01:20:35.900
Lara Hill: Yes, let's let's say 1 min left.

561
01:20:35.900 --> 01:20:36.379
Chris Knotts: Yeah, we're.

562
01:20:36.380 --> 01:20:40.590
Lara Hill: Running about 5 min behind. So if you could wrap up in a minute, that'd be great.

563
01:20:40.590 --> 01:20:44.180
Chris Knotts: Okay. All right. Well, I've I'll I'll.

564
01:20:44.180 --> 01:20:47.630
David Mantica--Co-host!!!: I would love to see a Chris. Show them how you do an image.

565
01:20:47.920 --> 01:20:49.070
David Mantica--Co-host!!!: You got a great let's do.

566
01:20:49.070 --> 01:20:50.580
Chris Knotts: Yeah, well, we'll. We'll.

567
01:20:50.580 --> 01:20:52.428
David Mantica--Co-host!!!: Do an image of

568
01:20:52.890 --> 01:20:53.770
Chris Knotts: Give me an idea.

569
01:20:53.770 --> 01:21:00.719
David Mantica--Co-host!!!: Of date of somebody at the conference, just overjoyed with all the information that they received.

570
01:21:00.720 --> 01:21:02.029
Chris Knotts: All right. That sounds

571
01:21:02.550 --> 01:21:17.629
Chris Knotts: sounds like a great idea. So within mid journey let me try to do this. So mid journey also uses a prompt now, a key thing to understand about the image generators is they also use the large language model right? So that but they use a different type of training data.

572
01:21:17.630 --> 01:21:34.110
Chris Knotts: Then the text-based AI tools do. So let me just think. Here, let's start with something basic. And then we'll craft it into something that might be a little more useful. So we're going to say, give me an image of an incredibly

573
01:21:34.110 --> 01:21:35.590
Chris Knotts: overjoyed

574
01:21:42.770 --> 01:21:46.619
Chris Knotts: attendee at a business conference

575
01:21:46.980 --> 01:21:53.030
Chris Knotts: very excited about a presentation she is seeing.

576
01:21:53.110 --> 01:22:18.090
Chris Knotts: And so the images take about 60 seconds to generate. So I'll give you just some other information about the image generators. So what the image generators will do is they will start by giving you 4 candidates. So in image, I was just playing with yesterday. You get 4 candidates, and then you can upscale the images. You can also dial in the aspect ratios in mid journey. So if you want something

577
01:22:18.090 --> 01:22:32.260
Chris Knotts: is rectangular, if it's a slide, you can tell it. Give me a aspect ratio of one by 2, or whatever you know, you can specify colors or specific characteristics. So this is pretty funny, right like incredibly overjoyed.

578
01:22:32.260 --> 01:22:38.269
David Mantica--Co-host!!!: So. Chris, can you add physical? Can you add things like physical features of your image?

579
01:22:38.270 --> 01:22:45.729
Chris Knotts: Absolutely. So. Now, what I'm gonna do is I'm gonna just this, just remember, it's a starting point, right? Like AI never gives you a final product number.

580
01:22:45.730 --> 01:22:50.080
David Mantica--Co-host!!!: I would say, try to get this virtual. Say, yeah, but they're doing a conference virtually.

581
01:22:50.680 --> 01:22:55.239
Chris Knotts: Okay, sounds good. So and we'll just add the language virtual

582
01:22:55.400 --> 01:23:02.349
Chris Knotts: and incred instead of just an incredibly overjoyed attendee. Let's say an incredibly overjoyed attendee, wearing

583
01:23:02.360 --> 01:23:03.720
Chris Knotts: glasses

584
01:23:04.320 --> 01:23:05.560
Chris Knotts: with brown

585
01:23:07.255 --> 01:23:08.120
Chris Knotts: here.

586
01:23:08.400 --> 01:23:10.600
Chris Knotts: and a blue suit.

587
01:23:11.120 --> 01:23:13.510
Chris Knotts: and let's see what that does.

588
01:23:13.750 --> 01:23:14.270
Chris Knotts: So.

589
01:23:14.270 --> 01:23:16.189
David Mantica--Co-host!!!: Blue suit? Or is this some

590
01:23:16.400 --> 01:23:18.709
David Mantica--Co-host!!!: dumb and dumber, or something? A blue suit.

591
01:23:18.710 --> 01:23:20.949
Chris Knotts: Yeah, I don't get that reference, but we'll.

592
01:23:20.950 --> 01:23:26.469
David Mantica--Co-host!!!: You don't get the reference. Oh, man, dumb and dumber, the Tuxedos, the blue and orange, or whatever.

593
01:23:26.470 --> 01:23:32.770
Chris Knotts: This is. This is my biggest pitfall with you, David is when we get into the movies. I haven't seen you make fun of me because I haven't seen.

594
01:23:32.770 --> 01:23:34.979
David Mantica--Co-host!!!: I know I forgot about that.

595
01:23:34.980 --> 01:23:49.979
Chris Knotts: I feel like a Pop culture ignoramus, because that is my weakest trivia category. So now we're starting to get like again, you know, it's still probably not quite there. But we're getting the blue suit. We're getting the glasses. We're getting the brown.

596
01:23:49.980 --> 01:23:52.379
David Mantica--Co-host!!!: And they didn't do the virtual conference very well.

597
01:23:52.380 --> 01:24:03.439
Chris Knotts: Yeah, I didn't really get the virtual conference, so they probably doesn't really understand what that means, that it's a virtual conference. So we have to say something like, probably a virtual business conference

598
01:24:03.840 --> 01:24:08.180
Chris Knotts: sitting in front of a computer

599
01:24:08.640 --> 01:24:10.449
Chris Knotts: on a zoom call

600
01:24:11.067 --> 01:24:19.842
Chris Knotts: and then let's add a few other things here, right like, for instance. Let's say we want this to be

601
01:24:20.490 --> 01:24:22.930
Chris Knotts: let's say photo

602
01:24:23.190 --> 01:24:25.500
Chris Knotts: journalistic style.

603
01:24:25.580 --> 01:24:30.370
Chris Knotts: And using let's say we want to use purple

604
01:24:31.480 --> 01:24:32.700
Chris Knotts: color palette.

605
01:24:33.490 --> 01:24:57.840
Chris Knotts: So you know, getting more and more specific. So like again, the reasoning that the approach that we're using with the image generator, it's logically. It's the same kind of approach that we want to be using with our chat Gpt or our language based tools in which we're, you know, we're starting. And then we're refining, refining, refining. You know, we're building on it. You know, we're not. Gonna we're not going to get the final product the 1st time. But then, just like within a text-based Gpt bot

606
01:24:57.890 --> 01:25:22.790
Chris Knotts: with this once you do find that template that works. Then you know, then you can start to use that again and again, just like I did in the O'reilly deck. So again, you see, we can see we're getting there right like we've got the purple color palette. We've got the incredibly overjoyed look so excited the glasses, the brown hair. Now we've got the computer in there. So you know again, right like we. We would probably have to play with this for a while, but within

607
01:25:22.790 --> 01:25:30.420
Chris Knotts: hour, I guarantee you, we would have a template that we could use that would generate an image that's like very close to what we need.

608
01:25:32.010 --> 01:25:32.900
David Mantica--Co-host!!!: Okay.

609
01:25:33.100 --> 01:25:46.479
David Mantica--Co-host!!!: yeah, it was great. That's really. I kind of wanted to show people the process the engagement associated with it. It's still a lot shorter than creating that image from scratch, because most of us on here couldn't do it.

610
01:25:46.760 --> 01:25:47.640
David Mantica--Co-host!!!: And oh, my goodness.

611
01:25:47.640 --> 01:25:48.460
Chris Knotts: Yes.

612
01:25:48.460 --> 01:25:59.139
David Mantica--Co-host!!!: And as you're if you're a female, a white female who's attending the conference, and you want to show your boss how great it was. You write it up, and you put that picture in there. A picture is worth a thousand words.

613
01:25:59.650 --> 01:26:26.290
Chris Knotts: Yeah, that's exactly right. And you know, the other thing, too, I will say, is just again, as with all AI, where this really becomes powerful is when, then, my designer, who understands how to use illustrator and Photoshop and indesign, can generate raw material with the AI, but then pull it into the actual professional expert run tools that are not AI and

614
01:26:26.350 --> 01:26:47.869
Chris Knotts: and and use them together like that. Right? So you know, it's the it's again. It's the combination of the human expertise with the AI. And the key, of course, is to understand what is the good robot use case for what the AI can do well. And where? Where do you want to reserve your bandwidth for the human needs to contribute to the the joint work product? If that makes sense.

615
01:26:47.870 --> 01:26:48.620
David Mantica--Co-host!!!: Yeah.

616
01:26:48.930 --> 01:27:09.869
David Mantica--Co-host!!!: okay, so one of the things we're talking about here and before we get is that you know that people are loving the real world examples. The challenge of the conference to do the real world examples is it's tough to integrate the different topic areas and also real world examples come with some risks. So what we're trying to do is get you empowered with the skills that you could take that next step.

617
01:27:09.870 --> 01:27:22.550
David Mantica--Co-host!!!: But when you take a hands-on class based on role for Gen. AI, or just in general, is where you can start getting that practice. And Laura, with that, said, Why don't you do the pitch share, and then we'll get into mark session.

618
01:27:22.970 --> 01:27:40.279
Lara Hill: Okay, great. We have a fun bonus giveaway to talk about skills. Development group is giving away a free course to one of the attendees here, and I'm just going to pick one at random, someone that is present here. I have a random generator here, so

619
01:27:42.400 --> 01:27:46.690
Lara Hill: let me just click the button over here, Amanda Craig.

620
01:27:46.840 --> 01:28:13.049
Lara Hill: If Amanda Craig is in the room, you won a free course, you can message me, and we will find which course you can take for free if you're interested. Oh, good! You are here. Thank you, Amanda, for being present with us, and we're going to do another giveaway after Mark's presentation, Mark is on deck, our instructor for AI for Bas, and also just want to say, thanks to Chris, everyone, give him

621
01:28:13.050 --> 01:28:36.730
Lara Hill: his shout outs in the chat. Let us know your feedback on Chris's presentation. Chris is our instructor for AI for project managers and a number of other courses as well, which I'll be sending out resources on those courses after the conference, but I want to, in the interest of time, go ahead and turn it over to Mark Balser. Thank you for joining us, mark.

622
01:28:36.730 --> 01:28:46.820
Marc Balcer: Oh, thank you so welcome, everybody! If you saw me just a moment ago. Looking around, it was because I saw a reference to Chesapeake by James Mischner, and

623
01:28:46.840 --> 01:28:49.479
Marc Balcer: I thought I had on my bookshelf

624
01:28:49.490 --> 01:28:53.059
Marc Balcer: a signed copy of it from James Missioner himself.

625
01:28:53.465 --> 01:29:02.790
Marc Balcer: That you know I had gotten from my mom many, many years ago, but somehow it, you know, it's probably staring me right in the face. But.

626
01:29:02.790 --> 01:29:06.129
David Mantica--Co-host!!!: Or somebody said, or somebody sold that way.

627
01:29:06.605 --> 01:29:11.830
Marc Balcer: Nobody sold it. You know we don't go there. Okay.

628
01:29:11.830 --> 01:29:13.290
David Mantica--Co-host!!!: Don't mind me.

629
01:29:13.290 --> 01:29:20.530
Michael Wolf: Oh, I I put that reference up there. I asked Chat Gpt to tell me the characters in the book, because I lose track of characters.

630
01:29:20.630 --> 01:29:34.490
Michael Wolf: so I had it tell me the 1st time a character appeared in a chapter and had it graph it out for me. It was really nice. And that's my reference to Mermaid graphic language which kind of reverts back to Chris's discussion about Uml.

631
01:29:34.890 --> 01:29:42.849
Marc Balcer: Oh, yeah. And and Chris, that was the other thing, Chris, you know, to actually have done a live demo with plant. Uml

632
01:29:43.578 --> 01:29:45.611
Marc Balcer: that that was gutsy.

633
01:29:47.760 --> 01:29:48.870
Marc Balcer: because you know, every.

634
01:29:48.870 --> 01:29:52.700
David Mantica--Co-host!!!: He didn't do it. Live. That was, that was a simulation. He built it.

635
01:29:53.560 --> 01:29:55.429
David Mantica--Co-host!!!: Make sure it would work. You can't do it.

636
01:29:55.430 --> 01:29:58.610
Chris Knotts: I did run. The Uml generation live, though.

637
01:29:58.610 --> 01:29:59.580
David Mantica--Co-host!!!: Oh, you did. Okay.

638
01:29:59.580 --> 01:30:05.510
Chris Knotts: And I gotta give a shout out to Mark, because Mark's the one you you're the one who originally gave me that idea. So thank you, Mark.

639
01:30:05.510 --> 01:30:07.059
Marc Balcer: Yeah, alright.

640
01:30:07.240 --> 01:30:08.850
Marc Balcer: Okay. So

641
01:30:09.349 --> 01:30:17.329
Marc Balcer: the other area that I'm also into not only the AI for BA, but also the AI for testing class.

642
01:30:17.630 --> 01:30:28.669
Marc Balcer: And one of the things that comes about, you know, that's really interesting. I found in putting together both of those classes, and, you know, working with clients in this particular area is that

643
01:30:29.050 --> 01:30:46.090
Marc Balcer: there just seems to be a lot of commonality in the problem of, you know, coming up with business requirements and doing software, testing or doing the analysis or doing the work required to create software testing. So

644
01:30:46.270 --> 01:31:10.169
Marc Balcer: purpose of this, you know, is to 1st kind of give you a couple of the acronyms that you might hear people talking about. You know all the Dds as I like to call them. Then to go through a couple of Demos. I actually, I'm not gonna make them Demos. I'm just gonna make them sort of talk throughs. You know of how you go from concept to tests.

645
01:31:10.260 --> 01:31:30.020
Marc Balcer: and how those tests help you to elicit requirements. In other words, we're kind of going to be doing things very backwards. If you think about it? You typically think, oh, I've got to elicit requirements first, st in order to be able to develop the software and to develop the tests. Well, it turns out that if you're paying attention during this whole process.

646
01:31:30.140 --> 01:31:34.060
Marc Balcer: you know, as you're coming up with test cases.

647
01:31:34.380 --> 01:31:39.989
Marc Balcer: it can actually help you to figure out the requirements of what it is you need to build.

648
01:31:40.110 --> 01:31:47.369
Marc Balcer: and then I'll finally wrap up by giving you a couple illustrations of how to go about doing this in practice

649
01:31:48.917 --> 01:31:56.119
Marc Balcer: plus one other nice little surprise based on some of the talk that has talks that have been going on. Okay.

650
01:31:56.816 --> 01:32:01.069
Marc Balcer: so about a year ago, year and a half ago, maybe.

651
01:32:02.450 --> 01:32:06.179
Marc Balcer: you know. The word I came up with was gobsmacked

652
01:32:06.600 --> 01:32:10.869
Marc Balcer: when I was able to take a drawing, do a drawing

653
01:32:10.890 --> 01:32:12.200
Marc Balcer: paper and pen.

654
01:32:13.440 --> 01:32:14.859
Marc Balcer: take a picture of it.

655
01:32:15.830 --> 01:32:19.569
Marc Balcer: upload it into chat, Gpt, and say, What is this?

656
01:32:20.240 --> 01:32:21.620
Marc Balcer: So I took.

657
01:32:22.320 --> 01:32:33.770
Marc Balcer: you know, a screen which I was using already in one of my software testing classes. And it was actually, interestingly enough, it was actually, during some.

658
01:32:34.160 --> 01:32:35.029
Marc Balcer: you know.

659
01:32:36.160 --> 01:32:59.949
Marc Balcer: informal time during one of those classes when we were talking about not the AI for testing, but just a regular, you know, test automation class. We were talking about AI and what we were doing. And someone said, You know, you think you could upload a picture or a screenshot? And so I played around with it, and then I even drew this out by hand.

660
01:33:00.310 --> 01:33:02.520
Marc Balcer: gave it to chat, gpt.

661
01:33:03.510 --> 01:33:04.879
Marc Balcer: and said, What's this?

662
01:33:07.250 --> 01:33:08.030
Marc Balcer: Oh.

663
01:33:09.000 --> 01:33:10.260
Marc Balcer: it's pretty good.

664
01:33:11.610 --> 01:33:15.900
Marc Balcer: It looked at it. It kind of figured out what I wanted.

665
01:33:16.150 --> 01:33:17.220
Marc Balcer: Okay.

666
01:33:17.930 --> 01:33:26.850
Marc Balcer: now, of course, the thing that everybody is talking about when they can do that is well, if you can have it. Figure it out. Can you have it? Write the code?

667
01:33:27.540 --> 01:33:28.290
Marc Balcer: Yeah.

668
01:33:29.010 --> 01:33:32.249
Marc Balcer: Yeah. And in fact, you know I have that.

669
01:33:32.640 --> 01:33:34.489
Marc Balcer: You can take that.

670
01:33:34.740 --> 01:33:35.530
Marc Balcer: you know.

671
01:33:35.680 --> 01:33:43.200
Marc Balcer: Tell it to write out HTML with Bootstrap for formatting and Javascript to make it work.

672
01:33:43.490 --> 01:33:49.650
Marc Balcer: and it will create something for you, and it will actually create this mileage reimbursement page.

673
01:33:52.510 --> 01:33:53.310
Marc Balcer: Great!

674
01:33:54.550 --> 01:33:55.840
Marc Balcer: How do I know I'm done?

675
01:33:56.690 --> 01:33:58.060
Marc Balcer: How do I know it's right?

676
01:34:00.120 --> 01:34:01.270
Marc Balcer: I got a test.

677
01:34:02.160 --> 01:34:03.050
Marc Balcer: Okay?

678
01:34:03.500 --> 01:34:04.600
Marc Balcer: So

679
01:34:05.240 --> 01:34:10.780
Marc Balcer: give you a little bit of background. Just in case, you know, some of these concepts might be new to you.

680
01:34:11.492 --> 01:34:15.050
Marc Balcer: Agile methods make very heavy use

681
01:34:15.060 --> 01:34:25.280
Marc Balcer: of the concepts of various concepts of test driven development. The idea of test driven development fundamentally comes down to the idea that

682
01:34:25.710 --> 01:34:35.080
Marc Balcer: before people start coding, we know what the test cases are going to be, and the goal is to make the test cases pass

683
01:34:35.510 --> 01:34:47.559
Marc Balcer: right. I tell people in my classes it's kind of like, you know, walking into a math or physics course on day one professor not only hands you the syllabus, but also says, Oh, by the way, here's the final exam

684
01:34:48.330 --> 01:34:56.940
Marc Balcer: now, on the last day of class, if you can solve every one of these kinds of problems, I might change up the numbers, but if you can solve each one of these kinds of problems and show your work.

685
01:34:57.020 --> 01:34:58.029
Marc Balcer: You'll get an A

686
01:34:58.330 --> 01:35:24.680
Marc Balcer: that's going to help really help direct your studies. Okay, so test driven development for a developer is kind of a very similar thing in the sense that you know, it really helps the developer to scope out the work they need to do, because one of my old buddies, Martin Fowler, who I used to work with many years ago at Thoughtworks. Martin had this great expression, he said, the most expensive code is the code that's never used.

687
01:35:25.380 --> 01:35:31.209
Marc Balcer: I don't know if it's it was him who came up with that or somebody else. But you know, it really makes a lot of sense.

688
01:35:31.220 --> 01:35:35.699
Marc Balcer: If, as a developer, you're writing something and nobody's using it.

689
01:35:36.396 --> 01:35:43.730
Marc Balcer: You have the lost opportunity cost, plus. You have to drag that code along with you at the same time.

690
01:35:44.027 --> 01:35:52.020
Marc Balcer: The last thing we want to have happen is, you know, 2 days before the end of the sprint, you know. Tester comes in and says, Well, you know, guess what

691
01:35:52.070 --> 01:35:55.429
Marc Balcer: you're not passing 5 of the 7 test cases.

692
01:35:55.940 --> 01:35:57.930
Marc Balcer: huh! Developer says.

693
01:35:58.130 --> 01:36:05.299
Marc Balcer: well, you know, when I do this, and the you know, and the developer is perfectly in his right mind to say, scope creep.

694
01:36:05.830 --> 01:36:06.750
Marc Balcer: because.

695
01:36:07.060 --> 01:36:08.850
Marc Balcer: after all, what's happened?

696
01:36:08.920 --> 01:36:12.949
Marc Balcer: The you know. Nobody said, Oh, this is what I want.

697
01:36:13.200 --> 01:36:21.919
Marc Balcer: you know. The requirements were given to them as these wonderful glittering generalities. So the developer kind of just filled in the blanks.

698
01:36:22.710 --> 01:36:27.909
Marc Balcer: All right. So test driven development basically says before we commit to doing the work.

699
01:36:28.060 --> 01:36:51.609
Marc Balcer: we're going to know what the test cases are in advance. Now, there are many different variations of this, you know, developers like Tdd, because that's like their unit tests. So that there's also behavior, driven development acceptance test, driven development. As far as I'm concerned, for our purposes here. They're all the Dds. It's all the idea of we're going to know in advance what needs to pass before we get started.

700
01:36:52.110 --> 01:36:52.950
Marc Balcer: Okay?

701
01:36:53.180 --> 01:36:54.379
Marc Balcer: So given that.

702
01:36:54.550 --> 01:36:57.060
Marc Balcer: let's go back to our

703
01:36:57.550 --> 01:36:59.990
Marc Balcer: mileage calculator. So

704
01:37:00.070 --> 01:37:02.950
Marc Balcer: just to keep things from being polluted

705
01:37:03.481 --> 01:37:11.069
Marc Balcer: clear out the chat. Tell it. Don't remember anything that you know you had seen before. Go to a new chat.

706
01:37:11.310 --> 01:37:15.099
Marc Balcer: clear the memory. If you're working with a professional version of Chat Gpt.

707
01:37:16.180 --> 01:37:21.349
Marc Balcer: give it the picture and say, now describe this behavior as a set of test scenarios.

708
01:37:22.370 --> 01:37:23.630
Marc Balcer: Now again.

709
01:37:24.110 --> 01:37:27.039
Marc Balcer: remember, forget the fact that you saw the code

710
01:37:27.370 --> 01:37:29.849
Marc Balcer: right? All we had was the picture.

711
01:37:30.060 --> 01:37:32.299
Marc Balcer: So now what's going to happen here?

712
01:37:32.310 --> 01:37:36.080
Marc Balcer: It's going to generate a whole bunch of different pictures

713
01:37:36.100 --> 01:37:38.820
Marc Balcer: or a whole bunch of different test cases for us.

714
01:37:38.990 --> 01:37:39.670
Marc Balcer: Right?

715
01:37:40.762 --> 01:37:42.690
Marc Balcer: Now, that's a little

716
01:37:43.010 --> 01:37:47.620
Marc Balcer: prompt for me. It says, go to the chat. So we can actually take a look at

717
01:37:49.030 --> 01:38:07.119
Marc Balcer: the chat. And we can see the different test cases that we got. Okay. So you got a, you know, basic input and reimbursement right away. I pick up a problem. It's like, Wait a minute. I don't expect the you know user to enter it in your month. Day format. Okay, make a note of that.

718
01:38:07.230 --> 01:38:13.710
Marc Balcer: All right, and the number of miles driven. Okay, that makes sense. Select business as the purpose. Click compute. Okay? Great

719
01:38:14.245 --> 01:38:17.439
Marc Balcer: and it figures out 58 cents per mile.

720
01:38:17.580 --> 01:38:20.039
Marc Balcer: Where did you get 58 cents per.

721
01:38:20.620 --> 01:38:31.400
Marc Balcer: Okay? No, I'm again. I'm making notes of all these things, because again, I'm going to throw those back into chat Gpt to say, either make a correction.

722
01:38:31.610 --> 01:38:36.789
Marc Balcer: or here's what I want to do differently. Or where did you get that from? Or something? Okay?

723
01:38:37.460 --> 01:38:39.769
Marc Balcer: So we leave the miles driven blank. Okay.

724
01:38:39.930 --> 01:38:44.080
Marc Balcer: this is great. It figured out that you know this is a required field

725
01:38:44.120 --> 01:38:53.880
Marc Balcer: non numeric miles driven. Okay? You know, again, it kind of knows, based on its training data, that if we have a numeric field that yeah.

726
01:38:53.930 --> 01:38:58.409
Marc Balcer: Got to put numbers into it. Purpose. Selection?

727
01:38:59.560 --> 01:39:00.430
Marc Balcer: Oh.

728
01:39:02.850 --> 01:39:19.010
Marc Balcer: what are the choices that we should have for purpose? Okay. Now, maybe that's not an opportunity just to correct Chat Gpt, that might be one of those questions that we need to go back to our subject matter experts and say, Hey, what do those have to be? Oh, I love it!

729
01:39:19.050 --> 01:39:21.750
Marc Balcer: I love it when I, you know, put my

730
01:39:22.190 --> 01:39:23.350
Marc Balcer: hand up.

731
01:39:23.470 --> 01:39:24.800
Marc Balcer: and

732
01:39:25.630 --> 01:39:26.360
Marc Balcer: it

733
01:39:26.960 --> 01:39:35.379
Marc Balcer: there we go. Thank you for clearing that. You know. It's a nice little feature in zoom that if you make certain gestures, it will, you know, make those gestures.

734
01:39:36.017 --> 01:39:37.690
Marc Balcer: Live on you

735
01:39:38.780 --> 01:39:42.969
Marc Balcer: until you do it. Okay? So we have all of these different test cases.

736
01:39:43.270 --> 01:39:49.670
Marc Balcer: We go through the different test cases. We see. You know what we have. We start to make up a list.

737
01:39:50.130 --> 01:39:51.870
Marc Balcer: Right? That's a pretty good

738
01:39:51.980 --> 01:39:53.719
Marc Balcer: idea. All right.

739
01:39:53.920 --> 01:39:54.800
Marc Balcer: Now.

740
01:39:54.950 --> 01:39:58.190
Marc Balcer: I'm going to use a little bit of vocabulary on you first.st

741
01:39:59.730 --> 01:40:02.210
Marc Balcer: I like to always, you know.

742
01:40:02.420 --> 01:40:08.860
Marc Balcer: use different words for different purposes. So when people talk about writing a user story

743
01:40:09.070 --> 01:40:25.920
Marc Balcer: back of the card contains the acceptance criteria, the acceptance criteria are not written necessarily in gherkin given. When, then, they are not a formal list, they are not a complete list. It's sort of. Here's what I think we need to have. Okay.

744
01:40:26.060 --> 01:40:28.810
Marc Balcer: Scenarios are the informal

745
01:40:28.840 --> 01:40:43.369
Marc Balcer: but a much more complete format of what needs to be delivered. Now, writing out scenarios. That's where typically people will write those in the gherkin given when then, formats or in similar kinds of formats, but acceptance criteria

746
01:40:43.980 --> 01:40:55.619
Marc Balcer: kind of bill. This is kind of what I want to see happen scenarios much more formal test cases are intended to be executable. Okay? So from here on out, I'm going to use the term scenarios for what we're doing.

747
01:40:56.240 --> 01:40:58.260
Marc Balcer: So the next thing I want to do.

748
01:40:58.420 --> 01:41:07.460
Marc Balcer: although when working with the agents. Sometimes I use the word test cases because scenarios it comes back with stuff that's a little too

749
01:41:07.600 --> 01:41:13.140
Marc Balcer: flaky for me. Okay, isn't it lovely that we're working in an industry where the terms are so well defined.

750
01:41:13.980 --> 01:41:15.249
Marc Balcer: All right. So

751
01:41:15.790 --> 01:41:21.169
Marc Balcer: once I have a few test cases, a few scenarios out of Chat. Gpt.

752
01:41:22.100 --> 01:41:23.860
Marc Balcer: I'm going to go over to Claude.

753
01:41:25.020 --> 01:41:30.230
Marc Balcer: Given Claude the same problem. Okay, now, Claude's going to come back in a totally different format.

754
01:41:30.740 --> 01:41:31.440
Marc Balcer: right?

755
01:41:31.770 --> 01:41:33.200
Marc Balcer: And so

756
01:41:34.080 --> 01:41:36.159
Marc Balcer: I need to merge the formats. Great.

757
01:41:36.210 --> 01:41:41.940
Marc Balcer: I give that to AI. Right? That's 1 of those boring, repetitive things that Chris was talking about.

758
01:41:42.260 --> 01:41:49.230
Marc Balcer: People hate doing, boring, repetitive. I love. Do I mean a. I hate doing boring. Repetitive

759
01:41:49.340 --> 01:41:55.310
Marc Balcer: people hate doing boring, repetitive computers love it. Give it to the AI agent. Okay.

760
01:41:55.440 --> 01:41:56.490
Marc Balcer: so

761
01:41:57.160 --> 01:42:18.190
Marc Balcer: give it to check. Give it to Claude. Claude comes back with the list of test cases. By the way, one of the features I'm seeing in Claude now is that when you ask it to generate something, it produces these documents as separate elements as opposed to putting them into the main body of the chat.

762
01:42:18.350 --> 01:42:19.120
Marc Balcer: Okay?

763
01:42:20.200 --> 01:42:24.280
Marc Balcer: So it would look something like this. Let me come back over here.

764
01:42:26.310 --> 01:42:27.620
Marc Balcer: And so

765
01:42:28.090 --> 01:42:29.579
Marc Balcer: this is what you get

766
01:42:32.770 --> 01:42:36.299
Marc Balcer: alright. And so it's pardon me.

767
01:42:37.450 --> 01:42:59.330
Marc Balcer: it's gonna give me, you know, again, you know, the same set of tests. I haven't corrected anything. By the way, I haven't corrected chat. Gpt. I haven't, corrected Claude, with any of things like the formatting and what the values for purpose ought to be, and so forth. I'm just letting it kind of free associate and come back with different lists of test cases.

768
01:42:59.630 --> 01:43:00.510
Marc Balcer: Now.

769
01:43:00.680 --> 01:43:08.039
Marc Balcer: the key notion of this is, and what I find is really valuable in all of this doing this so early

770
01:43:08.270 --> 01:43:09.480
Marc Balcer: is that

771
01:43:11.530 --> 01:43:18.189
Marc Balcer: I'm not prejudiced by what I have already done what I've already seen.

772
01:43:18.625 --> 01:43:27.569
Marc Balcer: I'm more likely to be thinking broadly about the whole problem than if I were letting the if I were

773
01:43:29.580 --> 01:43:31.080
Marc Balcer: working with this.

774
01:43:31.890 --> 01:43:38.720
Marc Balcer: you know well, into the the development phases. Okay? So I let AI do the job of managing. Now.

775
01:43:38.740 --> 01:43:57.479
Marc Balcer: one thing, though, that is important to do is to partition the problem by different subject matters. Okay? So I'm going to ask Chatgp. I'm going to ask Claude in this case to partition the set of test cases into the core calculation tests.

776
01:43:57.510 --> 01:43:58.879
Marc Balcer: ui tests

777
01:43:58.960 --> 01:44:00.010
Marc Balcer: and

778
01:44:00.020 --> 01:44:06.540
Marc Balcer: other non functional requirements or other. You know, what we sometimes call quality attributes.

779
01:44:06.780 --> 01:44:13.669
Marc Balcer: The value of doing this is that you know, typically there seem to almost be different

780
01:44:13.690 --> 01:44:33.040
Marc Balcer: subject matter experts in larger organizations who have opinions on each of these different areas. And so, for example, it makes a lot of sense to get the core business rules down right? Before we start to diddle in with the user interface. So separating those out can be very useful.

781
01:44:33.800 --> 01:44:39.899
Marc Balcer: Okay, so the bulk of the talk is really, how do we use the test to elicit requirements? Right?

782
01:44:40.110 --> 01:44:58.209
Marc Balcer: Well, in this case we do more of what I'm showing you and discussing earlier. We go through the scenarios, and we say, yep, that one's right. Nope, that one's wrong. We look at some of them, many of them, in fact, and go well, that's a very general statement. Any more detail on that.

783
01:44:59.140 --> 01:45:02.209
Marc Balcer: And then, of course, as we're going through the list.

784
01:45:02.640 --> 01:45:08.470
Marc Balcer: you know. We would like to think our AI was going to be very complete. It's not but

785
01:45:08.580 --> 01:45:15.160
Marc Balcer: what will happen is when you start to think about tests in one area or one set of tests.

786
01:45:15.610 --> 01:45:26.750
Marc Balcer: it might prompt you or your team to think about other sets of tests. Again, we're fighting the blank page syndrome. We're using AI to fight the blank page syndrome, the problem of

787
01:45:26.820 --> 01:45:29.839
Marc Balcer: staring at a blank page and wondering, okay, what do I do now?

788
01:45:30.020 --> 01:45:48.380
Marc Balcer: People are much better at criticizing real content than creating something new by staring at a blank page. Okay? And in fact, this idea is very related to the notion of hallucinations, because one of the questions I always ask students is.

789
01:45:48.510 --> 01:45:52.329
Marc Balcer: you know, you get a set of test cases like, you know things like

790
01:45:52.907 --> 01:45:58.279
Marc Balcer: decimal mileage and change in rates and things like that.

791
01:45:58.500 --> 01:46:01.060
Marc Balcer: How did AI know to come up with that?

792
01:46:01.550 --> 01:46:18.060
Marc Balcer: Okay, well, let me lose in training data, or you know, it had seen previous systems or test cases, or, you know, discussions or something. And what AI comes up with is kind of what we would consider to be

793
01:46:18.090 --> 01:46:24.110
Marc Balcer: typical expected behavior. Now that typical expected behavior might be right.

794
01:46:24.320 --> 01:46:25.969
Marc Balcer: But it might be wrong.

795
01:46:26.230 --> 01:46:30.029
Marc Balcer: But you know what the next best thing to being told you're right is being told you're wrong.

796
01:46:30.380 --> 01:46:40.219
Marc Balcer: And so I like to refer to these as good hallucinations as the AI is hallucinating things as it's making stuff up that we didn't tell it.

797
01:46:41.500 --> 01:46:47.960
Marc Balcer: That can be a good thing, because it might identify things. Oh, wow! I never thought about that, or Oh, wow!

798
01:46:48.050 --> 01:46:55.529
Marc Balcer: No, that's out of scope. We're not going to have decimal mileage, for example. So we'll say, that's that's out of scope. We'll cut that out.

799
01:46:55.750 --> 01:47:00.329
Marc Balcer: or you know, maybe other test cases where it's just plain wrong.

800
01:47:00.670 --> 01:47:03.320
Marc Balcer: right? That's perfectly fine.

801
01:47:03.420 --> 01:47:07.869
Marc Balcer: But a lot of cases we'll find, for example, that

802
01:47:07.940 --> 01:47:12.179
Marc Balcer: you know our knowledge as analysts and testers.

803
01:47:12.540 --> 01:47:13.560
Marc Balcer: We'll

804
01:47:13.580 --> 01:47:22.520
Marc Balcer: come to play because we'll look at the set of tests and we'll wait a minute. It's talking about earliest and latest valid dates.

805
01:47:24.080 --> 01:47:24.900
Marc Balcer: Okay.

806
01:47:25.130 --> 01:47:30.059
Marc Balcer: it kind of reminds me of this. The the people who say to me, Well, you know.

807
01:47:30.290 --> 01:47:35.740
Marc Balcer: we need, you know, in test case 32, you know, check for valid input.

808
01:47:36.370 --> 01:47:38.429
Marc Balcer: To which I respond, what's valid?

809
01:47:40.360 --> 01:47:42.319
Marc Balcer: What makes it not invalid?

810
01:47:44.740 --> 01:47:45.770
Marc Balcer: Guess what

811
01:47:45.930 --> 01:47:49.129
Marc Balcer: this is, where we need to catch it. Okay? So

812
01:47:49.220 --> 01:47:52.830
Marc Balcer: I'll ask here, what are the tests for minimum and mileage.

813
01:47:53.120 --> 01:47:56.859
Marc Balcer: you know, maximum date and mileage values? Okay?

814
01:47:56.910 --> 01:47:59.020
Marc Balcer: So it'll come back with the test cases.

815
01:47:59.270 --> 01:48:12.210
Marc Balcer: But and then we can begin to engage in a dialogue, for example, about the business rules. We realize that. Hey? Wait a minute. We never told it that there were business rules around this. We never said things like.

816
01:48:12.430 --> 01:48:28.780
Marc Balcer: Oh, by the way, you know, the business rule is that all expenses need to be submitted within 30 days of being incurred. Now, yeah, traditional requirements approach would have been to say, we have to make sure we capture all the business rules upfront.

817
01:48:29.370 --> 01:48:39.779
Marc Balcer: But you know what? Just the fact that you captured a bunch of business rules upfront doesn't necessarily mean you got all of them, or you got the ones that really matter for testing.

818
01:48:39.970 --> 01:48:45.989
Marc Balcer: And so as a result, what we're doing here is we're looking at the test cases and going. Okay, that test case is too general.

819
01:48:47.390 --> 01:48:56.759
Marc Balcer: How do I know it's valid, I mean. And again, it comes back to the difference between a scenario which can be informal, and a test case which hacks has to be executable.

820
01:48:56.780 --> 01:48:58.430
Marc Balcer: If you gave a

821
01:48:58.470 --> 01:48:59.970
Marc Balcer: the scenario

822
01:49:00.150 --> 01:49:02.689
Marc Balcer: like the one we saw

823
01:49:02.770 --> 01:49:06.480
Marc Balcer: on this, on one of the earlier pages to

824
01:49:06.650 --> 01:49:12.629
Marc Balcer: a manual tester, and said, it said, Enter a reasonable number of miles driven.

825
01:49:14.150 --> 01:49:21.229
Marc Balcer: What's a reasonable number? What's an unreasonable number. What's a too big number? What's the boundary between

826
01:49:21.350 --> 01:49:33.380
Marc Balcer: good and bad? Well, guess what? That's not just a testing problem. That's a business rule problem. And it turns out that by having to confront that issue we confront the problem of what's the business rule.

827
01:49:33.980 --> 01:49:49.030
Marc Balcer: right? And we can enumerate those things, and we enumerate them in a kind of a just in time basis, as we need that information now, traditionally, again, this kind of thing might have been the result of a conversation between a

828
01:49:49.360 --> 01:49:51.090
Marc Balcer: subject matter expert

829
01:49:51.100 --> 01:49:57.250
Marc Balcer: and a oops. There's my hand again, a subject matter expert and a developer.

830
01:49:57.660 --> 01:50:00.559
Marc Balcer: But where was the tester in all of that.

831
01:50:00.700 --> 01:50:04.039
Marc Balcer: Where was the agreement about those requirements? Okay.

832
01:50:05.230 --> 01:50:09.210
Marc Balcer: so bottom line is lots of work needed to get those rules. Well.

833
01:50:09.870 --> 01:50:15.399
Marc Balcer: you can ask your AI, you can actually go through some of these exchanges and say, All right.

834
01:50:15.410 --> 01:50:18.929
Marc Balcer: In order to do the test correctly, you need to know our business rules right.

835
01:50:19.900 --> 01:50:20.740
Marc Balcer: Wow!

836
01:50:22.350 --> 01:50:23.960
Marc Balcer: That was a pretty big list.

837
01:50:24.870 --> 01:50:26.130
Marc Balcer: and even more

838
01:50:26.670 --> 01:50:33.470
Marc Balcer: right. And so, after doing that, you can take the answers to the questions, put them back into the chat.

839
01:50:33.760 --> 01:50:38.010
Marc Balcer: and then see, how does it reinterpret its your answers?

840
01:50:38.560 --> 01:50:39.580
Marc Balcer: Pardon me

841
01:50:39.660 --> 01:50:42.749
Marc Balcer: as a series of improved test cases.

842
01:50:43.380 --> 01:50:51.870
Marc Balcer: Right? And so at that point you're back to looking at your scenarios again. Again, there's there's an iterative process to doing this, you know.

843
01:50:51.940 --> 01:50:55.130
Marc Balcer: Is it right? Is it wrong? Is it

844
01:50:55.450 --> 01:50:58.939
Marc Balcer: needing more definition, is it out of scope.

845
01:50:58.970 --> 01:51:10.599
Marc Balcer: you know. Again, we're but what we're doing here is we're looking at. We're working with real concrete examples as opposed to glittering generalities. Okay.

846
01:51:13.490 --> 01:51:14.420
Marc Balcer: so what?

847
01:51:15.090 --> 01:51:15.960
Marc Balcer: Well.

848
01:51:16.420 --> 01:51:22.319
Marc Balcer: someone might say, well, you've done all this work. But I need a traditional requirements document.

849
01:51:22.630 --> 01:51:34.230
Marc Balcer: Well, that's fine. After we've gotten all done, all the work of getting to the specifics. AI is great at being able to take content once it has its model built out.

850
01:51:34.320 --> 01:51:50.169
Marc Balcer: It's great taking that content and putting it into different representations. Okay, you need the test. You need it in the form of use cases great. You need it in the form of this great right. Our job right here has been to refine the knowledge that the AI

851
01:51:50.230 --> 01:52:12.010
Marc Balcer: has by looking at specific scenarios and working through those specific scenarios. It might feel at 1st that this is a lot of work. But it's actually the kind of work that turns out to be really valuable because you're answering really important questions that either a developer or a tester or others might have about the problem.

852
01:52:12.140 --> 01:52:18.959
Marc Balcer: Now, years ago there was a phrase that was very popular for a while and then died off. All right. Called shift left.

853
01:52:19.170 --> 01:52:27.759
Marc Balcer: You know, the basic idea behind. It was, we're gonna take a lot of things that are testing oriented, you know. Testing was sort of on the right side of the.

854
01:52:28.000 --> 01:52:37.220
Marc Balcer: you know, the old waterfall picture, and we're going to move those testing activities earlier into the life cycle. You know, we're shifting them left.

855
01:52:37.770 --> 01:52:38.680
Marc Balcer: Okay?

856
01:52:38.850 --> 01:52:39.700
Marc Balcer: Well.

857
01:52:39.870 --> 01:52:43.540
Marc Balcer: this is kind of a very much of a shift left approach, because

858
01:52:43.670 --> 01:52:48.130
Marc Balcer: we started off with a quick drawing of the page. We went right into the test cases.

859
01:52:48.490 --> 01:53:01.190
Marc Balcer: and we were then getting the traditional requirements, not as the 1st step to be able to get to the detail, but rather as a derived object, something derived out of.

860
01:53:01.350 --> 01:53:06.080
Marc Balcer: You know those detailed scenarios now predict, you know, in practice.

861
01:53:06.100 --> 01:53:17.639
Marc Balcer: there's a lot of iteration between all of this. And you know there's another document in here that I find is really useful, which is putting together a user guide. I'm a big believer in. For you know.

862
01:53:17.820 --> 01:53:24.308
Marc Balcer: applications that involve user interaction. Write the user manual first, st or what does it look like to the user?

863
01:53:24.890 --> 01:53:32.299
Marc Balcer: get that information, you know, that will help to, you know, having people help to visualize the problem is really valuable.

864
01:53:33.320 --> 01:53:34.260
Marc Balcer: So

865
01:53:34.460 --> 01:53:37.725
Marc Balcer: finally, one real example of this

866
01:53:38.780 --> 01:53:39.600
Marc Balcer: couple

867
01:53:39.920 --> 01:53:42.030
Marc Balcer: years ago I was given an app.

868
01:53:43.717 --> 01:53:45.330
Marc Balcer: I said, years

869
01:53:45.630 --> 01:53:47.380
Marc Balcer: months ago I was given a nap

870
01:53:47.700 --> 01:53:50.020
Marc Balcer: and was told, can you test this?

871
01:53:52.250 --> 01:53:54.139
Marc Balcer: No documentation

872
01:53:54.520 --> 01:53:56.170
Marc Balcer: could look at the code.

873
01:53:57.600 --> 01:54:00.110
Marc Balcer: How do I know this? You know? Okay.

874
01:54:00.240 --> 01:54:01.110
Marc Balcer: so

875
01:54:02.760 --> 01:54:03.990
Marc Balcer: this is the

876
01:54:05.070 --> 01:54:07.450
Marc Balcer: well, you know some of the screens

877
01:54:07.750 --> 01:54:09.900
Marc Balcer: from this particular application.

878
01:54:10.210 --> 01:54:13.030
Marc Balcer: Okay, so again, what's this?

879
01:54:13.940 --> 01:54:14.670
Marc Balcer: Right?

880
01:54:14.880 --> 01:54:15.820
Marc Balcer: Got that?

881
01:54:16.649 --> 01:54:24.700
Marc Balcer: Can we identify a process out of this? In other words, how much can AI infer, or, as I like to say, here, hallucinate out of?

882
01:54:24.730 --> 01:54:27.220
Marc Balcer: You know the visuals of the screen?

883
01:54:27.280 --> 01:54:34.809
Marc Balcer: You know, if we look at the screens, can they kind of tell us what's supposed to be happening? And this is, by the way, this is not just a fun. Little.

884
01:54:34.850 --> 01:54:52.739
Marc Balcer: you know, exercise. There are many people who have come through, you know, testing and test automation classes and consulting that I've had to work with where they go. I'm working with legacy code, these legacy applications. There's not a lot documented on how these work.

885
01:54:53.020 --> 01:55:00.980
Marc Balcer: How do I teach people how these applications work, or how do I figure out a test strategy for these applications? Right?

886
01:55:01.400 --> 01:55:12.970
Marc Balcer: You know this is great. You know the ability to take the screen snapshot. Use the snipping tool, snap it, put it into Chat Gpt, and say, What's this?

887
01:55:13.610 --> 01:55:20.900
Marc Balcer: Right? That's very powerful. And then, of course, once you have that, then you can start the dialogue about how to make this work.

888
01:55:21.270 --> 01:55:22.190
Marc Balcer: So

889
01:55:22.310 --> 01:55:38.249
Marc Balcer: our 1st cut is always going to be too general. This is an iterative process. This is very much a case of you know. Try it. What do you get? Okay, correct. Try it again, correct, and so forth. But you know

890
01:55:38.320 --> 01:55:47.870
Marc Balcer: the goal is ultimately to get to the specifics and to try to let AI help with that process of developing the understanding.

891
01:55:47.970 --> 01:55:48.810
Marc Balcer: Okay.

892
01:55:49.110 --> 01:55:55.299
Marc Balcer: so AI can really help to change the process. It can help us really to figure things out.

893
01:55:55.430 --> 01:56:14.140
Marc Balcer: You know, it produces the big list of questions. One thing somebody told me, you know, a few months ago, was, Smes are more comfortable answering specific questions than being told by an asked by an interviewer. Can you provide me with your requirements?

894
01:56:14.740 --> 01:56:44.550
Marc Balcer: Right. And so, being able to answer specific questions driven off, of what do you see on the page? What do you see in the documents and so forth can be very, very valuable. And of course the legacy applications where there isn't documentation this can be a really big asset in that process. Now, of course you can't enter into this without having the background knowledge. You know. So this is not an activity. I would say that you want to give to

895
01:56:44.560 --> 01:56:46.279
Marc Balcer: somebody who is

896
01:56:46.450 --> 01:57:05.610
Marc Balcer: not knowledgeable about software, not knowledgeable about your business, not knowledgeable about your application. This is the kind of thing that can really augment the capabilities of somebody who's knowledgeable in these areas. You know, to, you know, make things happen a whole lot faster. So

897
01:57:06.030 --> 01:57:06.900
Marc Balcer: again.

898
01:57:07.340 --> 01:57:08.740
Marc Balcer: this is shift left

899
01:57:08.920 --> 01:57:16.659
Marc Balcer: right. And if you haven't thought about being agile, incremental and iterative, this can help you in that way.

900
01:57:17.885 --> 01:57:18.620
Marc Balcer: So

901
01:57:20.590 --> 01:57:24.269
Marc Balcer: if you're having trouble getting the big picture. You can work up from examples.

902
01:57:24.819 --> 01:57:33.600
Marc Balcer: Generating it. You know, these scenarios through AI can make the whole process a whole lot faster, a whole lot more responsive.

903
01:57:33.640 --> 01:57:37.840
Marc Balcer: And then finally, Sam, you know you were one of our 1st speakers today.

904
01:57:38.100 --> 01:57:54.679
Marc Balcer: You know, you mentioned the Carnegie Hall way back at the beginning I had put together. You know I thought of the very same thing. How do you get good at doing generative? AI. It's practice practice practice, you know. The more of this that you do, the more opportunities that you'll see.

905
01:57:54.910 --> 01:57:55.680
Marc Balcer: So

906
01:57:56.162 --> 01:57:58.770
Marc Balcer: Want to thank you and take questions.

907
01:57:59.900 --> 01:58:04.340
Lara Hill: Awesome, mark. Thank you so much. You're getting some great feedback in the comments.

908
01:58:04.340 --> 01:58:05.380
David Mantica--Co-host!!!: Man mark with his.

909
01:58:05.380 --> 01:58:07.699
Lara Hill: Seen any questions, David, have you.

910
01:58:07.700 --> 01:58:11.749
David Mantica--Co-host!!!: Not yet mark with his cold, though, man, he's rocking it today.

911
01:58:12.520 --> 01:58:13.860
Lara Hill: Oh, you can't even tell.

912
01:58:14.120 --> 01:58:15.259
David Mantica--Co-host!!!: He's rocking it today.

913
01:58:15.260 --> 01:58:15.870
Marc Balcer: Oh!

914
01:58:16.370 --> 01:58:23.239
David Mantica--Co-host!!!: All right. Well, Mark will be on. If you have any specific questions, go to what tool for the cartoon. Did you use a tool for the cartoon?

915
01:58:23.240 --> 01:58:25.680
Marc Balcer: That was an early version of Dali.

916
01:58:25.680 --> 01:58:26.130
David Mantica--Co-host!!!: Nice.

917
01:58:26.130 --> 01:58:28.139
Marc Balcer: A year ago. It's actually pretty good.

918
01:58:28.140 --> 01:58:30.200
David Mantica--Co-host!!!: That's actually pretty good for Dolly.

919
01:58:30.420 --> 01:58:35.079
Marc Balcer: Oh, but you know it took about, you know, 22 iterations to get it to what I wanted.

920
01:58:35.410 --> 01:58:40.810
David Mantica--Co-host!!!: 22, but still faster than if you had to design it yourself.

921
01:58:40.810 --> 01:58:41.720
Marc Balcer: Oh, I couldn't.

922
01:58:41.720 --> 01:58:48.100
David Mantica--Co-host!!!: Yeah, you wouldn't. That's exactly. And it communicates a much, a much more powerful message. I love that part of it.

923
01:58:48.250 --> 01:58:53.009
David Mantica--Co-host!!!: Okay, Laura, do we have a break, or we go right into 2 o'clock. Where are we at.

924
01:58:53.750 --> 01:59:07.350
Lara Hill: So right now we are having a small break next up we have Om Hashmi. So if we could just take 5 and come back. Let's say let's come back at 2 0. 5. So.

925
01:59:07.350 --> 01:59:12.580
David Mantica--Co-host!!!: Yeah, it'd be great. Give a 6 min, bio, break and get people to relax a bit.

926
01:59:12.650 --> 01:59:15.480
David Mantica--Co-host!!!: share with Mark if they have any more follow-ups with them.

927
01:59:15.840 --> 01:59:17.450
David Mantica--Co-host!!!: or indoor Chris.

928
01:59:19.250 --> 01:59:26.349
Lara Hill: Absolutely, and we want to do another drawing. So when everybody comes back I'll do a quick drawing, and then we'll go right into Ohm's presentation.

929
01:59:27.190 --> 01:59:27.620
David Mantica--Co-host!!!: That.

930
01:59:27.620 --> 01:59:28.210
Michael Wolf: Oh, well.

931
01:59:28.210 --> 01:59:29.030
David Mantica--Co-host!!!: And then.

932
01:59:29.030 --> 01:59:31.660
Michael Wolf: Let Dolly do the drawing. But don't bother.

933
01:59:31.660 --> 01:59:32.725
Lara Hill: No.

934
01:59:33.790 --> 01:59:38.849
David Mantica--Co-host!!!: Michael, and rare form. There you go.

935
01:59:39.090 --> 01:59:39.885
Michael Wolf: No.

936
01:59:40.680 --> 01:59:44.710
David Mantica--Co-host!!!: I'm like what that shirt man that that shirt's pretty, you know.

937
01:59:44.710 --> 01:59:47.930
Michael Wolf: Thought about it this morning. I wanted to see if anybody's gonna be paying attention.

938
01:59:47.930 --> 01:59:52.430
David Mantica--Co-host!!!: Oh, I'm paying attention, man, I'm like Michael's got it going today.

939
01:59:52.430 --> 01:59:56.461
Michael Wolf: I. I also have a blue suit. I could wear that after the break baby.

940
01:59:56.730 --> 02:00:00.620
David Mantica--Co-host!!!: And you know the reference right?

941
02:00:01.820 --> 02:00:02.540
David Mantica--Co-host!!!: Do you know the.

942
02:00:02.540 --> 02:00:03.170
Michael Wolf: I'm in number.

943
02:00:03.170 --> 02:00:03.850
David Mantica--Co-host!!!: Difference.

944
02:00:04.120 --> 02:00:05.959
Michael Wolf: Because it won earlier today or.

945
02:00:05.960 --> 02:00:11.340
David Mantica--Co-host!!!: Yeah. Well, yeah, the ref I was teasing about dumb and dumber when talked about the purple and blue suit.

946
02:00:11.360 --> 02:00:17.629
David Mantica--Co-host!!!: That's the scene out of the show. That's the scene out of the movie, dumb and dumber, the blue and orange ruffle Tuxes.

947
02:00:18.820 --> 02:00:24.980
Michael Wolf: Yeah, we gotta watch out. Somebody recently said, you're using too many baby boomer references.

948
02:00:24.980 --> 02:00:26.369
David Mantica--Co-host!!!: Oh, yeah, I don't.

949
02:00:26.370 --> 02:00:28.099
Michael Wolf: Mentioned. I love Lucy.

950
02:00:28.100 --> 02:00:31.209
David Mantica--Co-host!!!: Oh, I didn't do that. But yes, I definitely have

951
02:00:31.370 --> 02:00:35.969
David Mantica--Co-host!!!: challenges with regard to the age of some of my references. I do have to.

952
02:00:36.120 --> 02:00:37.609
David Mantica--Co-host!!!: I'll find the way.

953
02:00:37.610 --> 02:00:41.894
Michael Wolf: I don't know people talking about it anymore.

954
02:00:42.430 --> 02:00:44.080
David Mantica--Co-host!!!: Oh, I'm I'm so old.

955
02:00:44.080 --> 02:00:47.750
Michael Wolf: I've been needing to get coffee for 3 h. I'm gonna go get some.

956
02:00:48.870 --> 02:00:49.940
David Mantica--Co-host!!!: Alright go for it!

957
02:00:55.620 --> 02:00:59.899
Jonathan Stephens (he/him): I'll be referencing 1939 things in my talk. So.

958
02:00:59.900 --> 02:01:00.930
David Mantica--Co-host!!!: Oh, Lordy!

959
02:01:01.260 --> 02:01:02.620
David Mantica--Co-host!!!: Oh, Lordy.

960
02:01:04.070 --> 02:01:08.270
David Mantica--Co-host!!!: that's okay. There it is. Laurel Hill. You're the best.

961
02:01:08.920 --> 02:01:13.089
David Mantica--Co-host!!!: Somebody asked for the link to the

962
02:01:13.500 --> 02:01:18.130
David Mantica--Co-host!!!: I'm sorry to the Chat Gtp class. I was doing the accelerated delivery one. Hold on a second.

963
02:01:18.260 --> 02:01:23.460
David Mantica--Co-host!!!: Let me put the Gpt class up here. Jeepers. I'm so slow.

964
02:01:27.460 --> 02:01:28.929
David Mantica--Co-host!!!: There we are.

965
02:01:37.710 --> 02:01:40.229
David Mantica--Co-host!!!: There's a link to the purpose-built class.

966
02:01:41.350 --> 02:01:42.470
Lara Hill: Awesome.

967
02:01:44.680 --> 02:01:48.149
Lara Hill: We actually have 9 AI classes now. So I'll put a link

968
02:01:48.280 --> 02:01:49.900
Lara Hill: to all

969
02:01:50.560 --> 02:01:53.229
Lara Hill: AI courses that we currently

970
02:01:54.610 --> 02:01:56.920
Lara Hill: have up on our website.

971
02:02:01.630 --> 02:02:18.430
Lara Hill: you will all get a coupon code for 25% off any of those courses. So look out for that in the follow up email. If you want to sign up for any continuing education through skills development group, we offer the Pdu codes as well. So

972
02:02:18.850 --> 02:02:23.210
Lara Hill: definitely check that out and and use that 25% off. If you're interested.

973
02:02:23.760 --> 02:02:25.384
David Mantica--Co-host!!!: You know, I'm sharing with

974
02:02:26.450 --> 02:02:31.939
David Mantica--Co-host!!!: one person in particular, and we're talking Laura, about the possibility of doing maybe a Mini conference for developers.

975
02:02:33.890 --> 02:02:34.419
Lara Hill: That'd be cool.

976
02:02:34.420 --> 02:02:38.770
David Mantica--Co-host!!!: With the stuff that Mark was talking about. Stuff that Jonathan's going to be talking about. It might be.

977
02:02:38.790 --> 02:02:42.489
David Mantica--Co-host!!!: you know. Even Ohm's presentation will have some relevance to

978
02:02:42.710 --> 02:02:47.379
David Mantica--Co-host!!!: development because it could be more technical minded. And how you manage a technical project.

979
02:02:47.790 --> 02:02:52.589
David Mantica--Co-host!!!: Yeah, yeah, thinking that might be something. Yeah, we'll we'll throw the idea around a little bit more.

980
02:02:55.710 --> 02:02:56.750
David Mantica--Co-host!!!: Oh.

981
02:02:57.760 --> 02:03:02.549
David Mantica--Co-host!!!: how you feeling, miss? Lh, you stretching out. Yeah.

982
02:03:02.550 --> 02:03:05.100
Lara Hill: Yeah, a little bit. How about you? You need a break.

983
02:03:05.100 --> 02:03:07.873
David Mantica--Co-host!!!: I snuck out twice for bio break.

984
02:03:08.270 --> 02:03:09.700
Lara Hill: But what about lunch?

985
02:03:09.700 --> 02:03:11.519
David Mantica--Co-host!!!: No no lunch today.

986
02:03:11.520 --> 02:03:13.320
Lara Hill: You're just gonna go till the end.

987
02:03:13.320 --> 02:03:18.659
David Mantica--Co-host!!!: Yeah. Going to the end. I got enough repository here that you can

988
02:03:19.470 --> 02:03:23.450
David Mantica--Co-host!!!: can last.

989
02:03:23.730 --> 02:03:24.380
Lara Hill: Oh!

990
02:03:24.380 --> 02:03:27.139
David Mantica--Co-host!!!: Oh, man! That could last for a few days!

991
02:03:27.140 --> 02:03:28.960
Om Hashmi: All right, David. Can you see my screen.

992
02:03:29.540 --> 02:03:30.379
David Mantica--Co-host!!!: We hear you.

993
02:03:30.380 --> 02:03:33.619
Lara Hill: Hey! Welcome to the stage, Joan! Glad to have you here.

994
02:03:33.620 --> 02:03:34.550
Om Hashmi: Thanks. Everyone.

995
02:03:36.290 --> 02:03:38.670
David Mantica--Co-host!!!: I cannot believe it's 78 degrees

996
02:03:38.730 --> 02:03:41.960
David Mantica--Co-host!!!: when we're so closed in November in Raleigh, North Carolina.

997
02:03:43.640 --> 02:03:44.480
David Mantica--Co-host!!!: Wow!

998
02:03:45.100 --> 02:03:46.010
David Mantica--Co-host!!!: Crazy!

999
02:03:46.910 --> 02:03:49.480
David Mantica--Co-host!!!: It is pretty, though it's gorgeous out there.

1000
02:03:50.220 --> 02:03:53.850
Lara Hill: Yes, I see the question about AI for software testing.

1001
02:03:54.070 --> 02:03:55.860
Lara Hill: Yes.

1002
02:03:56.090 --> 02:04:00.790
Lara Hill: 20% discount. That's correct. 3 or more attendees from the same company.

1003
02:04:02.690 --> 02:04:08.209
Lara Hill: So the 25% off code that I'll send is just for attendees of the conference.

1004
02:04:08.210 --> 02:04:09.129
David Mantica--Co-host!!!: For each individual.

1005
02:04:09.130 --> 02:04:13.529
Lara Hill: And you have 3 or more. Then you can get 20% for those additional folks.

1006
02:04:24.400 --> 02:04:26.000
David Mantica--Co-host!!!: And Mark is.

1007
02:04:26.650 --> 02:04:28.199
David Mantica--Co-host!!!: I think, always the

1008
02:04:28.390 --> 02:04:30.099
David Mantica--Co-host!!!: presented for that right now.

1009
02:04:31.140 --> 02:04:35.390
David Mantica--Co-host!!!: not presenter, the instructor. When you get to that level, it's instruction.

1010
02:04:51.530 --> 02:04:53.520
David Mantica--Co-host!!!: Yeah, she'll true that

1011
02:04:53.700 --> 02:04:57.150
David Mantica--Co-host!!!: where she and I are trading trading messages about the

1012
02:04:57.610 --> 02:04:59.159
David Mantica--Co-host!!!: the Raleigh weather.

1013
02:04:59.940 --> 02:05:05.539
Lara Hill: Well, at least the kids will have pleasant, trick-or-treating weather tomorrow. I can't work.

1014
02:05:05.540 --> 02:05:05.940
David Mantica--Co-host!!!: Trees.

1015
02:05:05.940 --> 02:05:06.940
Lara Hill: Freezing, cold.

1016
02:05:07.440 --> 02:05:11.299
David Mantica--Co-host!!!: Yeah, but here's a question, trick or treating. Would you rather be cold or sweating?

1017
02:05:13.090 --> 02:05:17.390
David Mantica--Co-host!!!: See for me, trick or treating is about being cold. That's part of the whole shtick right.

1018
02:05:17.390 --> 02:05:20.760
Lara Hill: Well, you're from up north, so you probably it was probably always cold, for.

1019
02:05:20.760 --> 02:05:21.850
David Mantica--Co-host!!!: Collaps, but.

1020
02:05:21.850 --> 02:05:25.730
Lara Hill: Here. It could go either way. It could be really cold, could be warm.

1021
02:05:25.730 --> 02:05:27.530
Jonathan Stephens (he/him): Highly cost, independent.

1022
02:05:28.880 --> 02:05:30.319
David Mantica--Co-host!!!: In what way, Jonathan.

1023
02:05:31.390 --> 02:05:36.030
Jonathan Stephens (he/him): I went as a wookie, or as what's an ewok. That was a very warm year.

1024
02:05:36.030 --> 02:05:41.959
David Mantica--Co-host!!!: Oh, costume! I thought you said yes, it is definitely costume dependent. Here's the other question.

1025
02:05:42.510 --> 02:05:45.579
David Mantica--Co-host!!!: does anybody here give out full size candy bars

1026
02:05:45.810 --> 02:05:53.020
David Mantica--Co-host!!!: because my crazy wife gives out full-size candy bars I spend. I mean, it's like the Costco Bill, or whether Bj's bill is just insane.

1027
02:05:53.020 --> 02:05:53.690
Lara Hill: I'm coming to.

1028
02:05:53.690 --> 02:05:54.200
David Mantica--Co-host!!!: Thank you.

1029
02:05:54.200 --> 02:05:54.520
Lara Hill: Richard.

1030
02:05:54.520 --> 02:05:58.780
David Mantica--Co-host!!!: Thank you. Thank you. No, I the Koya, I love you.

1031
02:05:58.780 --> 02:06:01.989
Lara Hill: Everyone. Every kid in the neighborhood loves her.

1032
02:06:02.220 --> 02:06:07.060
David Mantica--Co-host!!!: Oh, yeah. Well, it's what dogs do, too. She's a treat lady with a dog. She gives dogs treats all the time.

1033
02:06:07.620 --> 02:06:12.470
David Mantica--Co-host!!!: We give candy bars for kids and beer for the adults. It's actually pretty.

1034
02:06:12.930 --> 02:06:14.780
David Mantica--Co-host!!!: is it? Cold beer, Alan?

1035
02:06:16.890 --> 02:06:24.579
David Mantica--Co-host!!!: No full nickel out. I love you, I just I don't get it. I never got a full size candy bar, trick-or-treating ever.

1036
02:06:26.480 --> 02:06:31.949
David Mantica--Co-host!!!: Those parents are probably pretty upset. They probably are great point.

1037
02:06:33.750 --> 02:06:34.450
David Mantica--Co-host!!!: you know, the biggest.

1038
02:06:34.450 --> 02:06:38.359
Lara Hill: Be taking them, and kind of stashing those away for themselves. That's what I would have done.

1039
02:06:38.360 --> 02:06:47.159
David Mantica--Co-host!!!: Look at Sam's jealous shots for the adults. It's true. Here's my problem. We don't get enough people. Then all that candy's there, and I gain an extra 10 pounds

1040
02:06:47.600 --> 02:06:49.549
David Mantica--Co-host!!!: because she gets the good stuff. Man

1041
02:06:50.390 --> 02:06:52.190
David Mantica--Co-host!!!: skittles

1042
02:06:52.370 --> 02:06:57.549
David Mantica--Co-host!!!: man full-size skittles bag, you know, costs like freaking $3 in the store these days.

1043
02:06:57.550 --> 02:07:00.400
Lara Hill: How about everyone? Put in the chat? What's their favorite Halloween candy?

1044
02:07:00.400 --> 02:07:05.730
David Mantica--Co-host!!!: Give out apples. Yeah, favorite Halloween candy. Any chocolate I love.

1045
02:07:05.730 --> 02:07:06.939
Lara Hill: Candy, card.

1046
02:07:07.380 --> 02:07:08.060
David Mantica--Co-host!!!: Todd with.

1047
02:07:08.060 --> 02:07:08.660
Lara Hill: Yeah.

1048
02:07:08.660 --> 02:07:21.079
David Mantica--Co-host!!!: Give out apples. Alright, man, so Gil, you see candy corn. Look at apples for Michael. So Gil, you say candy corn. Would you take the candy corn if it was out of a package like if they just threw a bunch of candy corn in your bag, would you eat it?

1049
02:07:21.530 --> 02:07:24.090
David Mantica--Co-host!!!: And we have Reese's. We have skittles.

1050
02:07:24.690 --> 02:07:28.239
David Mantica--Co-host!!!: I freeze the chocolate. That's pretty cool thinking about it.

1051
02:07:28.240 --> 02:07:35.239
Lara Hill: I've done that too many a year with my kids, and then you just pull them out at Christmas time. Put it in their stuff.

1052
02:07:35.240 --> 02:07:35.840
David Mantica--Co-host!!!: Nothing but.

1053
02:07:35.840 --> 02:07:40.110
Michael Wolf: Favorite is actually the trick we used to sit in front of a

1054
02:07:40.460 --> 02:07:43.222
Michael Wolf: main street used to have a thing in Seattle.

1055
02:07:44.260 --> 02:07:51.320
Michael Wolf: kids would come up and say trick or treat, and my buddy would say, Which do you want? We've got both because he had the candy, and I had a can of worms

1056
02:07:52.890 --> 02:07:54.640
Michael Wolf: love the can of worms.

1057
02:07:54.740 --> 02:07:55.859
Michael Wolf: Oh, and that's.

1058
02:07:55.860 --> 02:07:59.539
David Mantica--Co-host!!!: Sounds like that sounds like Charlie Brown. There's another old reference.

1059
02:07:59.540 --> 02:08:00.500
Lara Hill: I just.

1060
02:08:00.500 --> 02:08:03.859
David Mantica--Co-host!!!: Watched the Charlie Brown Christmas, I mean Halloween. The other day man.

1061
02:08:03.860 --> 02:08:04.630
Michael Wolf: Do, do.

1062
02:08:04.630 --> 02:08:06.579
David Mantica--Co-host!!!: What did you get? I got a rock.

1063
02:08:06.580 --> 02:08:09.248
Michael Wolf: Come on, David, lead the dance, dude.

1064
02:08:10.380 --> 02:08:10.880
Michael Wolf: Jeez.

1065
02:08:10.960 --> 02:08:13.620
David Mantica--Co-host!!!: Do do do? Do

1066
02:08:14.000 --> 02:08:23.840
David Mantica--Co-host!!!: you know the best thing about the Charlie Brown? Special though, is the is the snoopy red Baron thing is so cool, and the music again did both the Halloween and the Christmas specials. That's some

1067
02:08:23.890 --> 02:08:25.710
David Mantica--Co-host!!!: classic music

1068
02:08:25.830 --> 02:08:32.820
David Mantica--Co-host!!!: stomach pain at the end of the day. Terrible congestion, all right. I think it's time to get back to reality. Don't you think, young lady.

1069
02:08:32.820 --> 02:08:42.339
Lara Hill: Yeah, let's do it. Welcome back everyone from your break. We have Om Ashmi here to present on technical program management. Om, are you ready to go.

1070
02:08:42.340 --> 02:08:43.990
Om Hashmi: Ready to go. You guys see my screen.

1071
02:08:44.440 --> 02:08:45.300
David Mantica--Co-host!!!: Yes, we do.

1072
02:08:45.300 --> 02:08:46.440
Lara Hill: Yeah. Awesome.

1073
02:08:46.440 --> 02:08:47.140
Om Hashmi: Good afternoon, everyone.

1074
02:08:47.140 --> 02:08:49.009
Lara Hill: Am I going to speak up just a bit.

1075
02:08:49.010 --> 02:08:49.700
Om Hashmi: Sure.

1076
02:08:49.700 --> 02:08:50.610
Lara Hill: Make sure you're.

1077
02:08:50.610 --> 02:09:00.329
Om Hashmi: Good afternoon and thanks everyone for my fellow presenters. It's been a wonderful day so far, and I'm looking forward to the the rest of the sessions and the keynote by David. I hope so

1078
02:09:02.345 --> 02:09:04.035
Om Hashmi: awesome. So

1079
02:09:06.440 --> 02:09:12.079
Om Hashmi: for the next 30 min or so I want to set the stage about. Why are we actually here?

1080
02:09:14.050 --> 02:09:18.450
Om Hashmi: Gartner's report from earlier this year projected that globally

1081
02:09:18.540 --> 02:09:21.310
Om Hashmi: we will be spending in the world of it

1082
02:09:21.470 --> 02:09:26.910
Om Hashmi: just over 5 trillion dollars. On it projects

1083
02:09:27.280 --> 02:09:29.790
Om Hashmi: 5 trillion with a T,

1084
02:09:30.640 --> 02:09:34.150
Om Hashmi: and they've been pretty spot on with the kind of spend that they've been tracking.

1085
02:09:34.470 --> 02:09:39.310
Om Hashmi: So if you're spending 5.1 trillion dollars over the course of 12 months.

1086
02:09:40.130 --> 02:09:46.690
Om Hashmi: One of the questions that they have been collaborating with C-suite with high level execs it leaders is around.

1087
02:09:46.920 --> 02:09:49.530
Om Hashmi: How effective has this spend really been?

1088
02:09:51.860 --> 02:09:55.570
Om Hashmi: So let's take a look at how effective it really has been. Right? So

1089
02:09:56.420 --> 02:09:57.550
Om Hashmi: turns out

1090
02:09:57.770 --> 02:09:58.979
Om Hashmi: our hit rate

1091
02:09:59.220 --> 02:10:03.280
Om Hashmi: on the traditional project. Triangle, scope, time

1092
02:10:03.530 --> 02:10:05.250
Om Hashmi: and dollars

1093
02:10:05.530 --> 02:10:09.359
Om Hashmi: is just under point 0 5%

1094
02:10:10.170 --> 02:10:15.009
Om Hashmi: meaning one in every 200. It projects

1095
02:10:15.230 --> 02:10:20.650
Om Hashmi: actually meet the success criteria across all of those 3 intended benefits.

1096
02:10:20.820 --> 02:10:21.760
Om Hashmi: time.

1097
02:10:21.970 --> 02:10:23.799
Om Hashmi: scope, and money.

1098
02:10:25.780 --> 02:10:27.840
Om Hashmi: Over 40% of them

1099
02:10:27.940 --> 02:10:30.380
Om Hashmi: of these initiatives end up being

1100
02:10:30.740 --> 02:10:33.480
Om Hashmi: overrun in terms of cost that gets spent on them.

1101
02:10:33.540 --> 02:10:38.089
Om Hashmi: More than half of them end up being delayed from the original intended time

1102
02:10:38.140 --> 02:10:39.400
Om Hashmi: or intended

1103
02:10:40.310 --> 02:10:41.440
Om Hashmi: timeline

1104
02:10:41.570 --> 02:10:43.489
Om Hashmi: delivery date, whatever you want to call that.

1105
02:10:44.190 --> 02:10:45.659
Om Hashmi: and more than

1106
02:10:45.770 --> 02:10:48.180
Om Hashmi: almost 2 thirds end up, being

1107
02:10:48.870 --> 02:10:52.270
Om Hashmi: not able to realize the value that was originally envisioned.

1108
02:10:53.430 --> 02:11:03.979
Om Hashmi: That's the market. That's the landscape where we generally run our it projects, and we haven't even touched the surface of everything that we've talked about all day around. Generative AI.

1109
02:11:04.650 --> 02:11:12.600
Om Hashmi: So Gardner did a more in-depth analysis about why is it that we keep missing the mark so vividly? And yet we keep spending

1110
02:11:12.760 --> 02:11:17.089
Om Hashmi: trillions of dollars globally on it projects.

1111
02:11:17.170 --> 02:11:22.609
Om Hashmi: Obviously the spend is justified because it technology runs the shops today.

1112
02:11:22.640 --> 02:11:25.139
Om Hashmi: But why is it that we continually keep missing.

1113
02:11:25.920 --> 02:11:27.059
David Mantica--Co-host!!!: And they.

1114
02:11:27.060 --> 02:11:31.110
Om Hashmi: Found out by surveys, by conversations with leaders

1115
02:11:32.020 --> 02:11:40.130
Om Hashmi: that about 13% individuals that participated in the discovery mentioned that the projects tend to

1116
02:11:40.180 --> 02:11:41.540
Om Hashmi: lack focus.

1117
02:11:42.190 --> 02:11:46.939
Om Hashmi: 11% came back with, Hey, we have consistent execution issues.

1118
02:11:46.950 --> 02:11:50.950
Om Hashmi: And about 10%, 9 and 9. And change came back with content issues.

1119
02:11:51.080 --> 02:11:57.370
Om Hashmi: And so across the board, there were areas that they recognized as contributing factors

1120
02:11:57.390 --> 02:12:02.809
Om Hashmi: as to why we continually, consistently miss the mark on it projects man.

1121
02:12:05.020 --> 02:12:21.089
Om Hashmi: Let's talk specifically in in generative AI world. Right like that's been the content. That's been the focus of the entire day. So far I happen to have been privileged that I was at the Gartner Summit, CIO. Summit last week, spent some time with some industry. Experts.

1122
02:12:21.550 --> 02:12:27.840
Om Hashmi: Take a while. Guess what was the most common reason that the Cios that were at the summit in Orlando

1123
02:12:28.450 --> 02:12:31.070
Om Hashmi: said, Here is the reason I'm here.

1124
02:12:31.550 --> 02:12:35.880
Om Hashmi: What do you all think? Why was every CIO that was in presence, and there were about

1125
02:12:35.960 --> 02:12:37.599
Om Hashmi: almost 10,000 people

1126
02:12:37.640 --> 02:12:39.580
Om Hashmi: over the course of the whole week there.

1127
02:12:39.890 --> 02:12:42.249
Om Hashmi: What was the most common reason for people to be there.

1128
02:12:46.760 --> 02:12:48.359
David Mantica--Co-host!!!: Of course. AI.

1129
02:12:49.250 --> 02:12:53.609
David Mantica--Co-host!!!: Now, one question real on your data. What do you? One person asked.

1130
02:12:54.143 --> 02:12:59.439
David Mantica--Co-host!!!: What does? What are content issues in that context for the information, what's your thought.

1131
02:12:59.440 --> 02:13:01.450
Om Hashmi: They were referring to

1132
02:13:01.570 --> 02:13:10.230
Om Hashmi: the overall holisticness of recognizing requirements, essentially content in the sense of the what needed to be built.

1133
02:13:10.240 --> 02:13:14.260
Om Hashmi: the moving target. The agility element of it requested.

1134
02:13:15.430 --> 02:13:22.609
David Mantica--Co-host!!!: So yes, definitely has to be Gartner. All those you know executive weenies there. Probably all I gotta know more about AI.

1135
02:13:22.610 --> 02:13:23.879
Om Hashmi: That's it, right?

1136
02:13:24.340 --> 02:13:26.750
David Mantica--Co-host!!!: Voice. Dude. That was a good Weenie voice.

1137
02:13:26.750 --> 02:13:34.669
Om Hashmi: So interesting enough. The keynote, the 1st keynote on Monday of last week literally started with, Hey, how much are we actually spending now on AI initiatives

1138
02:13:34.770 --> 02:13:40.539
Om Hashmi: just on Pocs alone. Organizations are spending anywhere from 300 K

1139
02:13:40.630 --> 02:13:42.530
Om Hashmi: to 3 million dollars

1140
02:13:42.820 --> 02:13:47.539
Om Hashmi: over a Poc that spans anywhere from a couple of weeks to 10 weeks.

1141
02:13:47.580 --> 02:13:52.670
Om Hashmi: That's the spend organizations are actually putting in. Now same question.

1142
02:13:53.510 --> 02:13:54.580
Om Hashmi: how are we doing.

1143
02:13:55.110 --> 02:13:57.529
David Mantica--Co-host!!!: Hey, poc is proof of concept, right.

1144
02:13:57.530 --> 02:13:59.030
Om Hashmi: Pocs. Proof of Concept.

1145
02:13:59.030 --> 02:13:59.907
David Mantica--Co-host!!!: Thank you. Alright!

1146
02:14:00.590 --> 02:14:02.879
Om Hashmi: So again, where did we end up?

1147
02:14:03.060 --> 02:14:04.929
Om Hashmi: Came back with finding out

1148
02:14:06.240 --> 02:14:15.670
Om Hashmi: one in 2, have mentioned any of the AI initiatives that they have tried or attempted in the form of proof of concepts, or in the form of actual executions.

1149
02:14:15.750 --> 02:14:21.699
Om Hashmi: have not met the Roi expectations, whether it was for personal gains. Whether it was for operational efficiencies.

1150
02:14:21.810 --> 02:14:27.579
Om Hashmi: business process, optimization, individual employee improvements and efficiencies.

1151
02:14:27.690 --> 02:14:30.510
Om Hashmi: one in 2 came back with did not meet.

1152
02:14:30.750 --> 02:14:33.239
Om Hashmi: Now. They are also recognizing the fact that

1153
02:14:33.540 --> 02:14:41.050
Om Hashmi: we have a ton of capabilities, opportunities available across the tools, the softwares, the products that get implemented across enterprises.

1154
02:14:41.350 --> 02:14:43.309
Om Hashmi: and we still struggle.

1155
02:14:43.430 --> 02:14:52.529
Om Hashmi: In fact, almost 3 and 4 struggle with actually utilizing efficiently any sort of generative AI tools that might be available at their disposal.

1156
02:14:52.710 --> 02:15:04.110
Om Hashmi: Most of us have already come across open as Chat Gpt. For those of us who are working in any sort of an enterprise that uses Microsoft. You must have heard and utilized some version of a

1157
02:15:04.150 --> 02:15:05.220
Om Hashmi: co-pilot

1158
02:15:05.450 --> 02:15:09.270
Om Hashmi: which is meant to be a generative AI based assistant. Smart assistant.

1159
02:15:10.850 --> 02:15:17.369
Om Hashmi: What's the worst thing in everything that we've captured over the course of the entirety of last week was

1160
02:15:17.900 --> 02:15:19.910
Om Hashmi: building on the

1161
02:15:20.010 --> 02:15:28.019
Om Hashmi: challenges as we are plagued with already in it projects where 40% of our projects end up being overrun in terms of dollars.

1162
02:15:28.360 --> 02:15:32.459
Om Hashmi: Gen. AI. Efforts can actually end up costing

1163
02:15:32.560 --> 02:15:35.130
Om Hashmi: nearly 5 to 10 x

1164
02:15:36.170 --> 02:15:42.699
Om Hashmi: of what was originally intended. So that's what's at stake for the next 30 min of this conversation.

1165
02:15:43.090 --> 02:15:48.689
Om Hashmi: This is why this whole idea of what we're going to talk about today is so critical.

1166
02:15:49.250 --> 02:15:57.779
David Mantica--Co-host!!!: Hey? Someone has. What are some of your thoughts about the barriers to AI adoption? Are you going to talk about that, or some thoughts you want to share.

1167
02:15:57.780 --> 02:15:58.659
Om Hashmi: There for us.

1168
02:15:58.860 --> 02:16:02.499
Om Hashmi: I'm going to talk a little bit about that. In in the second half of the

1169
02:16:02.820 --> 02:16:04.209
Om Hashmi: the presentation.

1170
02:16:04.560 --> 02:16:08.780
David Mantica--Co-host!!!: And and is there a way to turn up your turn up your mic a bit more.

1171
02:16:08.780 --> 02:16:10.180
Om Hashmi: Let me get a little closer.

1172
02:16:10.720 --> 02:16:11.780
David Mantica--Co-host!!!: Hey! Thanks!

1173
02:16:12.310 --> 02:16:13.539
Om Hashmi: Let me know if there's any better.

1174
02:16:15.570 --> 02:16:17.380
David Mantica--Co-host!!!: A bit. Keep rocking, though. Keep going.

1175
02:16:17.380 --> 02:16:17.750
Om Hashmi: Awesome.

1176
02:16:18.790 --> 02:16:24.029
Om Hashmi: So let's take a look at how the industry has been over the course of the last.

1177
02:16:24.290 --> 02:16:26.690
Om Hashmi: I want to say 3 to 5 years.

1178
02:16:27.250 --> 02:16:31.580
Om Hashmi: Look at the names! Look at the Logos, look at the brands that you see on the screen here.

1179
02:16:31.640 --> 02:16:32.959
Om Hashmi: Real big names.

1180
02:16:35.580 --> 02:16:36.719
Om Hashmi: What's common

1181
02:16:36.860 --> 02:16:38.560
Om Hashmi: in between all these companies?

1182
02:16:38.959 --> 02:16:40.139
Om Hashmi: What's consistent.

1183
02:16:41.770 --> 02:16:44.490
David Mantica--Co-host!!!: Regulatory. There's regulatory issues there.

1184
02:16:44.490 --> 02:16:48.870
Om Hashmi: Good. There is some level of regulatory elements associated. Yes.

1185
02:16:48.870 --> 02:16:51.370
David Mantica--Co-host!!!: Financial org. Someone says, here.

1186
02:16:51.370 --> 02:16:54.090
Om Hashmi: The financial orgs. They are related to

1187
02:16:54.170 --> 02:16:57.090
Om Hashmi: privacy. There's data. There is elements.

1188
02:16:57.090 --> 02:16:58.579
David Mantica--Co-host!!!: A lot of data.

1189
02:16:58.580 --> 02:16:59.539
Om Hashmi: A lot of data.

1190
02:16:59.980 --> 02:17:01.790
Om Hashmi: Here's what's even more interesting.

1191
02:17:02.660 --> 02:17:05.360
Om Hashmi: each of the 4 companies that are listed on the screen. Here

1192
02:17:05.600 --> 02:17:08.490
Om Hashmi: are one of the 1st ones that have actually gone out.

1193
02:17:08.490 --> 02:17:09.540
David Mantica--Co-host!!!: He killed their agile.

1194
02:17:09.540 --> 02:17:14.280
Om Hashmi: Completely killed the agile roles that existed in these organizations.

1195
02:17:14.540 --> 02:17:21.159
Om Hashmi: Capital one a couple years back, went on to lay off the entire agile division globally in one shot.

1196
02:17:21.809 --> 02:17:28.710
Om Hashmi: right the roles of an agile delivery lead the role of an agile coach, an enterprise Agilist, a scrum master eliminated

1197
02:17:29.020 --> 02:17:29.980
Om Hashmi: like this.

1198
02:17:30.680 --> 02:17:32.500
Om Hashmi: So where is the industry heading?

1199
02:17:33.549 --> 02:17:37.559
Om Hashmi: This is where Technical Program Management Institute comes in right.

1200
02:17:38.010 --> 02:17:41.059
Om Hashmi: The whole idea of the Institute is.

1201
02:17:41.190 --> 02:17:42.760
Om Hashmi: we have this vision.

1202
02:17:43.120 --> 02:17:46.890
Om Hashmi: Every it project should be run by a technologist.

1203
02:17:47.100 --> 02:17:50.400
Om Hashmi: Now, what's a technologist? Do I need to learn how to write code?

1204
02:17:50.559 --> 02:18:00.430
Om Hashmi: Do I now need to attend every single boot camp that's there. So that I can do estate type of activities. Do I need to become a Devops engineer? Oh, that's not the intention.

1205
02:18:00.549 --> 02:18:01.780
Om Hashmi: It's about

1206
02:18:01.820 --> 02:18:09.280
Om Hashmi: becoming intelligent and learned in the technology solution that you're building and deploying for your said customers.

1207
02:18:10.290 --> 02:18:20.999
Om Hashmi: The team that we've assembled as part of Tpm Institute is a team of professionals, technical program managers from the likes of Microsoft, Google, Amazon, Meta.

1208
02:18:21.080 --> 02:18:22.760
Om Hashmi: Netflix Apple.

1209
02:18:23.709 --> 02:18:27.820
Om Hashmi: with the philosophy that we are shifting from the

1210
02:18:28.170 --> 02:18:31.259
Om Hashmi: adage of going from project to product.

1211
02:18:31.700 --> 02:18:35.320
Om Hashmi: We are shifting from program to business.

1212
02:18:35.770 --> 02:18:39.749
Om Hashmi: And that's where the role of a technical program manager is going to come in

1213
02:18:39.930 --> 02:18:40.830
Om Hashmi: and

1214
02:18:41.520 --> 02:18:42.630
Om Hashmi: comes me.

1215
02:18:43.290 --> 02:18:45.670
Om Hashmi: My name is Omar Hashmi. I go by. Om.

1216
02:18:45.780 --> 02:18:51.960
Om Hashmi: I'm 1 of the Enterprise principal technical program managers currently supporting Amazon's retail experience team

1217
02:18:51.980 --> 02:18:59.620
Om Hashmi: been blessed to have served in a number of different capacities as a change agent, as a technologist, as an engineer. It's where my career started

1218
02:18:59.690 --> 02:19:04.569
Om Hashmi: in various different verticals, starting with oil and gas energy

1219
02:19:04.590 --> 02:19:08.659
Om Hashmi: utilities, consumer products. And most recently, with

1220
02:19:08.780 --> 02:19:11.709
Om Hashmi: most of you, if you're using a cell phone

1221
02:19:12.139 --> 02:19:14.629
Om Hashmi: which resembles a fruit

1222
02:19:15.299 --> 02:19:19.610
Om Hashmi: work on several different initiatives over there. So lot of.

1223
02:19:19.610 --> 02:19:23.369
David Mantica--Co-host!!!: Say, say, Ohm, someone has a question. Define a technologist.

1224
02:19:23.760 --> 02:19:33.370
Om Hashmi: Good. Just like I was mentioning before, a technologist is someone who has sound rudimentary knowledge of how technology gets built, worked and deployed.

1225
02:19:33.850 --> 02:19:37.689
Om Hashmi: They do not necessarily need to be somebody who writes code.

1226
02:19:37.820 --> 02:19:44.010
Om Hashmi: but should be very fluent and actively understanding of how to build, what to build

1227
02:19:44.090 --> 02:19:46.560
Om Hashmi: and how we test and how we deploy.

1228
02:19:49.880 --> 02:19:50.590
Om Hashmi: So.

1229
02:19:50.590 --> 02:19:51.399
David Mantica--Co-host!!!: Thank you.

1230
02:19:51.650 --> 02:20:09.199
Om Hashmi: When we start talking about running generative AI initiatives, right? So we've talked about utilization of a bunch of different tools that we got a chance. Some of my co-presenters and steam colleagues got a chance to demonstrate some of the tools that we've seen in action. Many of us might be using a lot of these tools in our daily lives already.

1231
02:20:09.600 --> 02:20:13.240
Om Hashmi: Here's how every generative AI initiative actually starts

1232
02:20:13.370 --> 02:20:21.530
Om Hashmi: right for those of us who have had a chance of actually playing with some of the tools or been a part of an implementation. This is how every one of these starts right expecting

1233
02:20:21.830 --> 02:20:27.439
Om Hashmi: holograms and and computer vision. And all these other fancy buzzwords coming in.

1234
02:20:27.640 --> 02:20:33.130
Om Hashmi: And here is how they actually end up being when it comes to actually

1235
02:20:33.210 --> 02:20:36.840
Om Hashmi: delivering Gen. AI solutions at the enterprise level.

1236
02:20:37.400 --> 02:20:38.360
Om Hashmi: Right? So

1237
02:20:38.870 --> 02:20:41.469
Om Hashmi: how do we go from the right to left

1238
02:20:41.580 --> 02:20:42.760
Om Hashmi: type of view

1239
02:20:43.610 --> 02:20:58.100
Om Hashmi: starts with getting an understanding of what is generative. AI. Now, it's not a novel concept. It's not something that just happened overnight, contrary to popular belief, when people started thinking that once Chat Gpt became mainstream. That's when generative AI came into existence.

1240
02:20:58.340 --> 02:21:01.640
Om Hashmi: Concepts of generative AI and

1241
02:21:01.740 --> 02:21:06.019
Om Hashmi: artificial intelligence in general have existed since the forties.

1242
02:21:06.100 --> 02:21:11.160
Om Hashmi: When we 1st started looking at rule-based algorithms to create the 1st chessboard.

1243
02:21:11.340 --> 02:21:18.560
Om Hashmi: the computerized rule-based chess that eventually ended up beating a human being for the 1st time back in the fifties

1244
02:21:18.880 --> 02:21:24.020
Om Hashmi: we built on that went into machine learning in the eighties. And then we built further, and we built

1245
02:21:24.050 --> 02:21:25.410
Om Hashmi: neural nets.

1246
02:21:25.580 --> 02:21:29.479
Om Hashmi: understanding those pieces is what led now to

1247
02:21:29.500 --> 02:21:32.290
Om Hashmi: generative retained models

1248
02:21:32.330 --> 02:21:37.040
Om Hashmi: that can now get us the ability to create new content.

1249
02:21:37.580 --> 02:21:40.059
Om Hashmi: That's what generative AI is in a nutshell.

1250
02:21:40.440 --> 02:21:46.129
Om Hashmi: Now, I wish it was as simple as what I just explained. And I wish it was just this much.

1251
02:21:46.230 --> 02:21:48.989
Om Hashmi: But the depth of the information, the depth

1252
02:21:49.040 --> 02:21:54.779
Om Hashmi: in the technology itself is what organizations today are actively looking for

1253
02:21:54.960 --> 02:22:21.550
Om Hashmi: when it comes to execution of their generative AI initiatives. And who's looking for it every single CIO, like I said before, at Gartner Summit last week clearly had the agenda. Hey? We have to go ahead and think about our governance for our Gen. AI. Initiatives. We have to think about our delivery of our Gen. AI initiatives. We need to staff appropriately. We need managers who are going to come in and run these in an ethically safe in a

1254
02:22:21.610 --> 02:22:25.800
Om Hashmi: a privacy-wise, safe, and acceptable manner

1255
02:22:26.020 --> 02:22:28.560
Om Hashmi: across every single vertical.

1256
02:22:32.100 --> 02:22:33.530
Om Hashmi: How big is the market?

1257
02:22:34.160 --> 02:22:38.750
Om Hashmi: It's projected to get to over a hundred 1 billion dollars over the next 5 years.

1258
02:22:39.630 --> 02:22:43.659
Om Hashmi: Right? We're already some of us are already utilizing or using

1259
02:22:43.680 --> 02:22:50.310
Om Hashmi: the different elements, the different pieces of generative AI in our daily lives. We've already started incorporating some of it

1260
02:22:50.330 --> 02:22:54.029
Om Hashmi: for content generation for writing emails, for

1261
02:22:54.320 --> 02:23:03.800
Om Hashmi: automating any simple task that exists in our life. Right? So think back to yourselves the last time someone had to create a visual on a Powerpoint presentation.

1262
02:23:04.340 --> 02:23:05.610
Om Hashmi: How long did it take?

1263
02:23:05.920 --> 02:23:09.999
Om Hashmi: Think about the series of steps that generally we would take to get to that point

1264
02:23:10.400 --> 02:23:23.900
Om Hashmi: right? We would generally go about going on Google doing some search, figuring out some content, putting it on a Powerpoint slide, looking at it, thinking about it, massaging it, then working on some shapes and icons and images

1265
02:23:24.310 --> 02:23:29.150
Om Hashmi: for those that were a little bit more advanced, they would go about searching for specific images.

1266
02:23:29.200 --> 02:23:32.609
Om Hashmi: try to crop images, modify images, work through that

1267
02:23:32.960 --> 02:23:35.720
Om Hashmi: today, with all the tools and technology that exists.

1268
02:23:36.070 --> 02:23:39.239
Om Hashmi: all of this can be automated soup to nuts

1269
02:23:39.790 --> 02:23:44.459
Om Hashmi: right? And this idea of us being able to bridge the gap

1270
02:23:44.640 --> 02:23:45.850
Om Hashmi: between

1271
02:23:46.040 --> 02:23:49.409
Om Hashmi: something that might work in the form of a prototype

1272
02:23:50.090 --> 02:23:54.570
Om Hashmi: to actually productionizing so that an Roi can be realized

1273
02:23:54.780 --> 02:23:58.579
Om Hashmi: is where the role of a technical program manager

1274
02:23:58.790 --> 02:24:01.870
Om Hashmi: thrives and is so essential and so critical.

1275
02:24:02.630 --> 02:24:09.179
Om Hashmi: So some of the roles that we've probably all seen in the past. A few of them listed out here.

1276
02:24:09.900 --> 02:24:19.039
Om Hashmi: We're all familiar with the project manager role, the product manager role, the product owners, the product managers, the program managers, the portfolio managers

1277
02:24:19.680 --> 02:24:21.340
Om Hashmi: no question for everyone here.

1278
02:24:21.880 --> 02:24:27.020
Om Hashmi: What's the responsibility of these roles in any enterprise. Small, medium, large, sized.

1279
02:24:28.120 --> 02:24:32.279
Om Hashmi: What's what is the what are these roles collectively responsible for.

1280
02:24:41.620 --> 02:24:43.929
David Mantica--Co-host!!!: All right. Folks delivering value.

1281
02:24:44.150 --> 02:24:46.160
Om Hashmi: Okay, good. They want. You want these

1282
02:24:46.580 --> 02:24:47.170
Om Hashmi: monitor.

1283
02:24:47.170 --> 02:24:52.859
David Mantica--Co-host!!!: That spoke like a true developer, monitoring, doing the work.

1284
02:24:52.990 --> 02:24:55.339
David Mantica--Co-host!!!: coordinating, and communicating.

1285
02:24:55.340 --> 02:24:57.689
Om Hashmi: Very good, coordinating, communicating.

1286
02:24:58.150 --> 02:24:59.740
Srinath Kondabathini: Providing the business.

1287
02:25:00.090 --> 02:25:02.010
Om Hashmi: Running the business. Very good.

1288
02:25:02.910 --> 02:25:03.760
David Mantica--Co-host!!!: Chair.

1289
02:25:04.810 --> 02:25:15.330
Om Hashmi: So bunch of different roles and responsibilities shared across these different roles. Let me tell you what I firsthand experienced about 6 weeks ago.

1290
02:25:15.430 --> 02:25:19.889
Om Hashmi: about 6 weeks ago I happened to be at an insurance clients

1291
02:25:20.200 --> 02:25:22.400
Om Hashmi: headquarters in the East Coast.

1292
02:25:22.880 --> 02:25:25.779
Om Hashmi: and I'm sitting down with one of the

1293
02:25:26.340 --> 02:25:31.930
Om Hashmi: leadership team members sitting as part of some of you might have attended what is called a scrum of scrums.

1294
02:25:32.420 --> 02:25:37.220
Om Hashmi: At that scrum of scrums we have a series of the rules that we see on the screen here.

1295
02:25:37.400 --> 02:25:44.390
Om Hashmi: Project managers were there some product owners? Were there, some scrum masters? Were there, some Agilists were there, and this

1296
02:25:44.870 --> 02:25:47.550
Om Hashmi: it, director it, senior director?

1297
02:25:48.160 --> 02:25:53.370
Om Hashmi: She asked the question. Hey? When is Project Unity getting completed

1298
02:25:55.150 --> 02:26:00.500
Om Hashmi: and the series of the crowd team members? They looked at each other for a second.

1299
02:26:01.130 --> 02:26:08.120
Om Hashmi: figuring out who is the most appropriate person to ask. Answer this question about when is Project Unity getting completed?

1300
02:26:09.010 --> 02:26:15.019
Om Hashmi: It took about 30 seconds for the assigned scrum master and the assigned project manager

1301
02:26:15.500 --> 02:26:17.440
Om Hashmi: to respond back by saying.

1302
02:26:18.520 --> 02:26:20.180
Om Hashmi: I'm just a scrum, master.

1303
02:26:21.160 --> 02:26:24.400
Om Hashmi: We'll have to check with the tech leads and get back to you on it.

1304
02:26:27.030 --> 02:26:30.040
Om Hashmi: Hence the need for a new role.

1305
02:26:31.000 --> 02:26:34.520
Om Hashmi: the role of a technical program manager.

1306
02:26:35.160 --> 02:26:41.629
Om Hashmi: It's a relatively new title, something that has not been in existence for the last 1015 years.

1307
02:26:41.850 --> 02:26:46.720
Om Hashmi: but it certainly has for the last 3 to 5 years.

1308
02:26:46.860 --> 02:26:52.480
Om Hashmi: It's a new role that has been identified by most of these West Coast companies that I was referring to earlier

1309
02:26:52.780 --> 02:27:00.119
Om Hashmi: because they realize the potential, the need for having a combination of the 3 pronged approach.

1310
02:27:01.360 --> 02:27:07.839
Om Hashmi: A technical program manager is an amalgamation, a consistent formulation of

1311
02:27:07.930 --> 02:27:09.300
Om Hashmi: being an Agilist.

1312
02:27:10.110 --> 02:27:11.750
Om Hashmi: Being a technologist

1313
02:27:12.140 --> 02:27:14.139
Om Hashmi: and being a project manager.

1314
02:27:15.220 --> 02:27:20.339
Om Hashmi: So the ability. The concoction of being able to play those 3 roles.

1315
02:27:20.410 --> 02:27:26.130
Om Hashmi: those 3 skill sets is what makes you a technical program manager. Now.

1316
02:27:26.390 --> 02:27:31.640
Om Hashmi: the role itself for those of you who are interested do me a favor. Go on your Linkedin page today

1317
02:27:31.930 --> 02:27:38.910
Om Hashmi: and on your Linkedin pages. Actually search for the job title technical program manager.

1318
02:27:39.870 --> 02:27:53.419
Om Hashmi: right? When you search for the title technical program manager. What you will see is it's gonna show you somewhere about 2,500 somewhere in that vicinity in terms of new job postings that are available for the title of a technical program manager

1319
02:27:54.250 --> 02:27:56.599
Om Hashmi: do the same for the term

1320
02:27:56.660 --> 02:27:59.009
Om Hashmi: or role scrum master.

1321
02:27:59.260 --> 02:28:01.340
Om Hashmi: That number is about half that today.

1322
02:28:02.940 --> 02:28:04.999
Om Hashmi: The industry is moving

1323
02:28:05.140 --> 02:28:11.570
Om Hashmi: from that traditional agile roles to the need of having technical program managers.

1324
02:28:11.810 --> 02:28:18.679
Om Hashmi: They're proficient in technology, because innately, they've upskilled themselves towards becoming a technologist.

1325
02:28:18.820 --> 02:28:23.629
Om Hashmi: And they understand how to execute complex programs and projects.

1326
02:28:23.920 --> 02:28:30.649
Om Hashmi: Now, this role started with the technical organizations started with the likes of the apples, the Metas, the Microsofts.

1327
02:28:31.160 --> 02:28:33.790
Om Hashmi: But more recently, I want to say over the last

1328
02:28:34.200 --> 02:28:40.299
Om Hashmi: 2 and a half to 3 years, especially once capital one came out and said, That's it. No more agile roles.

1329
02:28:40.330 --> 02:28:55.590
Om Hashmi: It has gained a lot of traction across non-technical organizations as well. We know utility companies who are doing this. We know banks who are doing this, insurance companies who are doing this, healthcare clients who are doing this shifting from traditional agile roles

1330
02:28:55.810 --> 02:28:57.930
Om Hashmi: to technical program management.

1331
02:28:57.960 --> 02:29:00.689
Om Hashmi: So what is their technical program manager? What's

1332
02:29:00.720 --> 02:29:02.260
Om Hashmi: what is the expectation

1333
02:29:02.680 --> 02:29:03.410
Om Hashmi: now?

1334
02:29:03.680 --> 02:29:10.110
Om Hashmi: Because the role is a combination of these 3 pillars like, I said before, the whole idea becomes

1335
02:29:10.180 --> 02:29:28.609
Om Hashmi: realizing you still have your traditional project management responsibilities. You're still helping out with building out a plan. You're also still figuring out how to manage your risks and how to manage your decisions and raid logs and resources, and all of that other fun, things that need to go into practice. If you were a traditional project manager.

1336
02:29:29.090 --> 02:29:43.330
Om Hashmi: because we are generally in all of these organizations, still executing in some sort of an iterative fashion. The expectation is, you couple those project management skills with your knowledge of agile practices being iterative in nature

1337
02:29:43.370 --> 02:29:45.530
Om Hashmi: ability to go back and say.

1338
02:29:45.610 --> 02:29:49.909
Om Hashmi: hey? What can we do in this iteration that we didn't do in the previous iteration retrospection?

1339
02:29:50.110 --> 02:29:53.300
Om Hashmi: But the last piece that's the most critical piece

1340
02:29:53.310 --> 02:29:58.360
Om Hashmi: is upskillment in the technologist aspect of becoming

1341
02:29:58.840 --> 02:30:02.729
Om Hashmi: somebody who treats the program or the project as an entrepreneur.

1342
02:30:03.640 --> 02:30:07.390
Om Hashmi: So somebody who's interested in actually delivering a Gen. AI initiative.

1343
02:30:07.450 --> 02:30:16.290
Om Hashmi: the deeper you can understand the technology elements of it the better you serve the program or the project to meet the return on that investment.

1344
02:30:16.480 --> 02:30:27.009
Om Hashmi: These are technically high complexity initiatives. That's what organizations are looking for. That's why they're investing a ton of money in trying to approach

1345
02:30:27.180 --> 02:30:30.409
Om Hashmi: technical program managers to run these types of

1346
02:30:30.710 --> 02:30:34.219
Om Hashmi: initiatives because they don't just require coordination.

1347
02:30:34.300 --> 02:30:38.160
Om Hashmi: They require a level of strategic and technical alignment

1348
02:30:38.190 --> 02:30:42.950
Om Hashmi: across the various different groups that get involved in any Gen. AI initiative.

1349
02:30:43.660 --> 02:30:50.870
Om Hashmi: So most recently, we've been working with an energy customer where we have the need for 3 technical program managers

1350
02:30:50.960 --> 02:30:58.129
Om Hashmi: where the ask was, Hey, how do we go about helping our call center because we get a bunch of calls about

1351
02:30:59.100 --> 02:31:05.490
Om Hashmi: our bills? Not correct service needs to be updated. How do we improve the handling time

1352
02:31:05.540 --> 02:31:06.550
Om Hashmi: on

1353
02:31:06.600 --> 02:31:08.140
Om Hashmi: our call center calls.

1354
02:31:09.050 --> 02:31:11.389
Om Hashmi: Think about that use case in particular.

1355
02:31:11.590 --> 02:31:18.830
Om Hashmi: Think about how many different departments within an organization a large utilities provider. Would I need to coordinate

1356
02:31:19.040 --> 02:31:20.110
Om Hashmi: this width?

1357
02:31:20.530 --> 02:31:26.139
Om Hashmi: I wish it was as simple as oh, I would go to the center of excellence for Gen. AI. And and that's it.

1358
02:31:26.810 --> 02:31:29.020
Om Hashmi: Think about how many hands

1359
02:31:29.280 --> 02:31:33.160
Om Hashmi: this technical program manager now needs to cut across.

1360
02:31:33.550 --> 02:31:36.850
Om Hashmi: All of this is not something that started yesterday.

1361
02:31:36.920 --> 02:31:41.889
Om Hashmi: It started with the West Coast Company. So I have a bunch of examples just for everybody to take back

1362
02:31:41.900 --> 02:31:44.180
Om Hashmi: and see what a technologist

1363
02:31:44.980 --> 02:31:54.839
Om Hashmi: or what a technical program manager brings to the table when it comes to actually executing real world projects real world programs from these West Coast companies.

1364
02:31:54.860 --> 02:32:01.640
Om Hashmi: So take an example of a Netflix. I hope many of us have had some experience with utilizing or having their own

1365
02:32:01.910 --> 02:32:05.849
Om Hashmi: personal subscription to Netflix, or any other streaming platform. For that matter.

1366
02:32:06.930 --> 02:32:12.020
Om Hashmi: Netflix created in 2,007 the concept of something called Netflix Prize.

1367
02:32:12.130 --> 02:32:17.070
Om Hashmi: That was their flagship, personalized content recommendations engine.

1368
02:32:17.450 --> 02:32:26.540
Om Hashmi: Contrary to popular belief, they started it back in 2,007. So they were not waiting for generative AI to catch where it is today. It's all built on the concepts

1369
02:32:26.560 --> 02:32:30.469
Om Hashmi: of big data and machine learning that has existed from the eighties.

1370
02:32:31.170 --> 02:32:32.899
Om Hashmi: But they didn't stop there.

1371
02:32:33.350 --> 02:32:37.189
Om Hashmi: They're a team led by a technical program manager

1372
02:32:37.360 --> 02:32:38.760
Om Hashmi: to curate

1373
02:32:38.890 --> 02:32:44.969
Om Hashmi: data quality efforts so that they can improve what they call qoe

1374
02:32:45.410 --> 02:32:47.930
Om Hashmi: quality of experience.

1375
02:32:48.480 --> 02:32:52.910
Om Hashmi: That's the outcome that they were trying to drive with the the data that they had now

1376
02:32:53.050 --> 02:32:55.559
Om Hashmi: to get to that point. They put in

1377
02:32:55.700 --> 02:32:59.909
Om Hashmi: over 400,000 different parameters

1378
02:33:00.380 --> 02:33:06.209
Om Hashmi: on every user's experience. As you're going through the series of watching anything.

1379
02:33:06.470 --> 02:33:10.690
Om Hashmi: Think about parameters that a layman would not even think about.

1380
02:33:11.060 --> 02:33:13.609
Om Hashmi: think about things like bit rates.

1381
02:33:13.730 --> 02:33:15.310
Om Hashmi: rebuffer rates.

1382
02:33:15.610 --> 02:33:20.820
Om Hashmi: the the combination of them impacting impacted by your network capacity

1383
02:33:20.920 --> 02:33:26.970
Om Hashmi: because the role of the Tpm here was to make sure that information gets captured

1384
02:33:27.100 --> 02:33:32.569
Om Hashmi: and shared with data engineers, data scientists to be able to look at and say.

1385
02:33:32.810 --> 02:33:36.210
Om Hashmi: is the subtitle that we display on the screen

1386
02:33:36.330 --> 02:33:42.619
Om Hashmi: appropriately buffered at the right instance when someone is actually speaking.

1387
02:33:43.560 --> 02:33:44.990
Om Hashmi: That was the initiative.

1388
02:33:45.740 --> 02:33:47.480
Om Hashmi: To get to that level.

1389
02:33:48.200 --> 02:33:57.479
Om Hashmi: The role of a Tpm is way more than just. I'm going to come in and run my ceremonies. It's just going to be, hey? I have a project plan that I need to execute.

1390
02:33:59.210 --> 02:34:07.719
Om Hashmi: Let's take another example. Some of us who might have had exposure or experience with the most recent versions of Apple's

1391
02:34:07.820 --> 02:34:17.230
Om Hashmi: devices. Apple just initiated or launched earlier this year something called Apple Intelligence. How many of you've had any experience with apple intelligence recently

1392
02:34:17.700 --> 02:34:19.240
Om Hashmi: heard of apple intelligence.

1393
02:34:19.840 --> 02:34:23.560
Om Hashmi: So apple intelligence is, is a very unique use case, because

1394
02:34:23.890 --> 02:34:30.339
Om Hashmi: the entire product portfolio is built on management of privacy concerns.

1395
02:34:30.680 --> 02:34:38.000
Om Hashmi: And again, head of technology program management over there hand selected individuals who came with deep knowledge

1396
02:34:38.060 --> 02:34:39.470
Om Hashmi: of privacy.

1397
02:34:39.690 --> 02:34:43.510
Om Hashmi: data, privacy and data governance to run this initiative

1398
02:34:43.750 --> 02:34:46.600
Om Hashmi: as part of the Tpm. Role for this initiative.

1399
02:34:46.960 --> 02:34:47.810
Om Hashmi: Why?

1400
02:34:48.290 --> 02:34:54.360
Om Hashmi: Because the expectation for a Tpm. Is the more knowledgeable you are in that domain

1401
02:34:54.820 --> 02:34:58.670
Om Hashmi: the more valuable. You are as an entrepreneur to this program.

1402
02:34:59.300 --> 02:35:00.939
Om Hashmi: So Apple's

1403
02:35:01.810 --> 02:35:06.840
Om Hashmi: apple intelligence combines 2 key combination or 2 key elements.

1404
02:35:06.940 --> 02:35:09.590
Om Hashmi: First, st on device data processing.

1405
02:35:10.030 --> 02:35:15.889
Om Hashmi: They have created a large language model that runs off of 3 billion

1406
02:35:16.340 --> 02:35:17.520
Om Hashmi: parameters

1407
02:35:17.700 --> 02:35:20.860
Om Hashmi: on the iphone device itself. Self-contained.

1408
02:35:21.190 --> 02:35:29.629
Om Hashmi: does not have to send any data does not have to correlate with an offshore or an offline model of any sort runs on device by itself.

1409
02:35:30.050 --> 02:35:39.009
Om Hashmi: Now, even with 3 billion data points, cell phone with their advanced a chip memories and and a chips and M. Chips. It runs out.

1410
02:35:39.420 --> 02:35:48.339
Om Hashmi: and therefore they came up with the next level, which was their private cloud compute an initiative that was led strictly by a data privacy

1411
02:35:48.590 --> 02:35:50.779
Om Hashmi: technical program manager.

1412
02:35:51.350 --> 02:36:01.239
Om Hashmi: coordinating with the legal teams, coordinating with the internal privacy standards, making sure that the User trust is of paramount importance. As we went through

1413
02:36:03.120 --> 02:36:08.530
Om Hashmi: Google did the exact same thing with when they 1st launched Google Photos and Google Assistant.

1414
02:36:08.780 --> 02:36:13.960
Om Hashmi: Look at the examples in your phones today for those of us who have a smartphone today. And I'm hoping it's everyone.

1415
02:36:14.230 --> 02:36:26.290
Om Hashmi: If you look at your smartphones today, you're gonna look at it and say, if I go on my photos, I can actually search in my photos by using a keyword. So I can just search for a photo where I'm playing a sport.

1416
02:36:26.310 --> 02:36:28.470
Om Hashmi: We can do a keyword search for sport.

1417
02:36:28.500 --> 02:36:35.589
Om Hashmi: You never tagged any of this information. You never annotated or labeled any of these images.

1418
02:36:36.700 --> 02:36:49.249
Om Hashmi: In order for you to do this, there is a governance, a rule book that gets prepared in any of these tools in any of these technologies organizations that are now striving. And this is probably one of the hottest space

1419
02:36:49.270 --> 02:36:59.469
Om Hashmi: 4 roles today is technical program managers who can help with data, governance and AI governance in organizations. Somebody who can guide ethical development

1420
02:37:00.700 --> 02:37:06.870
Om Hashmi: key theme in this being experience in data, privacy and and data governance.

1421
02:37:07.930 --> 02:37:10.169
Om Hashmi: Look back at a completely separate example.

1422
02:37:10.300 --> 02:37:14.710
Om Hashmi: Amazon. And again, this is not just limited to Amazon's

1423
02:37:15.060 --> 02:37:16.760
Om Hashmi: echo and and

1424
02:37:17.000 --> 02:37:29.540
Om Hashmi: Alexa devices, by any means any smart assistance. Home assistant that any one of us might be using today comes preequipped with, we've all heard of the term Nlp. Natural language processing. How many of you have heard of the term Nlu

1425
02:37:31.980 --> 02:37:33.040
Om Hashmi: nlu

1426
02:37:35.660 --> 02:37:37.739
Om Hashmi: natural language understanding.

1427
02:37:38.340 --> 02:37:39.619
Om Hashmi: That's when it started

1428
02:37:39.900 --> 02:37:42.479
Om Hashmi: the original paper on this from Google.

1429
02:37:42.970 --> 02:37:45.520
Om Hashmi: when Burt's paper came out in 2017

1430
02:37:46.150 --> 02:37:47.280
Om Hashmi: or 2018.

1431
02:37:48.600 --> 02:37:56.080
Om Hashmi: Right? So if we think about the technology element of being able to take multilingual data points.

1432
02:37:56.280 --> 02:38:00.720
Om Hashmi: get the effort to expand across multiple language supports.

1433
02:38:00.740 --> 02:38:10.999
Om Hashmi: deploy learning systems, deep neural networks, to be able to look at this continually learn and build out products that can actually not just

1434
02:38:11.210 --> 02:38:13.860
Om Hashmi: process, but actually understand the ask.

1435
02:38:14.120 --> 02:38:15.670
Om Hashmi: So, for instance.

1436
02:38:16.330 --> 02:38:22.080
Om Hashmi: back when metadata annotation and labeling 1st started in in mid 2 thousands.

1437
02:38:23.060 --> 02:38:27.339
Om Hashmi: organizations like Apple spent millions of hours

1438
02:38:27.440 --> 02:38:29.640
Om Hashmi: on annotating data.

1439
02:38:30.010 --> 02:38:41.759
Om Hashmi: Right? You would go about annotating data thinking, Hey, I've received a song. I have a song from Michael Jackson, and I want to say, Hey, this is a Michael Jackson song. So the next time someone comes in and says, Hey.

1440
02:38:41.810 --> 02:38:45.520
Om Hashmi: Hey, siri! Or Hey, Alexa, can you play me a Michael Jackson song.

1441
02:38:45.580 --> 02:38:48.599
Om Hashmi: It's gonna pick from one that has been annotated

1442
02:38:49.570 --> 02:38:51.390
Om Hashmi: today. That's no longer the case.

1443
02:38:51.490 --> 02:38:53.390
Om Hashmi: because now it understands

1444
02:38:54.320 --> 02:38:56.290
Om Hashmi: right. Natural language. Understanding.

1445
02:38:56.880 --> 02:39:02.590
Om Hashmi: This is one major element of the types of programs and projects. Organizations are actively deploying. Today.

1446
02:39:03.150 --> 02:39:04.380
Om Hashmi: There's a second

1447
02:39:04.720 --> 02:39:06.620
Om Hashmi: key element or key

1448
02:39:06.650 --> 02:39:11.920
Om Hashmi: marketplace. I want to say, when it comes to generative AI initiatives in large enterprises.

1449
02:39:12.640 --> 02:39:15.659
Om Hashmi: something called the science of computer Vision.

1450
02:39:15.740 --> 02:39:17.950
Om Hashmi: where you're utilizing the sense of

1451
02:39:18.350 --> 02:39:19.390
Om Hashmi: vision

1452
02:39:19.410 --> 02:39:24.599
Om Hashmi: to translate into any kind of a business outcome, the most common and the most

1453
02:39:24.940 --> 02:39:29.749
Om Hashmi: relevant example for those of us who drive one of these Evs.

1454
02:39:30.300 --> 02:39:33.069
Om Hashmi: or else otherwise any other car.

1455
02:39:33.310 --> 02:39:38.179
Om Hashmi: is the full self-driving mode that Tesla has been working on for the last several years.

1456
02:39:38.340 --> 02:39:40.979
Om Hashmi: Right? They just deployed this a few months ago.

1457
02:39:41.240 --> 02:39:46.289
Om Hashmi: happened to have been a part of some of the pilot programs and discussions back in the West Coast.

1458
02:39:46.420 --> 02:39:49.199
Om Hashmi: across the different programs and projects.

1459
02:39:49.530 --> 02:39:52.530
Om Hashmi: right capturing, processing.

1460
02:39:52.970 --> 02:39:54.530
Om Hashmi: recognizing.

1461
02:39:54.710 --> 02:40:05.170
Om Hashmi: dealing with opacity, dealing with luminousity of all the different elements when it comes to image, recognition, image, processing, image, classification, all of those elements.

1462
02:40:05.500 --> 02:40:13.270
Om Hashmi: If you get a good deep understanding of those technical pieces. Your ability to drive success to your Gen. AI initiatives

1463
02:40:13.500 --> 02:40:14.779
Om Hashmi: is quadrupled.

1464
02:40:16.210 --> 02:40:16.950
Om Hashmi: So

1465
02:40:19.660 --> 02:40:26.880
Om Hashmi: now that we've talked about all of these examples of Google and Netflix and Tesla and Apples. Let's see what job they're actually looking for.

1466
02:40:30.690 --> 02:40:32.169
Om Hashmi: These are from yesterday.

1467
02:40:35.240 --> 02:40:39.579
Om Hashmi: So what organizations are looking for technical program managers today.

1468
02:40:41.220 --> 02:40:46.069
Om Hashmi: all the companies that I've mentioned, plus the organizations that are

1469
02:40:46.560 --> 02:40:50.330
Om Hashmi: non-technical in nature are actively hiring

1470
02:40:50.890 --> 02:41:02.440
Om Hashmi: technical program managers a real role. Look at the one on the right side in the in the Middle Trust and Safety technical program manager. That's directly related to their Apple Intelligence program.

1471
02:41:04.140 --> 02:41:09.260
Om Hashmi: The deeper you can learn the more upscale, you are the better fit you become.

1472
02:41:10.000 --> 02:41:13.330
Om Hashmi: So how do you become a technical program manager? How do you

1473
02:41:13.350 --> 02:41:27.380
Om Hashmi: improve your skill set as a technical program manager right on the, you know, from the gecko starts with technical competence. Gotta start up skilling on the technology side, not just from the project management, not just at the surface, but go deeper.

1474
02:41:27.470 --> 02:41:31.780
Om Hashmi: Now. Does that mean drop everything and start learning how to write code? And

1475
02:41:31.870 --> 02:41:33.010
Om Hashmi: absolutely not.

1476
02:41:33.270 --> 02:41:34.809
Om Hashmi: on the contrary.

1477
02:41:34.970 --> 02:41:39.949
Om Hashmi: think about yourself. The last time you had to provide an effort estimate to get some work done.

1478
02:41:41.240 --> 02:41:53.200
Om Hashmi: Were you able to give even a swag estimate without having to deal with or talk to some of your technical engineers technical needs. The answer is, yes, you're already making good progress on that technical competence. But for most

1479
02:41:53.600 --> 02:41:58.709
Om Hashmi: project program managers, traditional agilist. The answer is, no, I have to go ask.

1480
02:41:59.120 --> 02:42:02.889
Om Hashmi: I have to go. Ask John DOE or Mary Jane, my tech lead

1481
02:42:03.560 --> 02:42:05.049
Om Hashmi: getting out of that.

